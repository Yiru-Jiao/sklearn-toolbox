{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#0-测试数据-Test-data\" data-toc-modified-id=\"0-测试数据-Test-data-1\">0 测试数据 Test data</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Dataset-split\" data-toc-modified-id=\"Dataset-split-1.0.1\">Dataset split</a></span></li></ul></li></ul></li><li><span><a href=\"#1-Before-Learning\" data-toc-modified-id=\"1-Before-Learning-2\">1 Before Learning</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.1-特征提取-sklearn.feature_extraction\" data-toc-modified-id=\"1.1-特征提取-sklearn.feature_extraction-2.1\">1.1 特征提取 <code>sklearn.feature_extraction</code></a></span><ul class=\"toc-item\"><li><span><a href=\"#1.1.1-字典➡特征-from-dict-like-objects-to-NumPy/SciPy-representations\" data-toc-modified-id=\"1.1.1-字典➡特征-from-dict-like-objects-to-NumPy/SciPy-representations-2.1.1\">1.1.1 字典➡特征 from dict-like objects to NumPy/SciPy representations</a></span></li><li><span><a href=\"#1.1.2-字符串➡稀疏矩阵（使用哈希函数）from-string-to-sparse-matrices-with-Hash-function\" data-toc-modified-id=\"1.1.2-字符串➡稀疏矩阵（使用哈希函数）from-string-to-sparse-matrices-with-Hash-function-2.1.2\">1.1.2 字符串➡稀疏矩阵（使用哈希函数）from string to sparse matrices with Hash function</a></span></li><li><span><a href=\"#1.1.3-文本特征提取-text-feature-extraction\" data-toc-modified-id=\"1.1.3-文本特征提取-text-feature-extraction-2.1.3\">1.1.3 文本特征提取 text feature extraction</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.1.3.1-文本➡token计数矩阵-from-text-to-token-count-matrix\" data-toc-modified-id=\"1.1.3.1-文本➡token计数矩阵-from-text-to-token-count-matrix-2.1.3.1\">1.1.3.1 文本➡token计数矩阵 from text to token count matrix</a></span></li><li><span><a href=\"#1.1.3.2-计数矩阵➡tf/tf-idf-频率表示-from-count-matrix-to-tf/tf-idf-representation\" data-toc-modified-id=\"1.1.3.2-计数矩阵➡tf/tf-idf-频率表示-from-count-matrix-to-tf/tf-idf-representation-2.1.3.2\">1.1.3.2 计数矩阵➡tf/tf-idf 频率表示 from count matrix to tf/tf-idf representation</a></span></li><li><span><a href=\"#1.1.3.3-文本➡tf/idf频率表示-from-text-to-tf/tf-idf-representation\" data-toc-modified-id=\"1.1.3.3-文本➡tf/idf频率表示-from-text-to-tf/tf-idf-representation-2.1.3.3\">1.1.3.3 文本➡tf/idf频率表示 from text to tf/tf-idf representation</a></span></li></ul></li><li><span><a href=\"#1.1.4-图像特征提取-image-feature-extraction\" data-toc-modified-id=\"1.1.4-图像特征提取-image-feature-extraction-2.1.4\">1.1.4 图像特征提取 image feature extraction</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.1.4.1-Patch提取-patch-extraction\" data-toc-modified-id=\"1.1.4.1-Patch提取-patch-extraction-2.1.4.1\">1.1.4.1 Patch提取 patch extraction</a></span></li><li><span><a href=\"#1.1.4.2-图像的连接图-connectivity-graph\" data-toc-modified-id=\"1.1.4.2-图像的连接图-connectivity-graph-2.1.4.2\">1.1.4.2 图像的连接图 connectivity graph</a></span></li></ul></li></ul></li><li><span><a href=\"#1.2-数据预处理-sklearn.preprocessing\" data-toc-modified-id=\"1.2-数据预处理-sklearn.preprocessing-2.2\">1.2 数据预处理 <code>sklearn.preprocessing</code></a></span><ul class=\"toc-item\"><li><span><a href=\"#1.2.1-数据标准化（对每列）Standardization\" data-toc-modified-id=\"1.2.1-数据标准化（对每列）Standardization-2.2.1\">1.2.1 数据标准化（对每列）Standardization</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.2.1.1-标准化-z-=-(x---u)-/-s\" data-toc-modified-id=\"1.2.1.1-标准化-z-=-(x---u)-/-s-2.2.1.1\">1.2.1.1 标准化 z = (x - u) / s</a></span></li><li><span><a href=\"#1.2.1.2-最小最大标准化-z-=-(x---min)-/-(max---min)\" data-toc-modified-id=\"1.2.1.2-最小最大标准化-z-=-(x---min)-/-(max---min)-2.2.1.2\">1.2.1.2 最小最大标准化 z = (x - min) / (max - min)</a></span></li><li><span><a href=\"#1.2.1.3-最大绝对值标准化-z-=-x-/-max(abs(x))\" data-toc-modified-id=\"1.2.1.3-最大绝对值标准化-z-=-x-/-max(abs(x))-2.2.1.3\">1.2.1.3 最大绝对值标准化 z = x / max(abs(x))</a></span></li><li><span><a href=\"#1.2.1.4-稳健标准化-IQR-=-Q3---Q1;-z-=-(x---median)-/-IQR\" data-toc-modified-id=\"1.2.1.4-稳健标准化-IQR-=-Q3---Q1;-z-=-(x---median)-/-IQR-2.2.1.4\">1.2.1.4 稳健标准化 IQR = Q3 - Q1; z = (x - median) / IQR</a></span></li></ul></li><li><span><a href=\"#1.2.2-非线性变换（对每列）Non-linear-transformation\" data-toc-modified-id=\"1.2.2-非线性变换（对每列）Non-linear-transformation-2.2.2\">1.2.2 非线性变换（对每列）Non-linear transformation</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.2.2.1-Quantile-transformer-变换为0-1均匀分布或标准正态分布\" data-toc-modified-id=\"1.2.2.1-Quantile-transformer-变换为0-1均匀分布或标准正态分布-2.2.2.1\">1.2.2.1 Quantile transformer 变换为0-1均匀分布或标准正态分布</a></span></li><li><span><a href=\"#1.2.2.2-Power-transformer-变换为高斯分布\" data-toc-modified-id=\"1.2.2.2-Power-transformer-变换为高斯分布-2.2.2.2\">1.2.2.2 Power transformer 变换为高斯分布</a></span></li></ul></li><li><span><a href=\"#1.2.3-数据归一化（对每行）Normalization\" data-toc-modified-id=\"1.2.3-数据归一化（对每行）Normalization-2.2.3\">1.2.3 数据归一化（对每行）Normalization</a></span></li><li><span><a href=\"#1.2.4-类别特征编码（对列）Encoding-categorical-features\" data-toc-modified-id=\"1.2.4-类别特征编码（对列）Encoding-categorical-features-2.2.4\">1.2.4 类别特征编码（对列）Encoding categorical features</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.2.4.1-编码为有序数字-Ordinal-encoder\" data-toc-modified-id=\"1.2.4.1-编码为有序数字-Ordinal-encoder-2.2.4.1\">1.2.4.1 编码为有序数字 Ordinal encoder</a></span></li><li><span><a href=\"#1.2.4.2-编码为二元变量-One-of-K-encoder\" data-toc-modified-id=\"1.2.4.2-编码为二元变量-One-of-K-encoder-2.2.4.2\">1.2.4.2 编码为二元变量 One-of-K encoder</a></span></li></ul></li><li><span><a href=\"#1.2.5-数据离散化（对列）Discretization\" data-toc-modified-id=\"1.2.5-数据离散化（对列）Discretization-2.2.5\">1.2.5 数据离散化（对列）Discretization</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.2.5.1-K-bins-discretization\" data-toc-modified-id=\"1.2.5.1-K-bins-discretization-2.2.5.1\">1.2.5.1 K-bins discretization</a></span></li><li><span><a href=\"#1.2.5.2-Binarization-x-=-0:-x-≤-threshold\" data-toc-modified-id=\"1.2.5.2-Binarization-x-=-0:-x-≤-threshold-2.2.5.2\">1.2.5.2 Binarization x = 0: x ≤ threshold</a></span></li></ul></li><li><span><a href=\"#1.2.6-生成多项式特征-Generating-polynomial-features\" data-toc-modified-id=\"1.2.6-生成多项式特征-Generating-polynomial-features-2.2.6\">1.2.6 生成多项式特征 Generating polynomial features</a></span></li><li><span><a href=\"#1.2.7-标签编码和离散化-Label-encoding-and-discretization\" data-toc-modified-id=\"1.2.7-标签编码和离散化-Label-encoding-and-discretization-2.2.7\">1.2.7 标签编码和离散化 Label encoding and discretization</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.2.7.1-Label-binarization\" data-toc-modified-id=\"1.2.7.1-Label-binarization-2.2.7.1\">1.2.7.1 Label binarization</a></span></li><li><span><a href=\"#1.2.7.2-Label-encoding\" data-toc-modified-id=\"1.2.7.2-Label-encoding-2.2.7.2\">1.2.7.2 Label encoding</a></span></li></ul></li><li><span><a href=\"#1.2.8-建立转换器-function➡transformer\" data-toc-modified-id=\"1.2.8-建立转换器-function➡transformer-2.2.8\">1.2.8 建立转换器 function➡transformer</a></span></li></ul></li><li><span><a href=\"#1.3-缺失值插补-sklearn.impute\" data-toc-modified-id=\"1.3-缺失值插补-sklearn.impute-2.3\">1.3 缺失值插补 <code>sklearn.impute</code></a></span><ul class=\"toc-item\"><li><span><a href=\"#1.3.1-单变量插补-Univariate-feature-imputation\" data-toc-modified-id=\"1.3.1-单变量插补-Univariate-feature-imputation-2.3.1\">1.3.1 单变量插补 Univariate feature imputation</a></span></li><li><span><a href=\"#1.3.2-多变量插补-Multivariate-feature-imputation\" data-toc-modified-id=\"1.3.2-多变量插补-Multivariate-feature-imputation-2.3.2\">1.3.2 多变量插补 Multivariate feature imputation</a></span></li><li><span><a href=\"#1.3.3-最近邻插值-Nearest-neighbors-imputation\" data-toc-modified-id=\"1.3.3-最近邻插值-Nearest-neighbors-imputation-2.3.3\">1.3.3 最近邻插值 Nearest neighbors imputation</a></span></li><li><span><a href=\"#1.3.4-缺失值定位-Marking-imputed-values\" data-toc-modified-id=\"1.3.4-缺失值定位-Marking-imputed-values-2.3.4\">1.3.4 缺失值定位 Marking imputed values</a></span></li></ul></li><li><span><a href=\"#1.4-特征选择-sklearn.feature_selection\" data-toc-modified-id=\"1.4-特征选择-sklearn.feature_selection-2.4\">1.4 特征选择 <code>sklearn.feature_selection</code></a></span><ul class=\"toc-item\"><li><span><a href=\"#1.4.1-方差过滤-Remove-features-whose-variance-doesn't-meet-some-threshold.\" data-toc-modified-id=\"1.4.1-方差过滤-Remove-features-whose-variance-doesn't-meet-some-threshold.-2.4.1\">1.4.1 方差过滤 Remove features whose variance doesn't meet some threshold.</a></span></li><li><span><a href=\"#1.4.2-单变量特征选择-Univariate-feature-selection\" data-toc-modified-id=\"1.4.2-单变量特征选择-Univariate-feature-selection-2.4.2\">1.4.2 单变量特征选择 Univariate feature selection</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.4.2.1-According-to-the-k-highest-scores\" data-toc-modified-id=\"1.4.2.1-According-to-the-k-highest-scores-2.4.2.1\">1.4.2.1 According to the k highest scores</a></span></li><li><span><a href=\"#1.4.2.2-According-to-a-percentile-of-the-highest-scores\" data-toc-modified-id=\"1.4.2.2-According-to-a-percentile-of-the-highest-scores-2.4.2.2\">1.4.2.2 According to a percentile of the highest scores</a></span></li><li><span><a href=\"#1.4.2.3-According-to-p-values-for-FPR\" data-toc-modified-id=\"1.4.2.3-According-to-p-values-for-FPR-2.4.2.3\">1.4.2.3 According to p-values for FPR</a></span></li><li><span><a href=\"#1.4.2.4-According-to-p-values-for-FDR\" data-toc-modified-id=\"1.4.2.4-According-to-p-values-for-FDR-2.4.2.4\">1.4.2.4 According to p-values for FDR</a></span></li><li><span><a href=\"#1.4.2.5-According-to-p-values-for-FWE\" data-toc-modified-id=\"1.4.2.5-According-to-p-values-for-FWE-2.4.2.5\">1.4.2.5 According to p-values for FWE</a></span></li><li><span><a href=\"#1.4.2.6-单变量特征选择器-Univariate-feature-selector\" data-toc-modified-id=\"1.4.2.6-单变量特征选择器-Univariate-feature-selector-2.4.2.6\">1.4.2.6 单变量特征选择器 Univariate feature selector</a></span></li></ul></li><li><span><a href=\"#1.4.3-递归式特征消除-Recursive-feature-elimination\" data-toc-modified-id=\"1.4.3-递归式特征消除-Recursive-feature-elimination-2.4.3\">1.4.3 递归式特征消除 Recursive feature elimination</a></span></li><li><span><a href=\"#1.4.4-元转换器-SelectFromModel\" data-toc-modified-id=\"1.4.4-元转换器-SelectFromModel-2.4.4\">1.4.4 元转换器 SelectFromModel</a></span></li></ul></li></ul></li><li><span><a href=\"#2-Supervised-Learning\" data-toc-modified-id=\"2-Supervised-Learning-3\">2 Supervised Learning</a></span><ul class=\"toc-item\"><li><span><a href=\"#2.1-线性模型-sklearn.linear_model\" data-toc-modified-id=\"2.1-线性模型-sklearn.linear_model-3.1\">2.1 线性模型 <code>sklearn.linear_model</code></a></span><ul class=\"toc-item\"><li><span><a href=\"#2.1.1-最小二乘-Ordinary-Least-Squares\" data-toc-modified-id=\"2.1.1-最小二乘-Ordinary-Least-Squares-3.1.1\">2.1.1 最小二乘 Ordinary Least Squares</a></span></li><li><span><a href=\"#2.1.2-岭回归与分类-Ridge-regression-and-classification\" data-toc-modified-id=\"2.1.2-岭回归与分类-Ridge-regression-and-classification-3.1.2\">2.1.2 岭回归与分类 Ridge regression and classification</a></span><ul class=\"toc-item\"><li><span><a href=\"#2.1.2.1-Ridge-regression\" data-toc-modified-id=\"2.1.2.1-Ridge-regression-3.1.2.1\">2.1.2.1 Ridge regression</a></span></li><li><span><a href=\"#2.1.2.2-Ridge-classification\" data-toc-modified-id=\"2.1.2.2-Ridge-classification-3.1.2.2\">2.1.2.2 Ridge classification</a></span></li><li><span><a href=\"#2.1.2.3-Cross-validated-ridge-regression\" data-toc-modified-id=\"2.1.2.3-Cross-validated-ridge-regression-3.1.2.3\">2.1.2.3 Cross-validated ridge regression</a></span></li></ul></li><li><span><a href=\"#2.1.3-Lasso\" data-toc-modified-id=\"2.1.3-Lasso-3.1.3\">2.1.3 Lasso</a></span><ul class=\"toc-item\"><li><span><a href=\"#2.1.3.1-Lasso-model\" data-toc-modified-id=\"2.1.3.1-Lasso-model-3.1.3.1\">2.1.3.1 Lasso model</a></span></li><li><span><a href=\"#2.1.3.2-Cross-validated-Lasso-model\" data-toc-modified-id=\"2.1.3.2-Cross-validated-Lasso-model-3.1.3.2\">2.1.3.2 Cross-validated Lasso model</a></span></li><li><span><a href=\"#2.1.3.3-Multi-task-Lasso\" data-toc-modified-id=\"2.1.3.3-Multi-task-Lasso-3.1.3.3\">2.1.3.3 Multi-task Lasso</a></span></li></ul></li><li><span><a href=\"#2.1.4-逻辑回归-Logistic-Regression/Log-it-Regression\" data-toc-modified-id=\"2.1.4-逻辑回归-Logistic-Regression/Log-it-Regression-3.1.4\">2.1.4 逻辑回归 Logistic Regression/Log-it Regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#2.1.4.1-Logistic-regression\" data-toc-modified-id=\"2.1.4.1-Logistic-regression-3.1.4.1\">2.1.4.1 Logistic regression</a></span></li><li><span><a href=\"#2.1.4.2-Cross-validated-logistic-regression\" data-toc-modified-id=\"2.1.4.2-Cross-validated-logistic-regression-3.1.4.2\">2.1.4.2 Cross-validated logistic regression</a></span></li></ul></li><li><span><a href=\"#2.1.5-广义线性回归-Generalized-Linear-Regression\" data-toc-modified-id=\"2.1.5-广义线性回归-Generalized-Linear-Regression-3.1.5\">2.1.5 广义线性回归 Generalized Linear Regression</a></span></li><li><span><a href=\"#2.1.6-随机梯度下降-Stochastic-Gradient-Descent\" data-toc-modified-id=\"2.1.6-随机梯度下降-Stochastic-Gradient-Descent-3.1.6\">2.1.6 随机梯度下降 Stochastic Gradient Descent</a></span><ul class=\"toc-item\"><li><span><a href=\"#2.1.6.1-SGD-regression\" data-toc-modified-id=\"2.1.6.1-SGD-regression-3.1.6.1\">2.1.6.1 SGD regression</a></span></li><li><span><a href=\"#2.1.6.2-SGD-classification\" data-toc-modified-id=\"2.1.6.2-SGD-classification-3.1.6.2\">2.1.6.2 SGD classification</a></span></li></ul></li><li><span><a href=\"#2.1.7-多项式回归-Polynomial-regression-with-pipeline\" data-toc-modified-id=\"2.1.7-多项式回归-Polynomial-regression-with-pipeline-3.1.7\">2.1.7 多项式回归 Polynomial regression with pipeline</a></span></li></ul></li><li><span><a href=\"#2.2-判别分析-sklearn.discriminant_analysis\" data-toc-modified-id=\"2.2-判别分析-sklearn.discriminant_analysis-3.2\">2.2 判别分析 <code>sklearn.discriminant_analysis</code></a></span><ul class=\"toc-item\"><li><span><a href=\"#2.2.1-线性判别分析-Linear-discriminant-analysis\" data-toc-modified-id=\"2.2.1-线性判别分析-Linear-discriminant-analysis-3.2.1\">2.2.1 线性判别分析 Linear discriminant analysis</a></span></li><li><span><a href=\"#2.2.2-二次判别分析-Quadratic-discriminant-analysis\" data-toc-modified-id=\"2.2.2-二次判别分析-Quadratic-discriminant-analysis-3.2.2\">2.2.2 二次判别分析 Quadratic discriminant analysis</a></span></li></ul></li><li><span><a href=\"#2.3-支持向量机-sklearn.svm\" data-toc-modified-id=\"2.3-支持向量机-sklearn.svm-3.3\">2.3 支持向量机 <code>sklearn.svm</code></a></span><ul class=\"toc-item\"><li><span><a href=\"#2.3.1-支持向量分类-Support-Vector-Classification\" data-toc-modified-id=\"2.3.1-支持向量分类-Support-Vector-Classification-3.3.1\">2.3.1 支持向量分类 Support Vector Classification</a></span><ul class=\"toc-item\"><li><span><a href=\"#2.3.1.1-SVC\" data-toc-modified-id=\"2.3.1.1-SVC-3.3.1.1\">2.3.1.1 SVC</a></span></li><li><span><a href=\"#2.3.1.2-NuSVC\" data-toc-modified-id=\"2.3.1.2-NuSVC-3.3.1.2\">2.3.1.2 NuSVC</a></span></li><li><span><a href=\"#2.3.1.3-LinearSVC\" data-toc-modified-id=\"2.3.1.3-LinearSVC-3.3.1.3\">2.3.1.3 LinearSVC</a></span></li></ul></li><li><span><a href=\"#2.3.2-支持向量回归-Support-Vector-Regression\" data-toc-modified-id=\"2.3.2-支持向量回归-Support-Vector-Regression-3.3.2\">2.3.2 支持向量回归 Support Vector Regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#2.3.2.1-SVR\" data-toc-modified-id=\"2.3.2.1-SVR-3.3.2.1\">2.3.2.1 SVR</a></span></li><li><span><a href=\"#2.3.2.2-NuSVR\" data-toc-modified-id=\"2.3.2.2-NuSVR-3.3.2.2\">2.3.2.2 NuSVR</a></span></li><li><span><a href=\"#2.3.2.3-LinearSVR\" data-toc-modified-id=\"2.3.2.3-LinearSVR-3.3.2.3\">2.3.2.3 LinearSVR</a></span></li></ul></li><li><span><a href=\"#2.3*-核近似-sklearn.kernel_approximation\" data-toc-modified-id=\"2.3*-核近似-sklearn.kernel_approximation-3.3.3\">2.3* 核近似 sklearn.kernel_approximation</a></span></li></ul></li><li><span><a href=\"#2.4-最近邻-sklearn.neighbors\" data-toc-modified-id=\"2.4-最近邻-sklearn.neighbors-3.4\">2.4 最近邻 <code>sklearn.neighbors</code></a></span><ul class=\"toc-item\"><li><span><a href=\"#2.4.1-无监督最近邻-Unsupervised-Nearest-Neighbors\" data-toc-modified-id=\"2.4.1-无监督最近邻-Unsupervised-Nearest-Neighbors-3.4.1\">2.4.1 无监督最近邻 Unsupervised Nearest Neighbors</a></span></li><li><span><a href=\"#2.4.2-最近邻分类-Nearest-Neighbors-Classification\" data-toc-modified-id=\"2.4.2-最近邻分类-Nearest-Neighbors-Classification-3.4.2\">2.4.2 最近邻分类 Nearest Neighbors Classification</a></span><ul class=\"toc-item\"><li><span><a href=\"#2.4.2.1-K-nearest-neighbors-classification\" data-toc-modified-id=\"2.4.2.1-K-nearest-neighbors-classification-3.4.2.1\">2.4.2.1 K-nearest neighbors classification</a></span></li><li><span><a href=\"#2.4.2.2-Radius-based-neighbors-classification\" data-toc-modified-id=\"2.4.2.2-Radius-based-neighbors-classification-3.4.2.2\">2.4.2.2 Radius-based neighbors classification</a></span></li></ul></li><li><span><a href=\"#2.4.3-最近邻回归-Nearest-Neighbors-Regression\" data-toc-modified-id=\"2.4.3-最近邻回归-Nearest-Neighbors-Regression-3.4.3\">2.4.3 最近邻回归 Nearest Neighbors Regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#2.4.3.1-K-nearest-neighbors-regression\" data-toc-modified-id=\"2.4.3.1-K-nearest-neighbors-regression-3.4.3.1\">2.4.3.1 K-nearest neighbors regression</a></span></li><li><span><a href=\"#2.4.3.2-Radius-based-neighbors-regressor\" data-toc-modified-id=\"2.4.3.2-Radius-based-neighbors-regressor-3.4.3.2\">2.4.3.2 Radius-based neighbors regressor</a></span></li></ul></li><li><span><a href=\"#2.4.4-最近质心分类-Nearest-Centroid-Classification\" data-toc-modified-id=\"2.4.4-最近质心分类-Nearest-Centroid-Classification-3.4.4\">2.4.4 最近质心分类 Nearest Centroid Classification</a></span></li></ul></li><li><span><a href=\"#2.5-高斯过程-sklearn.gaussian_process\" data-toc-modified-id=\"2.5-高斯过程-sklearn.gaussian_process-3.5\">2.5 高斯过程 <code>sklearn.gaussian_process</code></a></span><ul class=\"toc-item\"><li><span><a href=\"#2.5.1-高斯过程回归-Gaussian-Process-Regression-(GPR)\" data-toc-modified-id=\"2.5.1-高斯过程回归-Gaussian-Process-Regression-(GPR)-3.5.1\">2.5.1 高斯过程回归 Gaussian Process Regression (GPR)</a></span></li><li><span><a href=\"#2.5.2-高斯过程分类-Gaussian-Process-Classification-(GPC)\" data-toc-modified-id=\"2.5.2-高斯过程分类-Gaussian-Process-Classification-(GPC)-3.5.2\">2.5.2 高斯过程分类 Gaussian Process Classification (GPC)</a></span></li><li><span><a href=\"#2.5.3-高斯过程的核-Kernels-for-Gaussian-Process\" data-toc-modified-id=\"2.5.3-高斯过程的核-Kernels-for-Gaussian-Process-3.5.3\">2.5.3 高斯过程的核 Kernels for Gaussian Process</a></span></li></ul></li><li><span><a href=\"#2.6-交叉分解-sklearn.cross_decomposition\" data-toc-modified-id=\"2.6-交叉分解-sklearn.cross_decomposition-3.6\">2.6 交叉分解 <code>sklearn.cross_decomposition</code></a></span><ul class=\"toc-item\"><li><span><a href=\"#2.6.1-偏最小二乘回归-Partial-Least-Squares-(PLS)-Regression\" data-toc-modified-id=\"2.6.1-偏最小二乘回归-Partial-Least-Squares-(PLS)-Regression-3.6.1\">2.6.1 偏最小二乘回归 Partial Least Squares (PLS) Regression</a></span></li><li><span><a href=\"#2.6.2-典型相关分析-Canonical-Correlation-Analysis\" data-toc-modified-id=\"2.6.2-典型相关分析-Canonical-Correlation-Analysis-3.6.2\">2.6.2 典型相关分析 Canonical Correlation Analysis</a></span></li></ul></li><li><span><a href=\"#2.7-朴素贝叶斯-sklearn.naive_bayes\" data-toc-modified-id=\"2.7-朴素贝叶斯-sklearn.naive_bayes-3.7\">2.7 朴素贝叶斯 <code>sklearn.naive_bayes</code></a></span><ul class=\"toc-item\"><li><span><a href=\"#2.7.1-高斯朴素贝叶斯-Gaussian-Naive-Bayes\" data-toc-modified-id=\"2.7.1-高斯朴素贝叶斯-Gaussian-Naive-Bayes-3.7.1\">2.7.1 高斯朴素贝叶斯 Gaussian Naive Bayes</a></span></li><li><span><a href=\"#2.7.2-多项式朴素贝叶斯-Multinomial-Naive-Bayes\" data-toc-modified-id=\"2.7.2-多项式朴素贝叶斯-Multinomial-Naive-Bayes-3.7.2\">2.7.2 多项式朴素贝叶斯 Multinomial Naive Bayes</a></span></li><li><span><a href=\"#2.7.3-伯努利贝叶斯-Bernoulli-Naive-Bayes\" data-toc-modified-id=\"2.7.3-伯努利贝叶斯-Bernoulli-Naive-Bayes-3.7.3\">2.7.3 伯努利贝叶斯 Bernoulli Naive Bayes</a></span></li><li><span><a href=\"#2.7.4-不同类特征的朴素贝叶斯分类器-Naive-Bayes-classifier-for-categorical-features\" data-toc-modified-id=\"2.7.4-不同类特征的朴素贝叶斯分类器-Naive-Bayes-classifier-for-categorical-features-3.7.4\">2.7.4 不同类特征的朴素贝叶斯分类器 Naive Bayes classifier for categorical features</a></span></li></ul></li><li><span><a href=\"#2.8-决策树-sklearn.tree\" data-toc-modified-id=\"2.8-决策树-sklearn.tree-3.8\">2.8 决策树 <code>sklearn.tree</code></a></span><ul class=\"toc-item\"><li><span><a href=\"#2.8.1-决策树分类-Decision-Tree-for-Classification\" data-toc-modified-id=\"2.8.1-决策树分类-Decision-Tree-for-Classification-3.8.1\">2.8.1 决策树分类 Decision Tree for Classification</a></span></li><li><span><a href=\"#2.8.2-决策树回归-Decision-Tree-for-Regression\" data-toc-modified-id=\"2.8.2-决策树回归-Decision-Tree-for-Regression-3.8.2\">2.8.2 决策树回归 Decision Tree for Regression</a></span></li><li><span><a href=\"#2.8.3-多输出问题-Multi-output-problems\" data-toc-modified-id=\"2.8.3-多输出问题-Multi-output-problems-3.8.3\">2.8.3 多输出问题 Multi-output problems</a></span></li></ul></li><li><span><a href=\"#2.9-集成学习-sklearn.ensemble\" data-toc-modified-id=\"2.9-集成学习-sklearn.ensemble-3.9\">2.9 集成学习 <code>sklearn.ensemble</code></a></span><ul class=\"toc-item\"><li><span><a href=\"#2.9.1-Bagging-元估计器-Bagging-meta-estimator\" data-toc-modified-id=\"2.9.1-Bagging-元估计器-Bagging-meta-estimator-3.9.1\">2.9.1 Bagging 元估计器 Bagging meta-estimator</a></span><ul class=\"toc-item\"><li><span><a href=\"#2.9.1.1-Bagging-Classifier\" data-toc-modified-id=\"2.9.1.1-Bagging-Classifier-3.9.1.1\">2.9.1.1 Bagging Classifier</a></span></li><li><span><a href=\"#2.9.1.2-Bagging-Regressor\" data-toc-modified-id=\"2.9.1.2-Bagging-Regressor-3.9.1.2\">2.9.1.2 Bagging Regressor</a></span></li></ul></li><li><span><a href=\"#2.9.2-随机森林-Random-Forests\" data-toc-modified-id=\"2.9.2-随机森林-Random-Forests-3.9.2\">2.9.2 随机森林 Random Forests</a></span><ul class=\"toc-item\"><li><span><a href=\"#2.9.2.1-Random-Forest-Classification\" data-toc-modified-id=\"2.9.2.1-Random-Forest-Classification-3.9.2.1\">2.9.2.1 Random Forest Classification</a></span></li><li><span><a href=\"#2.9.2.2-Random-Forest-Regression\" data-toc-modified-id=\"2.9.2.2-Random-Forest-Regression-3.9.2.2\">2.9.2.2 Random Forest Regression</a></span></li></ul></li><li><span><a href=\"#2.9.3-AdaBoost\" data-toc-modified-id=\"2.9.3-AdaBoost-3.9.3\">2.9.3 AdaBoost</a></span><ul class=\"toc-item\"><li><span><a href=\"#2.9.3.1-AdaBoost-Classification\" data-toc-modified-id=\"2.9.3.1-AdaBoost-Classification-3.9.3.1\">2.9.3.1 AdaBoost Classification</a></span></li><li><span><a href=\"#2.9.3.2-AdaBoost-Regression\" data-toc-modified-id=\"2.9.3.2-AdaBoost-Regression-3.9.3.2\">2.9.3.2 AdaBoost Regression</a></span></li></ul></li><li><span><a href=\"#2.9.4-Gradient-Tree-Boosting-/-Gradient-Boosted-Decision-Trees\" data-toc-modified-id=\"2.9.4-Gradient-Tree-Boosting-/-Gradient-Boosted-Decision-Trees-3.9.4\">2.9.4 Gradient Tree Boosting / Gradient Boosted Decision Trees</a></span><ul class=\"toc-item\"><li><span><a href=\"#2.9.4.1-Gradient-Boosting-Classification\" data-toc-modified-id=\"2.9.4.1-Gradient-Boosting-Classification-3.9.4.1\">2.9.4.1 Gradient Boosting Classification</a></span></li><li><span><a href=\"#2.9.4.2-Gradient-Boosting-Regression\" data-toc-modified-id=\"2.9.4.2-Gradient-Boosting-Regression-3.9.4.2\">2.9.4.2 Gradient Boosting Regression</a></span></li></ul></li><li><span><a href=\"#2.9.5-Histogram-based-Gradient-Boosting\" data-toc-modified-id=\"2.9.5-Histogram-based-Gradient-Boosting-3.9.5\">2.9.5 Histogram-based Gradient Boosting</a></span></li><li><span><a href=\"#2.9.6-投票分类/回归器-Voting-Classifier/Regressor\" data-toc-modified-id=\"2.9.6-投票分类/回归器-Voting-Classifier/Regressor-3.9.6\">2.9.6 投票分类/回归器 Voting Classifier/Regressor</a></span><ul class=\"toc-item\"><li><span><a href=\"#2.9.6.1-Voting-Classification\" data-toc-modified-id=\"2.9.6.1-Voting-Classification-3.9.6.1\">2.9.6.1 Voting Classification</a></span></li><li><span><a href=\"#2.9.6.2-Voting-Regressor\" data-toc-modified-id=\"2.9.6.2-Voting-Regressor-3.9.6.2\">2.9.6.2 Voting Regressor</a></span></li></ul></li></ul></li><li><span><a href=\"#2.10-半监督学习-sklearn.semi_supervised\" data-toc-modified-id=\"2.10-半监督学习-sklearn.semi_supervised-3.10\">2.10 半监督学习 <code>sklearn.semi_supervised</code></a></span><ul class=\"toc-item\"><li><span><a href=\"#2.10.1-标签传播算法-Label-Propagation\" data-toc-modified-id=\"2.10.1-标签传播算法-Label-Propagation-3.10.1\">2.10.1 标签传播算法 Label Propagation</a></span><ul class=\"toc-item\"><li><span><a href=\"#2.10.1.1-Label-Propagation\" data-toc-modified-id=\"2.10.1.1-Label-Propagation-3.10.1.1\">2.10.1.1 Label Propagation</a></span></li><li><span><a href=\"#2.10.1.2-Label-Spreading\" data-toc-modified-id=\"2.10.1.2-Label-Spreading-3.10.1.2\">2.10.1.2 Label Spreading</a></span></li></ul></li></ul></li><li><span><a href=\"#2.11-保序回归-sklearn.isotonic\" data-toc-modified-id=\"2.11-保序回归-sklearn.isotonic-3.11\">2.11 保序回归 <code>sklearn.isotonic</code></a></span></li><li><span><a href=\"#2.12-概率校准-sklearn.calibration\" data-toc-modified-id=\"2.12-概率校准-sklearn.calibration-3.12\">2.12 概率校准 <code>sklearn.calibration</code></a></span><ul class=\"toc-item\"><li><span><a href=\"#2.12.1-校准图/可靠性曲线-Calibration-plots-(reliability-curve)\" data-toc-modified-id=\"2.12.1-校准图/可靠性曲线-Calibration-plots-(reliability-curve)-3.12.1\">2.12.1 校准图/可靠性曲线 Calibration plots (reliability curve)</a></span></li></ul></li><li><span><a href=\"#2.13-神经网络-sklearn.neural_network\" data-toc-modified-id=\"2.13-神经网络-sklearn.neural_network-3.13\">2.13 神经网络 <code>sklearn.neural_network</code></a></span><ul class=\"toc-item\"><li><span><a href=\"#2.13.1-多层感知机分类-Multi-layer-Perceptron-Classification\" data-toc-modified-id=\"2.13.1-多层感知机分类-Multi-layer-Perceptron-Classification-3.13.1\">2.13.1 多层感知机分类 Multi-layer Perceptron Classification</a></span></li><li><span><a href=\"#2.13.2-多层感知机回归-Multi-layer-Perceptron-Regression\" data-toc-modified-id=\"2.13.2-多层感知机回归-Multi-layer-Perceptron-Regression-3.13.2\">2.13.2 多层感知机回归 Multi-layer Perceptron Regression</a></span></li></ul></li></ul></li><li><span><a href=\"#3-Unsupervised-Learning\" data-toc-modified-id=\"3-Unsupervised-Learning-4\">3 Unsupervised Learning</a></span><ul class=\"toc-item\"><li><span><a href=\"#3.1-高斯混合模型-sklearn.mixture\" data-toc-modified-id=\"3.1-高斯混合模型-sklearn.mixture-4.1\">3.1 高斯混合模型 <code>sklearn.mixture</code></a></span><ul class=\"toc-item\"><li><span><a href=\"#3.1.1-高斯混合模型的EM算法-Expecation-maximization-(EM)-algorithm-for-fitting-Guassian-Mixture-Model\" data-toc-modified-id=\"3.1.1-高斯混合模型的EM算法-Expecation-maximization-(EM)-algorithm-for-fitting-Guassian-Mixture-Model-4.1.1\">3.1.1 高斯混合模型的EM算法 Expecation-maximization (EM) algorithm for fitting Guassian Mixture Model</a></span></li><li><span><a href=\"#3.1.2-变分贝叶斯高斯混合模型-Variational-Bayesian-Gaussian-Mixture\" data-toc-modified-id=\"3.1.2-变分贝叶斯高斯混合模型-Variational-Bayesian-Gaussian-Mixture-4.1.2\">3.1.2 变分贝叶斯高斯混合模型 Variational Bayesian Gaussian Mixture</a></span></li></ul></li><li><span><a href=\"#3.1*-密度估计\" data-toc-modified-id=\"3.1*-密度估计-4.2\">3.1* 密度估计</a></span><ul class=\"toc-item\"><li><span><a href=\"#3.1*.1-核密度估计-Kernel-Density-Estimation-(KDE)\" data-toc-modified-id=\"3.1*.1-核密度估计-Kernel-Density-Estimation-(KDE)-4.2.1\">3.1*.1 核密度估计 Kernel Density Estimation (KDE)</a></span></li></ul></li><li><span><a href=\"#3.2-流形学习-sklearn.manifold\" data-toc-modified-id=\"3.2-流形学习-sklearn.manifold-4.3\">3.2 流形学习 <code>sklearn.manifold</code></a></span><ul class=\"toc-item\"><li><span><a href=\"#3.2.1-等距特征映射-Isometric-Mapping-(Isomap)\" data-toc-modified-id=\"3.2.1-等距特征映射-Isometric-Mapping-(Isomap)-4.3.1\">3.2.1 等距特征映射 Isometric Mapping (Isomap)</a></span></li><li><span><a href=\"#3.2.2-局部线性嵌入-Locally-Linear-Embedding\" data-toc-modified-id=\"3.2.2-局部线性嵌入-Locally-Linear-Embedding-4.3.2\">3.2.2 局部线性嵌入 Locally Linear Embedding</a></span></li><li><span><a href=\"#3.2.3-谱嵌入-Spectral-Embedding\" data-toc-modified-id=\"3.2.3-谱嵌入-Spectral-Embedding-4.3.3\">3.2.3 谱嵌入 Spectral Embedding</a></span></li><li><span><a href=\"#3.2.4-多维缩放-Multi-dimensional-Scaling-(MDS)\" data-toc-modified-id=\"3.2.4-多维缩放-Multi-dimensional-Scaling-(MDS)-4.3.4\">3.2.4 多维缩放 Multi-dimensional Scaling (MDS)</a></span></li><li><span><a href=\"#3.2.5-TSNE-t-distributed-Stochastic-Neighbor-Embedding\" data-toc-modified-id=\"3.2.5-TSNE-t-distributed-Stochastic-Neighbor-Embedding-4.3.5\">3.2.5 TSNE t-distributed Stochastic Neighbor Embedding</a></span></li></ul></li><li><span><a href=\"#3.3-聚类-sklearn.cluster\" data-toc-modified-id=\"3.3-聚类-sklearn.cluster-4.4\">3.3 聚类 <code>sklearn.cluster</code></a></span><ul class=\"toc-item\"><li><span><a href=\"#3.3.1-K-means\" data-toc-modified-id=\"3.3.1-K-means-4.4.1\">3.3.1 K-means</a></span><ul class=\"toc-item\"><li><span><a href=\"#3.3.1.1-K-means-clustring\" data-toc-modified-id=\"3.3.1.1-K-means-clustring-4.4.1.1\">3.3.1.1 K-means clustring</a></span></li><li><span><a href=\"#3.3.1.2-Mini-Batch-K-means-clustring\" data-toc-modified-id=\"3.3.1.2-Mini-Batch-K-means-clustring-4.4.1.2\">3.3.1.2 Mini-Batch K-means clustring</a></span></li></ul></li><li><span><a href=\"#3.3.2-吸引子传播-Affinity-Propagation\" data-toc-modified-id=\"3.3.2-吸引子传播-Affinity-Propagation-4.4.2\">3.3.2 吸引子传播 Affinity Propagation</a></span></li><li><span><a href=\"#3.3.3-均值偏移-Mean-Shift\" data-toc-modified-id=\"3.3.3-均值偏移-Mean-Shift-4.4.3\">3.3.3 均值偏移 Mean Shift</a></span></li><li><span><a href=\"#3.3.4-谱聚类-Spectral-Clustering\" data-toc-modified-id=\"3.3.4-谱聚类-Spectral-Clustering-4.4.4\">3.3.4 谱聚类 Spectral Clustering</a></span></li><li><span><a href=\"#3.3.5-层次聚类-Hierarchical-Clustering\" data-toc-modified-id=\"3.3.5-层次聚类-Hierarchical-Clustering-4.4.5\">3.3.5 层次聚类 Hierarchical Clustering</a></span></li><li><span><a href=\"#3.3.6-Density-Based-Spatial-Clustering-of-Applications-with-Noise-(DBSCAN)\" data-toc-modified-id=\"3.3.6-Density-Based-Spatial-Clustering-of-Applications-with-Noise-(DBSCAN)-4.4.6\">3.3.6 Density-Based Spatial Clustering of Applications with Noise (DBSCAN)</a></span></li><li><span><a href=\"#3.3.7-Ordering-Points-To-Identify-the-Clustering-Structure-(OPTICS)\" data-toc-modified-id=\"3.3.7-Ordering-Points-To-Identify-the-Clustering-Structure-(OPTICS)-4.4.7\">3.3.7 Ordering Points To Identify the Clustering Structure (OPTICS)</a></span></li><li><span><a href=\"#3.3.8-Balanced-Iterative-Reducing-and-Clustering-using-Hierarchies-(BIRCH)\" data-toc-modified-id=\"3.3.8-Balanced-Iterative-Reducing-and-Clustering-using-Hierarchies-(BIRCH)-4.4.8\">3.3.8 Balanced Iterative Reducing and Clustering using Hierarchies (BIRCH)</a></span></li><li><span><a href=\"#3.3.9-双聚类-Biclustering\" data-toc-modified-id=\"3.3.9-双聚类-Biclustering-4.4.9\">3.3.9 双聚类 Biclustering</a></span></li></ul></li><li><span><a href=\"#3.4-成分分解-sklearn.decomposition\" data-toc-modified-id=\"3.4-成分分解-sklearn.decomposition-4.5\">3.4 成分分解 <code>sklearn.decomposition</code></a></span><ul class=\"toc-item\"><li><span><a href=\"#3.4.1-主成分分析-Principal-component-analysis-(PCA)\" data-toc-modified-id=\"3.4.1-主成分分析-Principal-component-analysis-(PCA)-4.5.1\">3.4.1 主成分分析 Principal component analysis (PCA)</a></span><ul class=\"toc-item\"><li><span><a href=\"#3.4.1.1-Principal-component-analysis\" data-toc-modified-id=\"3.4.1.1-Principal-component-analysis-4.5.1.1\">3.4.1.1 Principal component analysis</a></span></li><li><span><a href=\"#3.4.1.2-Incremental-principal-component-analysis\" data-toc-modified-id=\"3.4.1.2-Incremental-principal-component-analysis-4.5.1.2\">3.4.1.2 Incremental principal component analysis</a></span></li><li><span><a href=\"#3.4.1.3-Kernel-PCA\" data-toc-modified-id=\"3.4.1.3-Kernel-PCA-4.5.1.3\">3.4.1.3 Kernel PCA</a></span></li><li><span><a href=\"#3.4.1.4-Sparse-PCA\" data-toc-modified-id=\"3.4.1.4-Sparse-PCA-4.5.1.4\">3.4.1.4 Sparse PCA</a></span></li><li><span><a href=\"#3.4.1.5-Mini-batch-Sparse-PCA\" data-toc-modified-id=\"3.4.1.5-Mini-batch-Sparse-PCA-4.5.1.5\">3.4.1.5 Mini batch Sparse PCA</a></span></li></ul></li><li><span><a href=\"#3.4.2-截断奇异值分解/潜在语义分析-Truncated-singular-value-decomposition-/-latent-semantic-analysis\" data-toc-modified-id=\"3.4.2-截断奇异值分解/潜在语义分析-Truncated-singular-value-decomposition-/-latent-semantic-analysis-4.5.2\">3.4.2 截断奇异值分解/潜在语义分析 Truncated singular value decomposition / latent semantic analysis</a></span></li><li><span><a href=\"#3.4.3-字典学习-Dictionary-learning\" data-toc-modified-id=\"3.4.3-字典学习-Dictionary-learning-4.5.3\">3.4.3 字典学习 Dictionary learning</a></span><ul class=\"toc-item\"><li><span><a href=\"#3.4.3.1-Generic-dictionary-learning\" data-toc-modified-id=\"3.4.3.1-Generic-dictionary-learning-4.5.3.1\">3.4.3.1 Generic dictionary learning</a></span></li><li><span><a href=\"#3.4.3.2-Mini-batch-dictionary-learning\" data-toc-modified-id=\"3.4.3.2-Mini-batch-dictionary-learning-4.5.3.2\">3.4.3.2 Mini batch dictionary learning</a></span></li></ul></li><li><span><a href=\"#3.4.4-因子分析-Factor-Analysis\" data-toc-modified-id=\"3.4.4-因子分析-Factor-Analysis-4.5.4\">3.4.4 因子分析 Factor Analysis</a></span></li><li><span><a href=\"#3.4.5-独立成分分析-Independent-component-analysis-(ICA)\" data-toc-modified-id=\"3.4.5-独立成分分析-Independent-component-analysis-(ICA)-4.5.5\">3.4.5 独立成分分析 Independent component analysis (ICA)</a></span></li><li><span><a href=\"#3.4.6-非负矩阵分解-Non-negative-matrix-factorization-(NMF-or-NNMF)\" data-toc-modified-id=\"3.4.6-非负矩阵分解-Non-negative-matrix-factorization-(NMF-or-NNMF)-4.5.6\">3.4.6 非负矩阵分解 Non-negative matrix factorization (NMF or NNMF)</a></span></li></ul></li><li><span><a href=\"#3.4*-随机映射-sklearn.random_projection\" data-toc-modified-id=\"3.4*-随机映射-sklearn.random_projection-4.6\">3.4* 随机映射 <code>sklearn.random_projection</code></a></span><ul class=\"toc-item\"><li><span><a href=\"#3.4*.1-高斯随机映射-Gaussian-random-projection\" data-toc-modified-id=\"3.4*.1-高斯随机映射-Gaussian-random-projection-4.6.1\">3.4*.1 高斯随机映射 Gaussian random projection</a></span></li><li><span><a href=\"#3.4*.2-稀疏随机映射-Sparse-random-projection\" data-toc-modified-id=\"3.4*.2-稀疏随机映射-Sparse-random-projection-4.6.2\">3.4*.2 稀疏随机映射 Sparse random projection</a></span></li></ul></li><li><span><a href=\"#3.5-异常值检测-sklearn.***\" data-toc-modified-id=\"3.5-异常值检测-sklearn.***-4.7\">3.5 异常值检测 <code>sklearn.***</code></a></span><ul class=\"toc-item\"><li><span><a href=\"#3.5.1-输入数据包含异常值-Outlier-Detection\" data-toc-modified-id=\"3.5.1-输入数据包含异常值-Outlier-Detection-4.7.1\">3.5.1 输入数据包含异常值 Outlier Detection</a></span><ul class=\"toc-item\"><li><span><a href=\"#3.5.1.1-Fitting-an-elliptic-envelope\" data-toc-modified-id=\"3.5.1.1-Fitting-an-elliptic-envelope-4.7.1.1\">3.5.1.1 Fitting an elliptic envelope</a></span></li><li><span><a href=\"#3.5.1.2-Isolation-Forest\" data-toc-modified-id=\"3.5.1.2-Isolation-Forest-4.7.1.2\">3.5.1.2 Isolation Forest</a></span></li><li><span><a href=\"#3.5.1.3-Local-Outlier-Factor\" data-toc-modified-id=\"3.5.1.3-Local-Outlier-Factor-4.7.1.3\">3.5.1.3 Local Outlier Factor</a></span></li></ul></li><li><span><a href=\"#3.5.2-输入数据不包含异常值-Novelty-Detection\" data-toc-modified-id=\"3.5.2-输入数据不包含异常值-Novelty-Detection-4.7.2\">3.5.2 输入数据不包含异常值 Novelty Detection</a></span><ul class=\"toc-item\"><li><span><a href=\"#3.5.2.1-One-Class-SVM\" data-toc-modified-id=\"3.5.2.1-One-Class-SVM-4.7.2.1\">3.5.2.1 One-Class SVM</a></span></li><li><span><a href=\"#3.5.2.2-Local-Outlier-Factor-with-novelty-set-to-True\" data-toc-modified-id=\"3.5.2.2-Local-Outlier-Factor-with-novelty-set-to-True-4.7.2.2\">3.5.2.2 Local Outlier Factor with <code>novelty</code> set to <code>True</code></a></span></li></ul></li></ul></li><li><span><a href=\"#3.6-神经网络-sklearn.neural_network\" data-toc-modified-id=\"3.6-神经网络-sklearn.neural_network-4.8\">3.6 神经网络 <code>sklearn.neural_network</code></a></span><ul class=\"toc-item\"><li><span><a href=\"#3.6.1-伯努利受限玻尔兹曼机-Bernoulli-Restricted-Boltzmann-Machine-(RBM)\" data-toc-modified-id=\"3.6.1-伯努利受限玻尔兹曼机-Bernoulli-Restricted-Boltzmann-Machine-(RBM)-4.8.1\">3.6.1 伯努利受限玻尔兹曼机 Bernoulli Restricted Boltzmann Machine (RBM)</a></span></li></ul></li></ul></li><li><span><a href=\"#4-Model-Selection-and-Evaluation-sklearn.model_selection\" data-toc-modified-id=\"4-Model-Selection-and-Evaluation-sklearn.model_selection-5\">4 Model Selection and Evaluation <code>sklearn.model_selection</code></a></span><ul class=\"toc-item\"><li><span><a href=\"#4.1-交叉验证-Cross-validation\" data-toc-modified-id=\"4.1-交叉验证-Cross-validation-5.1\">4.1 交叉验证 Cross validation</a></span><ul class=\"toc-item\"><li><span><a href=\"#4.1.1-Cross-validation-score\" data-toc-modified-id=\"4.1.1-Cross-validation-score-5.1.1\">4.1.1 Cross validation score</a></span></li><li><span><a href=\"#4.1.2-Cross-validation-function-and-multiple-metric-evaluation\" data-toc-modified-id=\"4.1.2-Cross-validation-function-and-multiple-metric-evaluation-5.1.2\">4.1.2 Cross validation function and multiple metric evaluation</a></span></li><li><span><a href=\"#4.1.3-Obtaining-predictions-by-cross-validation\" data-toc-modified-id=\"4.1.3-Obtaining-predictions-by-cross-validation-5.1.3\">4.1.3 Obtaining predictions by cross-validation</a></span></li></ul></li><li><span><a href=\"#4.2-评分指标-Scoring-metrics\" data-toc-modified-id=\"4.2-评分指标-Scoring-metrics-5.2\">4.2 评分指标 Scoring metrics</a></span><ul class=\"toc-item\"><li><span><a href=\"#4.2.1-Predefined-values\" data-toc-modified-id=\"4.2.1-Predefined-values-5.2.1\">4.2.1 Predefined values</a></span></li><li><span><a href=\"#4.2.2-Score-functions\" data-toc-modified-id=\"4.2.2-Score-functions-5.2.2\">4.2.2 Score functions</a></span></li></ul></li><li><span><a href=\"#4.3-Cross-validation-iterators\" data-toc-modified-id=\"4.3-Cross-validation-iterators-5.3\">4.3 Cross validation iterators</a></span><ul class=\"toc-item\"><li><span><a href=\"#4.3.1-For-i.i.d.-data\" data-toc-modified-id=\"4.3.1-For-i.i.d.-data-5.3.1\">4.3.1 For i.i.d. data</a></span><ul class=\"toc-item\"><li><span><a href=\"#4.3.1.1-K-fold\" data-toc-modified-id=\"4.3.1.1-K-fold-5.3.1.1\">4.3.1.1 K-fold</a></span></li><li><span><a href=\"#4.3.1.2-Repeated-K-fold\" data-toc-modified-id=\"4.3.1.2-Repeated-K-fold-5.3.1.2\">4.3.1.2 Repeated K-fold</a></span></li><li><span><a href=\"#4.3.1.3-Leave-One-Out-(LOO)\" data-toc-modified-id=\"4.3.1.3-Leave-One-Out-(LOO)-5.3.1.3\">4.3.1.3 Leave One Out (LOO)</a></span></li><li><span><a href=\"#4.3.1.4-Leave-P-Out-(LPO)\" data-toc-modified-id=\"4.3.1.4-Leave-P-Out-(LPO)-5.3.1.4\">4.3.1.4 Leave P Out (LPO)</a></span></li><li><span><a href=\"#4.3.1.5-Random-permutations-cross-validation-a.k.a.-Shuffle-&amp;-Split\" data-toc-modified-id=\"4.3.1.5-Random-permutations-cross-validation-a.k.a.-Shuffle-&amp;-Split-5.3.1.5\">4.3.1.5 Random permutations cross-validation a.k.a. Shuffle &amp; Split</a></span></li></ul></li><li><span><a href=\"#4.3.2-For-imbalance-data\" data-toc-modified-id=\"4.3.2-For-imbalance-data-5.3.2\">4.3.2 For imbalance data</a></span><ul class=\"toc-item\"><li><span><a href=\"#4.3.2.1-Stratified-K-fold\" data-toc-modified-id=\"4.3.2.1-Stratified-K-fold-5.3.2.1\">4.3.2.1 Stratified K-fold</a></span></li><li><span><a href=\"#4.3.2.2-Stratified-Shuffle-Split\" data-toc-modified-id=\"4.3.2.2-Stratified-Shuffle-Split-5.3.2.2\">4.3.2.2 Stratified Shuffle Split</a></span></li></ul></li><li><span><a href=\"#4.3.3-For-grouped-data\" data-toc-modified-id=\"4.3.3-For-grouped-data-5.3.3\">4.3.3 For grouped data</a></span><ul class=\"toc-item\"><li><span><a href=\"#4.3.3.1-Group-K-fold\" data-toc-modified-id=\"4.3.3.1-Group-K-fold-5.3.3.1\">4.3.3.1 Group K-fold</a></span></li><li><span><a href=\"#4.3.3.2-Leave-One-Group-Out\" data-toc-modified-id=\"4.3.3.2-Leave-One-Group-Out-5.3.3.2\">4.3.3.2 Leave One Group Out</a></span></li><li><span><a href=\"#4.3.3.3-Leave-P-Groups-Out\" data-toc-modified-id=\"4.3.3.3-Leave-P-Groups-Out-5.3.3.3\">4.3.3.3 Leave P Groups Out</a></span></li><li><span><a href=\"#4.3.3.4-Group-Shuffle-Split\" data-toc-modified-id=\"4.3.3.4-Group-Shuffle-Split-5.3.3.4\">4.3.3.4 Group Shuffle Split</a></span></li></ul></li><li><span><a href=\"#4.3.4-For-time-series-data\" data-toc-modified-id=\"4.3.4-For-time-series-data-5.3.4\">4.3.4 For time series data</a></span></li></ul></li><li><span><a href=\"#4.4-调参-Tuning-the-hyper-parameters\" data-toc-modified-id=\"4.4-调参-Tuning-the-hyper-parameters-5.4\">4.4 调参 Tuning the hyper-parameters</a></span></li><li><span><a href=\"#4.5-评估可视化-Evaluation-Visualization\" data-toc-modified-id=\"4.5-评估可视化-Evaluation-Visualization-5.5\">4.5 评估可视化 Evaluation Visualization</a></span><ul class=\"toc-item\"><li><span><a href=\"#4.5.1-验证曲线-Validation-curve\" data-toc-modified-id=\"4.5.1-验证曲线-Validation-curve-5.5.1\">4.5.1 验证曲线 Validation curve</a></span></li><li><span><a href=\"#4.5.2-学习曲线-Learning-curve\" data-toc-modified-id=\"4.5.2-学习曲线-Learning-curve-5.5.2\">4.5.2 学习曲线 Learning curve</a></span></li><li><span><a href=\"#4.5.3-Confusion-matrix\" data-toc-modified-id=\"4.5.3-Confusion-matrix-5.5.3\">4.5.3 Confusion matrix</a></span></li><li><span><a href=\"#4.5.4-Roc-curve\" data-toc-modified-id=\"4.5.4-Roc-curve-5.5.4\">4.5.4 Roc curve</a></span></li><li><span><a href=\"#4.5.5-Precision-recall\" data-toc-modified-id=\"4.5.5-Precision-recall-5.5.5\">4.5.5 Precision recall</a></span></li></ul></li></ul></li><li><span><a href=\"#5-Pipelines-and-Composite-Estimators\" data-toc-modified-id=\"5-Pipelines-and-Composite-Estimators-6\">5 Pipelines and Composite Estimators</a></span><ul class=\"toc-item\"><li><span><a href=\"#5.1-Pipeline-sklearn.pipeline\" data-toc-modified-id=\"5.1-Pipeline-sklearn.pipeline-6.1\">5.1 Pipeline <code>sklearn.pipeline</code></a></span><ul class=\"toc-item\"><li><span><a href=\"#5.1.1-Chaining-estimators\" data-toc-modified-id=\"5.1.1-Chaining-estimators-6.1.1\">5.1.1 Chaining estimators</a></span></li><li><span><a href=\"#5.1.2-Composite-feature-spaces\" data-toc-modified-id=\"5.1.2-Composite-feature-spaces-6.1.2\">5.1.2 Composite feature spaces</a></span></li></ul></li><li><span><a href=\"#5.2-Composite-estimators-sklearn.compose\" data-toc-modified-id=\"5.2-Composite-estimators-sklearn.compose-6.2\">5.2 Composite estimators <code>sklearn.compose</code></a></span><ul class=\"toc-item\"><li><span><a href=\"#5.2.1-Transforming-target-in-regression\" data-toc-modified-id=\"5.2.1-Transforming-target-in-regression-6.2.1\">5.2.1 Transforming target in regression</a></span></li><li><span><a href=\"#5.2.2-Transformer-for-heterogeneous-data\" data-toc-modified-id=\"5.2.2-Transformer-for-heterogeneous-data-6.2.2\">5.2.2 Transformer for heterogeneous data</a></span></li></ul></li></ul></li><li><span><a href=\"#6-Model-persistence\" data-toc-modified-id=\"6-Model-persistence-7\">6 Model persistence</a></span><ul class=\"toc-item\"><li><span><a href=\"#6.1-Python自带的方法-Python's-built-in-method\" data-toc-modified-id=\"6.1-Python自带的方法-Python's-built-in-method-7.1\">6.1 Python自带的方法 Python's built-in method</a></span><ul class=\"toc-item\"><li><span><a href=\"#6.1.1-pickle\" data-toc-modified-id=\"6.1.1-pickle-7.1.1\">6.1.1 <code>pickle</code></a></span></li><li><span><a href=\"#6.1.2-joblib\" data-toc-modified-id=\"6.1.2-joblib-7.1.2\">6.1.2 <code>joblib</code></a></span></li></ul></li><li><span><a href=\"#6.2-可用于交互操作的方式-Interoperable-formats\" data-toc-modified-id=\"6.2-可用于交互操作的方式-Interoperable-formats-7.2\">6.2 可用于交互操作的方式 Interoperable formats</a></span></li></ul></li><li><span><a href=\"#Take-advantage-of-these-tools-and-use-your-talents-to-create-a-better-world!\" data-toc-modified-id=\"Take-advantage-of-these-tools-and-use-your-talents-to-create-a-better-world!-8\">Take advantage of these tools and use your talents to create a better world!</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 测试数据 Test data\n",
    "Used for Section 1 ~ Section 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "\n",
    "## Selection function to keep a handy sample set\n",
    "def getsamples(X, y=None, size=100):\n",
    "    index = np.random.choice(np.arange(len(X)), size, replace=False)\n",
    "    X_sampled = X[index, :]\n",
    "    if y.any()!=None:\n",
    "        y_sampled = y[index]\n",
    "        return X_sampled, y_sampled\n",
    "    return X_sampled\n",
    "\n",
    "## Sample data for binary classification\n",
    "# breast_cancer = datasets.load_breast_cancer()\n",
    "# X, y = getsamples(breast_cancer.data, breast_cancer.target)\n",
    "# print(breast_cancer.feature_names)\n",
    "\n",
    "## Sample data for multi-classification (image size = 8 x 8)\n",
    "digits = datasets.load_digits(return_X_y=False, as_frame=False)\n",
    "X, y = getsamples(digits.data, digits.target)\n",
    "print(digits.target_names)\n",
    "\n",
    "## Data for regression\n",
    "# california = datasets.fetch_california_housing()\n",
    "# X, y = getsamples(california.data, california.target)\n",
    "# print(california.feature_names)\n",
    "\n",
    "## Data for clustering/neighbors\n",
    "# X, y = datasets.make_blobs(n_samples=100,\n",
    "#                            n_features=2,\n",
    "#                            centers=4,\n",
    "#                            random_state=0,\n",
    "#                            return_centers=False)\n",
    "\n",
    "print('shape of X:', X.shape)\n",
    "print('shape of y:', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, ## allowed inputs are lists, numpy arrays, scipy-sparse matrices or pandas dataframes\n",
    "                                                    test_size=0.3, # float or int # float between 0.0 and 1.0 represents the proportion while int represents the absolute number\n",
    "                                                    random_state=0, ## controls the shuffling applied to the data before applying the split\n",
    "                                                    shuffle=True) ## whether or not to shuffle the data before splitting\n",
    "# random_state pass an int for reproducible results across mltiple function calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Before Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 特征提取 `sklearn.feature_extraction`\n",
    "__Feature extraction__: transforming original data to make them usable for ML models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 字典➡特征 from dict-like objects to NumPy/SciPy representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = [{'city': 'Dubai', 'temperature': 33.},\n",
    "     {'city': 'London', 'temperature': 12., 'country': 'USA'},\n",
    "     {'city': 'San Franciso', 'temperature': 18.}]\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "dict_vectorizer = DictVectorizer(sparse=True, sort=True)\n",
    "dict_vectorizer.fit(D)\n",
    "X = dict_vectorizer.transform(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_vectorizer.feature_names_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 字符串➡稀疏矩阵（使用哈希函数）from string to sparse matrices with Hash function\n",
    "`.fit()` is invalid and no `.fit_transform()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = [{'dog': 1, 'cat': 2, 'elephant': 4}, {'dog': 2, 'run': 5}]\n",
    "\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "feature_hasher = FeatureHasher(n_features=10,\n",
    "                               input_type='dict') # or 'pair' when input is like (feature_name, value)\n",
    "X = feature_hasher.transform(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 文本特征提取 text feature extraction\n",
    "tokenizing -> counting -> normalizing\\\n",
    "The following 3 transformer are all not directly suitable for larger corpus\\\n",
    "https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.3.1 文本➡token计数矩阵 from text to token count matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['This is the first document.',\n",
    "          'This document is the second document.',\n",
    "          'And this is the third one.','Is it the first document?']\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vectorizer = CountVectorizer(input='content', #'filename', or 'file'\n",
    "                                   encoding='utf-8',decode_error='strict', #'ignore', or 'replace'\n",
    "                                   strip_accents=None,\n",
    "                                   lowercase=True,\n",
    "                                   stop_words=None, # 'english', or a list[] # 'english' uses a built-in stop word list; list[] only works when analyzer=='word'\n",
    "                                   ngram_range=(1,1), # (1,2), or (2,2) # (1,1) means only unigrams; (1,2) means unigrams and bigrams; (2,2) means only bigrams, applying if analyzer is not callable\n",
    "                                   analyzer='word', # 'char', 'char_wb', or callable\n",
    "                                   max_df=1.0, ## ignore terms whose frequency > max_df, float inidicates proportion and integer indicates counts\n",
    "                                   min_df=1, ## ignore terms whose frequency < min_df, similar with max_df\n",
    "                                   max_features=None, # or int # reserve top max_features terms ordered by frequency\n",
    "                                   vocabulary=None, # mapping, or iterable\n",
    "                                   binary=False) ## True sets non-zero counts to 1\n",
    "countmatrix = count_vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count_vectorizer.vocabulary_ # {term: feature index}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countmatrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer2 = CountVectorizer(ngram_range=(2,2))\n",
    "countmatrix2 = count_vectorizer2.fit_transform(corpus)\n",
    "print(count_vectorizer2.get_feature_names())\n",
    "print(countmatrix2.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.3.2 计数矩阵➡tf/tf-idf 频率表示 from count matrix to tf/tf-idf representation\n",
    "tf: term-frequency\\\n",
    "tf-idf: term-frequency * inverse document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer(norm='l2', ## 'l1': Σ|x|=1; 'l2': Σx^2=1\n",
    "                                     use_idf=True,\n",
    "                                     smooth_idf=True, ## idf=log((1+n)/(1+df))+1, prevents zero divisions\n",
    "                                     sublinear_tf=False) ## True replaces tf with 1+log(tf)\n",
    "tfidf_transformer.fit(countmatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer.transform(countmatrix).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer.idf_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.3.3 文本➡tf/idf频率表示 from text to tf/tf-idf representation\n",
    "equivalent to CountVectorizer followed by TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "# parameters={CountVectorizer's, TfidfTransformer's}\n",
    "tfidf_vectorizer.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer.transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4 图像特征提取 image feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_sample_image\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "image_sample = load_sample_image(\"china.jpg\")\n",
    "print(image_sample.shape)\n",
    "plt.imshow(image_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.4.1 Patch提取 patch extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import image\n",
    "patches = image.extract_patches_2d(image_sample, ## shape of (image_height, image_width) or (image_height, image_width, n_channels)\n",
    "                                   (2, 2), ## patch_size=(path_height, patch_width)\n",
    "                                   max_patches=None, # or int # the maximum number of patches to extract and float indicates proportion\n",
    "                                   random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.4.2 图像的连接图 connectivity graph\n",
    "sklearn.feature_extraction.image.img_to_graph\\\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.image.img_to_graph.html#sklearn.feature_extraction.image.img_to_graph \\\n",
    "sklearn.feature_extraction.image.grid_to_graph\\\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.image.grid_to_graph.html#sklearn.feature_extraction.image.grid_to_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 数据预处理 `sklearn.preprocessing`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 数据标准化（对每列）Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1.1 标准化 z = (x - u) / s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-off, accept 1-D data\n",
    "from sklearn import preprocessing\n",
    "X_scaled = preprocessing.scale(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facilitate standardization of new data: scaler.transform(X_new)\n",
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler().fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1.2 最小最大标准化 z = (x - min) / (max - min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-off\n",
    "from sklearn import preprocessing\n",
    "preprocessing.minmax_scale(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facilitate standardization of new data: min_max_scaler.transform(X_new)\n",
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_minmax = min_max_scaler.fit_transform(X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_minmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1.3 最大绝对值标准化 z = x / max(abs(x)) \n",
    "由于不移动数据中心，常用于sparse data的标准化\\\n",
    "Because the data center is not moved, it is often used for the standardization of sparse data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-off\n",
    "from sklearn import preprocessing\n",
    "preprocessing.maxabs_scale(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facilitate standardization of new data: max_abs_scaler.transform(X_new)\n",
    "from sklearn import preprocessing\n",
    "max_abs_scaler = preprocessing.MaxAbsScaler()\n",
    "X_maxabs = max_abs_scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_maxabs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1.4 稳健标准化 IQR = Q3 - Q1; z = (x - median) / IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-off\n",
    "from sklearn import preprocessing\n",
    "preprocessing.robust_scale(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facilitate standardization of new data: robust_scaler.transform(X_new)\n",
    "from sklearn import preprocessing\n",
    "robust_scaler = preprocessing.RobustScaler(with_centering=True).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robust_scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 非线性变换（对每列）Non-linear transformation\n",
    "将数据变换为正态分布或均匀分布，以减小极端值对分析的影响。\\\n",
    "The data is converted to a normal or uniform distribution to reduce the impact of extreme values.\\\n",
    "✔ 可能应用于需要正态分布假设的情景\\\n",
    "might apply to situations that require a normal distribution assumption\\\n",
    "❗ 分类任务/聚类任务中会改变数据的分布形态，可能会有负面影响\\\n",
    "the distribution pattern of data might change in classification /clustering tasks, which may have negative effects\\\n",
    "❗ 不知道在其他任务中会不会有不好影响\\\n",
    "not sure about possible bad effects in other tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2.1 Quantile transformer 变换为0-1均匀分布或标准正态分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-off\n",
    "from sklearn import preprocessing\n",
    "preprocessing.quantile_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facilitate standardization of new data\n",
    "from sklearn import preprocessing\n",
    "quantile_transformer = preprocessing.QuantileTransformer(n_quantiles=1000,\n",
    "                                                         output_distribution='uniform', # or 'normal'\n",
    "                                                         ignore_implicit_zeros=False, ## True applies to sparse matrices\n",
    "                                                         random_state=0)\n",
    "X_trans = quantile_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2.2 Power transformer 变换为高斯分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "power_transformer = preprocessing.PowerTransformer(method='yeo-johnson',# or 'box-cox' # 'box-cox' only works with positive values\n",
    "                                    standardize=True)\n",
    "X_trans = power_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power_transformer.lambdas_ # lambdas_ are parameters used in the transformation, determined by the maximum likelihood method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 数据归一化（对每行）Normalization\n",
    "p-norm = (Σx^p)^(1/p); z = x / p-norm\n",
    "\n",
    "可用于sparse input，推荐用CSR表示法\\\n",
    "accept sparse input and CSR representation is recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-off\n",
    "from sklearn import preprocessing\n",
    "preprocessing.normalize(X, norm='l2', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facilitate standardization of new data\n",
    "from sklearn import preprocessing\n",
    "normalizer = preprocessing.Normalizer(norm='l2').fit(X) # norm='l1','l2',or 'max'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4 类别特征编码（对列）Encoding categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [['male', 'from America', 'uses Safari'],\n",
    "     ['female', 'from Europe', 'uses Firefox'],\n",
    "     ['male', 'from Aisa', 'uses Chrome'],\n",
    "     ['female', 'from Africa', 'uses Safari']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.4.1 编码为有序数字 Ordinal encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "ordenc = OrdinalEncoder(categories='auto', # or an array describing expected order\n",
    "                        dtype=np.float64)\n",
    "ordenc.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordenc.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordenc.inverse_transform([[0, 1, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordenc.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.4.2 编码为二元变量 One-of-K encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# define categories\n",
    "genders = ['female', 'male']\n",
    "locations = ['from Africa', 'from Aisa', 'from Europe', 'from America']\n",
    "browsers = ['uses Chrome', 'uses Firefox', 'uses Safari']\n",
    "ohenc = OneHotEncoder(categories=[genders, locations, browsers])\n",
    "ohenc.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing categorical features\n",
    "ohenc = OneHotEncoder(handle_unknown='ignore', # or 'error'\n",
    "                      sparse=False)\n",
    "ohenc.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohenc.transform([['male', 'from Aisa', 'uses IE']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohenc.inverse_transform([[0., 1., 0., 1., 0., 0., 0., 0., 0.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohenc.get_feature_names(['gender', 'Continent', 'Browser'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avoid co-linearity\n",
    "drop_ohenc = OneHotEncoder(drop='first', # 'if_binary', or an array-like\n",
    "                      dtype=np.float)\n",
    "drop_ohenc.fit(X)\n",
    "drop_ohenc.transform(X).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.5 数据离散化（对列）Discretization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.5.1 K-bins discretization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "kbins_discretizer = preprocessing.KBinsDiscretizer(n_bins=5*[3]+5*[5], # or int\n",
    "                                                   encode='ordinal', #'onehot', or 'onehot-dense'\n",
    "                                                   strategy='uniform') #'quantile', or 'kmeans'\n",
    "kbins_discretizer.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kbins_discretizer.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.5.2 Binarization x = 0: x ≤ threshold\n",
    "可用于sparse input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-off\n",
    "from sklearn import preprocessing\n",
    "preprocessing.binarize(X, threshold=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facilitate standardization of new data\n",
    "from sklearn import preprocessing\n",
    "binarizer = preprocessing.Binarizer(threshold=0).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binarizer.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.6 生成多项式特征 Generating polynomial features\n",
    "增加特征复杂度\\\n",
    "add complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "poly_features = preprocessing.PolynomialFeatures(degree=2,\n",
    "                                                 interaction_only=False, # True excludes x^degree\n",
    "                                                 include_bias=True)\n",
    "poly_features.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.7 标签编码和离散化 Label encoding and discretization\n",
    "Transform the prediction target(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.7.1 Label binarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "label_binarizer = preprocessing.LabelBinarizer(neg_label=0,\n",
    "                                               pos_label=1,\n",
    "                                               sparse_output=False)\n",
    "label_binarizer.fit(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_binarizer.transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_binarizer.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.7.2 Label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "label_encoder.fit([12, 23, 25, 36, 28]) ## [numbers/strings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder.transform([12, 36])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder.inverse_transform([0, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.8 建立转换器 function➡transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "mytransformer = preprocessing.FunctionTransformer(func=np.log1p, ## log1p(X)=log(1+x)\n",
    "                                                  #inverse_func=,\n",
    "                                                  validate=False, ## True converts input to a 2D numpy array or sparse matrix\n",
    "                                                  check_inverse=True,\n",
    "                                                  #kw_args={}, ## dictionary of additional keyword arguments to pass to func\n",
    "                                                  #inv_kw_args={}, ## dictionary of additional keyword arguments to pass to inverse_func\n",
    "                                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mytransformer.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 缺失值插补 `sklearn.impute`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[np.nan, 2, 3], [4, np.nan, 6], [10, np.nan, 9]]\n",
    "import pandas as pd\n",
    "df = pd.DataFrame([[\"a\", \"x\"], [np.nan, \"y\"], [\"a\", np.nan], [\"b\", \"y\"]], dtype=\"category\")\n",
    "print('X:', X)\n",
    "print('----df-------------------')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 单变量插补 Univariate feature imputation\n",
    "accept sparse input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import impute\n",
    "simple_imputer = impute.SimpleImputer(missing_values=np.nan, # number, string, or None\n",
    "                                      strategy='mean', #'median', 'most_frequent'(also supports string) or 'constant'(also supports string)\n",
    "                                      fill_value=None, ## when strategy='constant' fill_value replaces missing_values\n",
    "                                      add_indicator=False)\n",
    "simple_imputer.fit([[7, 2, 3], [4, np.nan, 6], [10, 5, 9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_imputer.transform(X) # 按照 fit()得到的strategy插补到 X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 多变量插补 Multivariate feature imputation\n",
    "以缺失值所在列为y其他列为X，通过回归预测缺失值\\\n",
    "predicting missing value through regression\\\n",
    "在scikit-learn 0.23中仍在实验，不是稳定功能，参见: \\\n",
    "still experimental in scikit-learn 0.23, referring:\\\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html#sklearn.impute.IterativeImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3 最近邻插值 Nearest neighbors imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "knn_imputer = KNNImputer(missing_values=np.nan, # number, string, or None\n",
    "                         n_neighbors=5,weights='uniform', # 'distance'(weighted by the inverse of their distance), or callable\n",
    "                         metric='nan_euclidean') # or callable\n",
    "X_imputed = knn_imputer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.4 缺失值定位 Marking imputed values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import MissingIndicator\n",
    "indicator = MissingIndicator(missing_values=np.nan, # number, string, or None\n",
    "                             features='all', # or 'missing-only'\n",
    "                             sparse='auto')\n",
    "missing_values = indicator.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when using in a pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.impute import SimpleImputer, MissingIndicator\n",
    "transformer = FeatureUnion(transformer_list=[('features', SimpleImputer(strategy='mean')),\n",
    "                                             ('indicators', MissingIndicator())])\n",
    "transformer = transformer.fit(X)\n",
    "X_imputed = transformer.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 特征选择 `sklearn.feature_selection`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.1 方差过滤 Remove features whose variance doesn't meet some threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "selector = VarianceThreshold() ## threshold, zero by default\n",
    "selector.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('shape of X: ', X.shape)\n",
    "print('shape of filtered X: ', selector.transform(X).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector.variances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.2 单变量特征选择 Univariate feature selection\n",
    "Univariate feature selection selects the best (several) features based on univariate statistical tests.\n",
    "\n",
    "\\* can deal with sparse data without making it dense\n",
    "\n",
    "__callable for regression__\n",
    "- `f_regression`:\\\n",
    "step 1. correlation between each regressor and the target\\\n",
    "step 2. converted to an F score then to a p-value \\* F=corr^2/(1-corr^2)\n",
    "- `mutual_info_regression`\\*:\\\n",
    "estimate mutual information, which measures the dependency between involved variables, based on entropy estimation from k-nearest neighbors distances\\\n",
    "\n",
    "__callable for classification__\n",
    "- `chi2`\\*: \\\n",
    "chi-squared stats between each non-negative feature and class, measuring dependency between one feature and the class \\* chi square usually is used to measure difference between observation and prediction\n",
    "- `f_classif`: \\\n",
    "ANOVA F-value between each feature and class \\* measure difference between two sets, theoretically can be used with both continuous and discrete data\n",
    "- `mutual_info_classif`\\*:\\\n",
    "estimate mutual information, similar with mutual_info_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.2.1 According to the k highest scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2 \n",
    "selector = SelectKBest(chi2, k=10) ## default score_func is f_classif, k=int or 'all'\n",
    "X_new = selector.fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('shape of X: ', X.shape)\n",
    "print('shape of selected X: ', X_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('scores: \\n',selector.scores_)\n",
    "print('pvalues: \\n',selector.pvalues_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.2.2 According to a percentile of the highest scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectPercentile, chi2\n",
    "X_new = SelectPercentile(chi2, percentile=10).fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('shape of X: ', X.shape)\n",
    "print('shape of selected X: ', X_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.2.3 According to p-values for FPR\n",
    "FPR: false positive rate, rate of accepting the false hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFpr, chi2\n",
    "X_new = SelectFpr(chi2, alpha=0.01).fit_transform(X, y)\n",
    "# alpha is the highest probability for features to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('shape of X: ', X.shape)\n",
    "print('shape of selected X: ', X_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.2.4 According to p-values for FDR\n",
    "FDR: false discovery rate, rate of rejecting the true hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFdr, chi2\n",
    "X_new = SelectFdr(chi2, alpha=0.01).fit_transform(X, y)\n",
    "# alpha is the highest uncorrected probability for features to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('shape of X: ', X.shape)\n",
    "print('shape of selected X: ', X_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.2.5 According to p-values for FWE\n",
    "FWE: family-wise error rate, overall (rather than individual-followed-general) rate of rejecting the true hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFwe, chi2\n",
    "X_new = SelectFwe(chi2, alpha=0.01).fit_transform(X, y)\n",
    "# alpha is the highest uncorrected probability for features to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('shape of X: ', X.shape)\n",
    "print('shape of selected X: ', X_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.2.6 单变量特征选择器 Univariate feature selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import GenericUnivariateSelect, chi2\n",
    "selector = GenericUnivariateSelect(chi2, \n",
    "                                   mode='k_best', # 'fpr', 'fdr', or 'fwe'\n",
    "                                   param=20) # int, or float corresponding with mode\n",
    "X_new = selector.fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('shape of X: ', X.shape)\n",
    "print('shape of selected X: ', X_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.3 递归式特征消除 Recursive feature elimination\n",
    "Recursive feature elimination selects features by recursively considering smaller and smaller sets of features until the desired number of features to select is reached.\\\n",
    "\\* Allows NaN/Inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVC\n",
    "estimator = SVC(kernel='linear', C=1) # an instance\n",
    "selector = RFE(estimator, n_features_to_select=25, step=1) ## step corresponds to the number of features to remove at each iteration, float step indicates percentage\n",
    "selector = selector.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "selector.ranking_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validated selection of the best number of features\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.svm import SVR\n",
    "estimator = SVR(kernel='linear')\n",
    "selector = RFECV(estimator, step=1, min_features_to_select=5, cv=5) \n",
    "# cv can be int or cv generator\n",
    "# refer https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html#sklearn.feature_selection.RFECV\n",
    "selector.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector.get_support()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.4 元转换器 SelectFromModel\n",
    "Build a meta-transformer that can be used along with any estimator that has attribute `coef_` or `feature_importances_` after fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "selector = SelectFromModel(estimator=LogisticRegression(max_iter=5000), ## an instance\n",
    "                           threshold=None, # string, or float # greater or equal are kept\n",
    "                           prefit=False).fit(X, y) ## True requires transform being called directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector.transform(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector.estimator_.coef_.shape # only when prefit is False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector.get_support()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Supervised Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 线性模型 `sklearn.linear_model`\n",
    "$\\hat{y} (w, x)=w_0+w_1x_1+\\cdots +w_px_p$\\\n",
    "`.coef_`: $w=(w_1,\\dots ,w_p)$\\\n",
    "`.intercept_`: $w_0$\\\n",
    "\\* feature independence is important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 最小二乘 Ordinary Least Squares\n",
    "$\\min_{w} || X w - y||_2^2$\n",
    "\n",
    "minimizing the residual sum of squares between the observed targets and predicted targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "linear_reg = LinearRegression(fit_intercept=True,\n",
    "                              normalize=False, ## ignored when fit_intercept is set to False, *not standardize\n",
    "                              n_jobs=None) # or int # parallel method to speedup for large problems, computations are partitioned into several jobs and run on several cores of the machine; refer https://scikit-learn.org/stable/glossary.html#term-n-jobs\n",
    "linear_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_reg.score(X_train, y_train) # R square，R^2=SSR/SST; a larger R^2 indicates a better prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_reg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 岭回归与分类 Ridge regression and classification\n",
    "$\\min_{w} || X w - y||_2^2 + \\alpha ||w||_2^2$\n",
    "\n",
    "minimizing a coefficient-penalized residual sum of squares, equivalent to ordinary least squares with l2 regularization\\\n",
    "$\\alpha$ controls the amount of shrinkage: the larger the value of $\\alpha$, the coeffients become more robust to collinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2.1 Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge_reg = Ridge(alpha=1., max_iter=None,\n",
    "                  tol=1e-3, ## precision of the solution\n",
    "                  solver='auto', # 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', or 'saga'\n",
    "                  random_state=None) ## used when solver=='sag' or 'saga'\n",
    "ridge_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_reg.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ridge_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2.2 Ridge classification\n",
    "Classifier using Ridge regression.\n",
    "1) convert target values into {-1, 1}\\\n",
    "2) treat the problem as a regression task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "ridge_clf = RidgeClassifier(class_weight=None).fit(X_train, y_train) ## class_weight=dict{class_label: weight} or 'balanced'(inversely proportional to class frequencies)\n",
    "# other parameters are like Ridge's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2.3 Cross-validated ridge regression\n",
    "Ridge regression with built-in-cross-validation(leave-one-out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "ridge_reg_cv = RidgeCV(alphas=[0.1, 1.0, 10.0], ## array of alpha values to try\n",
    "                       scoring=None, # string, or callable # None indicate negative mean squared error if cv=None/int, r2 score otherwise\n",
    "                       cv=None, # int, cross-validation generator, or an iterable # None: leave-one-out; int: certain number of folds\n",
    "                       gcv_mode='auto', # 'svd', or 'eigen' # strategy used to perform cross-validation\n",
    "                       store_cv_values=True) ## if False then alpha will not be stored in the cv_values_\n",
    "# other parameters are like Ridge's\n",
    "ridge_reg_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_reg_cv.cv_values_ # mean squared errors for each alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_reg_cv.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 Lasso\n",
    "$\\min_{w} { \\frac{1}{2n_{\\text{samples}}} ||X w - y||_2 ^ 2 + \\alpha ||w||_1}$\n",
    "\n",
    "Linear model trained with L1 prior as regularizer, preferring solutions with fewer non-zero coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3.1 Lasso model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "lasso_reg = Lasso(alpha=1.0,\n",
    "                  precompute=False, # 'auto', bool, or array-like # precomputed Gram matrix to speed up caculations\n",
    "                  max_iter=1000,\n",
    "                  tol=1e-4,\n",
    "                  warm_start=False, ## whether reuse the solution of the previous call to fit as initialization\n",
    "                  positive=False, ## True forces the coefficients to be positive\n",
    "                  selection='cyclic', # or 'random' # the way of which coefficient is updated in each iteration\n",
    "                  random_state=None) ## used when selection=='random'\n",
    "lasso_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_reg.sparse_coef_.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lasso_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3.2 Cross-validated Lasso model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "lasso_reg_cv = LassoCV(eps=1e-3, ## length of the path, alpha_min/alpha_max=eps\n",
    "                       n_alphas=100, ## number of alphas along the regularization path\n",
    "                       alphas=None, # array, or list of alphas # if None then alphas are set automatically\n",
    "                       cv=None) # int, cv generator, or iterable # None uses default 5-fold cross-validation\n",
    "# other parameters are like Lasso's\n",
    "lasso_reg_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lasso_reg_cv.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_reg_cv.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3.3 Multi-task Lasso\n",
    "Estimating sparse coeffients for multiple regression problems jointly: `y` is a 2D array of shape `(n_samples, n_tasks)`.\\\n",
    "Please refer https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.MultiTaskLasso.html#sklearn.linear_model.MultiTaskLasso \\\n",
    "To see cross-validated multi-task Lasso, refer https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.MultiTaskLassoCV.html#sklearn.linear_model.MultiTaskLassoCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4 逻辑回归 Logistic Regression/Log-it Regression \n",
    "\\* regularization is applied by default\\\n",
    "\\* accept sparse input (using CSR representation for optimal performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.4.1 Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logistic_reg = LogisticRegression(penalty='l2', # 'l1', 'elasticnet', or 'none' \n",
    "                                  # 'newton-cg', 'sag', and 'lbfgs' solvers support only 'l2' penalties\n",
    "                                  # 'elasticnet' is only supported by the 'saga' solver\n",
    "                                  C=1.0, # float # inverse of regularization strength (smaller values specify stronger regularization)\n",
    "                                  class_weight=None, # dict{class_label: weight}, or 'balanced'\n",
    "                                  solver='lbfgs', # 'netwon-cg', 'liblinear', 'sag', or 'saga'\n",
    "                                  max_iter=50, # int # maximum number of iterations to converge\n",
    "                                  multi_class='auto', # 'ovr', or 'multinomial'\n",
    "                                  random_state=None) ## used when solver=='sag', 'saga', or 'liblinear'\n",
    "logistic_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_reg.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_reg.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.4.2 Cross-validated logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "logistic_reg_cv = LogisticRegressionCV(Cs=10, # int, or list of floats # each C used in validations\n",
    "                                       cv=None, # int, or cross-validation generator\n",
    "                                       refit=True) ## whether or not average scores \n",
    "# other parameters are like LogisticRegression's\n",
    "logistic_reg_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_reg_cv.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_reg_cv.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.5 广义线性回归 Generalized Linear Regression\n",
    "With target values $y$ subjecting to normal distribution, `Ridge` and above function are generally appropriate.\n",
    "\n",
    "If the target values $y$ are counts or relative frequencies, you might use `PoissonRegressor` with log-link or `TweedieRegressor(power=1, link='log)`. \n",
    "\n",
    "If the target values are positive valued and skewed, you might try `GammaRegressor` with log-link or `TweedieRegressor(power=2, link='log')`. \n",
    "\n",
    "If the target values seem to be heavier tailed than a Gamma distribution, you might try an Inverse Gaussian deviance, `TweedieRegressor(power=3 or higher)`.\n",
    "\n",
    "`PoissonRegressor`: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.PoissonRegressor.html#sklearn.linear_model.PoissonRegressor\n",
    "\n",
    "`GammaRegressor` : https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.GammaRegressor.html#sklearn.linear_model.GammaRegressor\n",
    "\n",
    "`TweedieRegressor` : https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.TweedieRegressor.html#sklearn.linear_model.TweedieRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.6 随机梯度下降 Stochastic Gradient Descent\n",
    "A efficient way for large number of samples/features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.6.1 SGD regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "sgd_reg = SGDRegressor(loss='squared_loss', # 'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'\n",
    "                       penalty='l2', # 'l1', or 'elasticnet'\n",
    "                       alpha=0.0001, # float # higher alpha means stronger regularization\n",
    "                       shuffle=True, ## whether or not shuffling traning data after each epoch \n",
    "                       learning_rate='invscaling', # 'constant', 'optimal', or 'adaptive'\n",
    "                       n_iter_no_change=5, # int # number of iteration with no improvement to wait before early stopping\n",
    "                       random_state=None) ## used when shuffle==True\n",
    "# other parameters support complex adjustment\n",
    "sgd_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_reg.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.6.2 SGD classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "sgd_clf = SGDClassifire().fit(X_train, y_train) # parameters are the same with SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.7 多项式回归 Polynomial regression with pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "model = Pipeline([('poly', PolynomialFeatures(degree=3)),\n",
    "                  ('linear', LinearRegression())])\n",
    "model = model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.named_steps['linear'].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 判别分析 `sklearn.discriminant_analysis`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 线性判别分析 Linear discriminant analysis\n",
    "A classifier with a linear decision boundary, generated by fitting class conditional densities to the data, assuming that all classes share the same covariance matrix, and using Bayes’ rule.\n",
    "\n",
    "\\* Also used for __Dimensionality Reduction__ by projecting input to the most discriminative directions, with `transform()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "lda_clf = LinearDiscriminantAnalysis(solver='svd', # 'svd': Singular value decomposition, recommended for data with a large number of features.\n",
    "                                                   # 'lsqr': Least squares solution, can be combined with shrinkage\n",
    "                                                   # 'eigen': Eigenvalue decomposition, can be combined with shrinkage\n",
    "                                     shrinkage=None) # 'auto', or float # works only with solver='lsqr' and solver='eigen'\n",
    "lda_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_clf.explained_variance_ratio_ # Only available when eigen or svd solver is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_clf.transform(X_train) # reducing dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 二次判别分析 Quadratic discriminant analysis\n",
    "A classifier similar with Linear discriminant analysis but with a quadratic decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "qda_clf = QuadraticDiscriminantAnalysis().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qda_clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 支持向量机 `sklearn.svm`\n",
    "__Advantages__\n",
    "- effective when number of dimensions is greater than the number of samples\n",
    "- memory efficient\n",
    "- different Kernel functions can be specified\n",
    "\n",
    "__Disadvantages__\n",
    "- If the number of features is much greater than the number of samples, avoid over-fitting in choosing Kernel functions and regularization term is crucial.\n",
    "- SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation.\n",
    "\n",
    "\\* Data standardization is highly recommended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 支持向量分类 Support Vector Classification\n",
    "It may be very slow to deal with beyond tens of thousands of samples. For large datasets consider using `sklearn.svm.LinearSVC` or `sklearn.linear_model.SGDClassifier` instead.\n",
    "\n",
    "\\* unbalanced problems can be addressed by parameters `class_weight` and `sample_weight`, refer https://scikit-learn.org/stable/modules/svm.html#unbalanced-problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1.1 SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc_clf = SVC(C=1.0, # positive float # strength of the regularization is inversely proportional to C\n",
    "              kernel='rbf', # 'linear', 'poly', 'sigmoid', or 'precomputed'\n",
    "              gamma='scale', # 'auto', or float # kernel cofficient for 'rbf', 'poly', and 'sigmoid'\n",
    "              degree=3, # int # degree of the polynomial kernel function ('poly'), ignored by all other kernels\n",
    "              shrinking=True, ## whether or not to use the shrinking heuristic\n",
    "              cache_size=200, # float #the size of the kernel cache (in MB)\n",
    "              decision_function_shape='ovr', # or 'ovo' # 'ovr': one-vs-rest; 'ovo': one-vs-one; ignored for binary classification\n",
    "              random_state=None)\n",
    "svc_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_clf.support_ # indices of support vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_clf.support_vectors_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1.2 NuSVC\n",
    "Similar to SVC but introduces a parameter to control the number of support vectors and margin errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import NuSVC\n",
    "nusvc_clf = NuSVC(nu=0.5) # float in (0, 1] # an upper bound on the fraction of margin errors and a lower bound of the fraction of support vectors\n",
    "# other parameters are the same with SVC's\n",
    "nusvc_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nusvc_clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nusvc_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1.3 LinearSVC\n",
    "Similar to SVC with kernel='linear', but more flexible in the choice of penalties and loss functions, thus better with a larger number of samples.\n",
    "\n",
    "\\* accept sparse input\\\n",
    "\\* multiclass support is handled according to one-vs-rest scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "linear_svc_clf = LinearSVC(penalty='l2', # or 'l1'\n",
    "                           loss='squared_hinge') # or 'hinge'\n",
    "# other parameters are like SVC's\n",
    "linear_svc_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svc_clf.coef_ # no attributes such as .support_ or .support_vectors_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svc_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 支持向量回归 Support Vector Regression\n",
    "Extended from SVC, SVR similarly depends only on a subset of the training data by cost function ignoring samples whose prediction is close to their target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2.1 SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "svr_reg = SVR(C=1.0, # positive float # strength of the regularization is inversely proportional to C\n",
    "              kernel='rbf', # 'linear', 'poly', 'sigmoid', or 'precomputed'\n",
    "              gamma='scale', # 'auto', or float # kernel cofficient for 'rbf', 'poly', and 'sigmoid'\n",
    "              degree=3, # int # degree of the polynomial kernel function ('poly'), ignored by all other kernels\n",
    "              epsilon=0.1, # float # epsilon in the epsilon-SVR model\n",
    "              shrinking=True, ## whether or not to use the shrinking heuristic\n",
    "              cache_size=200, # float #the size of the kernel cache (in MB)\n",
    "              )\n",
    "svr_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_reg.support_ # indices of support vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_reg.support_vectors_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_reg.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2.2 NuSVR\n",
    "Similar to NuSVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import NuSVR\n",
    "nusvr_reg = NuSVR(nu=0.5) # float in (0, 1] # an upper bound on the fraction of margin errors and a lower bound of the fraction of support vectors\n",
    "# other parameters are the same with SVR's\n",
    "nusvr_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nusvr_reg.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nusvr_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2.3 LinearSVR\n",
    "Similar to SVR with kernel='linear', but more flexible in the choice of penalties and loss functions, thus better with a larger number of samples.\n",
    "\n",
    "\\* accept sparse input\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVR\n",
    "linear_svr_reg = LinearSVR(epsilon=0.0, # float # different from SVR or NuSVR, here epsilon is in the epsilon_insensitive loss function, if unsure, set 0 to it\n",
    "                           loss='epsilon_insensitive') # or 'squared_epsilon_insensitive'\n",
    "# other parameters are like SVR's\n",
    "linear_svr_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svr_reg.coef_ # no such attributes as .support_ or .support_vectors_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "linear_svr_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3* 核近似 sklearn.kernel_approximation\n",
    "Feature functions in this submodule perform non-linear transformations of the input, which can serve as a basis for linear classification or other algorithms.\n",
    "\n",
    "Compared to the kernel trick, these explicit mappings suit online learning better and can reduce the cost of learning with very large datasets, including `AdditiveChi2Sampler`, `Nystroem`, `RBFSampler`, and `SkewedChi2sampler`.\n",
    "\n",
    "Please refer https://scikit-learn.org/stable/modules/kernel_approximation.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 最近邻 `sklearn.neighbors`\n",
    "Find a certain number of training samples closest in distance to the new point, and predict the label from these.\n",
    "\n",
    "For details of nearest neighbor algorithms refer <https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbor-algorithms>\n",
    "\n",
    "\\* For sparse matrices, only Minkowski metrics are supported for searches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1 无监督最近邻 Unsupervised Nearest Neighbors\n",
    "Unsupervised learner for implementing neighbor searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "neighbor = NearestNeighbors(n_neighbors=5, ## number of neighbors to get\n",
    "                            radius=1.0, # float # limiting distance of neighbors to return\n",
    "                            algorithm='auto', # 'ball_tree', 'kd_tree', or 'brute'\n",
    "                            leaf_size=30, # int # passed to BallTree or KDTree, affecting speed and memory required\n",
    "                            metric='minkowski', # str or callable # the distance metric to use\n",
    "                            p=2, # int # parameter of the Minkowski metric, if p=1 then metric is equivalent to manhattan distance, if p=2 then metric is equivalent to euclidean distance\n",
    "                            n_jobs=None) ## number of parallel jobs to run, n_jobs=-1 uses all cores available on the machine\n",
    "neighbor.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbor.kneighbors([[5, 19]], 3, return_distance=True) # return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbrs = neighbor.radius_neighbors([[5, 19]], 11, return_distance=False) # return indices\n",
    "np.asarray(nbrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 最近邻分类 Nearest Neighbors Classification\n",
    "Classified by a simple majority vote of the nearest neighbors of each point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.2.1 K-nearest neighbors classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=5, ## number of neighbors to use for each query\n",
    "                               weights='uniform', # 'distance', or callable # weight function used in prediction/vote, 'distance' means weighed by the inverse of distance (closer neighbors have greater impacts)\n",
    "                               algorithm='auto', # 'ball_tree', 'kd_tree', or 'brute'\n",
    "                               leaf_size=30, # int # passed to BallTree or KDTree, affecting speed and memory required\n",
    "                               metric='minkowski', # str or callable # the distance metric to use\n",
    "                               p=2, # int # parameter of the Minkowski metric, if p=1 then metric is equivalent to manhattan distance, if p=2 then metric is equivalent to euclidean distance\n",
    "                               n_jobs=None) ## number of parallel jobs to run, n_jobs=-1 uses all cores available on the machine\n",
    "knn_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.2.2 Radius-based neighbors classification\n",
    "In cases where the data is not uniformly sampled, radius-based neighbors classification in RadiusNeighborsClassifier can be a better choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import RadiusNeighborsClassifier\n",
    "radius_knn_clf = RadiusNeighborsClassifier(radius=1.0, # float # limiting distance of neighbors\n",
    "                                           outlier_label=None) # 'most_frequent', or manual label # label for samples with no neighbors in given radius\n",
    "# other parameters are the same with KNeighborsClassifier's\n",
    "radius_knn_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.3 最近邻回归 Nearest Neighbors Regression\n",
    "The continuous label assigned to a query point is computed based on the mean of the labels of its nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.3.1 K-nearest neighbors regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "knn_reg = KNeighborsRegressor().fit(X_train, y_train)\n",
    "# the same parameters with KNeighborsClassifier's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_reg.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.3.2 Radius-based neighbors regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import RadiusNeighborsRegressor\n",
    "radius_knn_reg = RadiusNeighborsRegressor().fit(X_train, y_train)\n",
    "# the same parameters with RadiusNeighborsClassifier but no outlier_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_reg.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "knn_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.4 最近质心分类 Nearest Centroid Classification\n",
    "Each class is represented by its centroid, with test samples classified to the class with the nearest centroid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestCentroid\n",
    "nearest_centroid_clf = NearestCentroid().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_centroid_clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nearest_centroid_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 高斯过程 `sklearn.gaussian_process`\n",
    "Gaussian Processes (GP) are a generic supervised learning method designed to solve _regression_ and _probabilistic classification_ problems.\n",
    "\n",
    "Advantages:\n",
    "- the prediction interpolates the observations (at least for regular kernels)\n",
    "- the prediction is probabilistic so that one can compute empirical confidence intervals\n",
    "- different kernels can be specified\n",
    "\n",
    "Disadvantages:\n",
    "- not sparse, i.e., it uses the whole samples/features information to perform the prediction\n",
    "- efficiency loss in high dimensional spaces – namely when the number of features exceeds a few dozens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.1 高斯过程回归 Gaussian Process Regression (GPR)\n",
    "In addition to standard scikit-learn estimator API, GaussianProcessRegressor:\n",
    "- allows prediction without prior fitting (based on the GP prior)\n",
    "- provides an additional method sample_y(X), which evaluates samples drawn from the GPR (prior or posterior) at given inputs\n",
    "- exposes a method log_marginal_likelihood(theta), which can be used externally for other ways of selecting hyperparameters, e.g., via Markov chain Monte Carlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel\n",
    "kernel = DotProduct() + WhiteKernel()\n",
    "gpr_reg = GaussianProcessRegressor(kernel=kernel, ## the kernel specifies the covariance function of the GP, note that the kernel's hyperparameters are optimized during fitting\n",
    "                                   alpha=1e-10, # float or array-like of shape(n_samples) # larger values correspond to increased noise level; equivalent to adding a WhiteKernel with c=alpha\n",
    "                                   optimizer='fmin_l_bfgs_b', # or callable # externally defined optimizer should be like the below box\n",
    "                                   n_restarts_optimizer=0, # int # the number of restarts of the optimizer for finding the kernel's parameters which maximize the log-marginal-likelihood\n",
    "                                   # n_restarts_optimizer==0 implies that one run is performed\n",
    "                                   normalize_y=False, ## whether the target values y are normalized; recommended for cases where zero-mean and unit-variance priors are used\n",
    "                                   random_state=None)\n",
    "gp_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizer(obj_func, initial_theta, bounds):\n",
    "    # * 'obj_func' is the objective function to be minimized, which\n",
    "    #   takes the hyperparameters theta as parameter and an\n",
    "    #   optional flag eval_gradient, which determines if the\n",
    "    #   gradient is returned additionally to the function value\n",
    "    # * 'initial_theta': the initial value for theta, which can be\n",
    "    #   used by local optimizers\n",
    "    # * 'bounds': the bounds on the values of theta\n",
    "    ...\n",
    "    # Returned are the best found hyperparameters theta and\n",
    "    # the corresponding value of the target function.\n",
    "    return theta_opt, func_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_reg.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.2 高斯过程分类 Gaussian Process Classification (GPC)\n",
    "Gaussian process classification (GPC) based on Laplace approximation, which is internally used for approximating the non-Gaussian posterior by a Gaussian.\n",
    "\n",
    "For multi-class classification, several binary one-vs-rest classifiers are fitted, thus it does not implement a true multi-class Laplace approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "gp_clf = GaussianProcessClassifier(kernel=RBF(),\n",
    "                                   multi_class='one_vs_rest',\n",
    "                                   random_state=None).fit(X_train, y_train)\n",
    "# other parameters are like GaussianProcessRegressor's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.3 高斯过程的核 Kernels for Gaussian Process\n",
    "Kernels (also called “covariance functions” in the context of GPs) are a crucial ingredient of GPs which determine the shape of prior and posterior of the GP. \n",
    "\n",
    "They encode the assumptions on the function being learned by defining the “similarity” of two datapoints combined with the assumption that similar datapoints should have similar target values.\n",
    "\n",
    "1. __stationary kernels__, which depend only on the distance of two datapoints $k(x_i, x_j)= k(d(x_i, x_j))$, and are thus invariant to translations in the input space\n",
    "2. __non-stationary kernels__, which depend also on the specific values of the datapoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|kernels|description|\n",
    "|:----|:----|\n",
    "|[gaussian_process.kernels.ConstantLernel](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.ConstantKernel.html#sklearn.gaussian_process.kernels.ConstantKernel)|$k(x_1, x_2) = constant\\_value \\;\\forall\\; x_1, x_2$|\n",
    "|[gaussian_process.kernels.DotProduct](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.DotProduct.html#sklearn.gaussian_process.kernels.DotProduct)|$k(x_i, x_j) = \\sigma_0 ^ 2 + x_i \\cdot x_j$|\n",
    "|[gaussian_process.kernels.ExpSineSquared](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.ExpSineSquared.html#sklearn.gaussian_process.kernels.ExpSineSquared)|$k(x_i, x_j) = \\text{exp}\\left(-\\frac{ 2\\sin^2(\\pi d(x_i, x_j)/p) }{ l^ 2} \\right)$|\n",
    "|[gaussian_process.kernels.Matern](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.Matern.html#sklearn.gaussian_process.kernels.Matern)|$k(x_i, x_j) = \\frac{1}{\\Gamma(\\nu)2^{\\nu-1}}\\Bigg(\\frac{\\sqrt{2\\nu}}{l} d(x_i , x_j ) \\Bigg)^\\nu K_\\nu\\Bigg(\\frac{\\sqrt{2\\nu}}{l} d(x_i , x_j )\\Bigg)$|\n",
    "|[gaussian_process.kernels.Product](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.Product.html#sklearn.gaussian_process.kernels.Product)|$k_{prod}(X, Y) = k_1(X, Y) * k_2(X, Y)$|\n",
    "|[gaussian_process.kernels.RBF](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.RBF.html#sklearn.gaussian_process.kernels.RBF)|$k(x_i, x_j) = \\exp\\left(- \\frac{d(x_i, x_j)^2}{2l^2} \\right)$|\n",
    "|[gaussian_process.kernels.RationalQuadratic](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.RationalQuadratic.html#sklearn.gaussian_process.kernels.RationalQuadratic)|$k(x_i, x_j) = \\left(1 + \\frac{d(x_i, x_j)^2 }{ 2\\alpha  l^2}\\right)^{-\\alpha}$|\n",
    "|[gaussian_process.kernels.Sum](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.Sum.html#sklearn.gaussian_process.kernels.Sum)|$k_{sum}(X, Y) = k_1(X, Y) + k_2(X, Y)$|\n",
    "|[gaussian_process.kernels.WhiteKernel](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.WhiteKernel.html#sklearn.gaussian_process.kernels.WhiteKernel)|$k(x_1, x_2) = noise\\_level \\text{ if } x_i == x_j \\text{ else } 0$|\n",
    "|[gaussian_process.kernels.CompoundKernel](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.CompoundKernel.html#sklearn.gaussian_process.kernels.CompoundKernel)|Kernel which is composed of a set of other kernels.|\n",
    "|[gaussian_process.kernels.PairwiseKernel](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.PairwiseKernel.html#sklearn.gaussian_process.kernels.PairwiseKernel)|Wrapper for kernels in sklearn.metrics.pairwise.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 交叉分解 `sklearn.cross_decomposition`\n",
    "Useful to find linear relations between two multivariate datasets: the `X` and `Y` arguments of the `fit` method are 2D arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.1 偏最小二乘回归 Partial Least Squares (PLS) Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import PLSRegression\n",
    "pls2_reg = PLSRegression(n_components=2, ## number of components to keep\n",
    "                         scale=True) ## whether to scale the data\n",
    "X = [[0., 0., 1.], [1.,0.,0.], [2.,2.,2.], [2.,5.,4.]]\n",
    "Y = [[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]]\n",
    "pls2_reg.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pls2_reg.score(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pls2_reg.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pls2_reg.transform(X) # apply the dimension reduction learned on the train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.2 典型相关分析 Canonical Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import CCA\n",
    "X = [[0., 0., 1.], [1.,0.,0.], [2.,2.,2.], [2.,5.,4.]]\n",
    "Y = [[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]]\n",
    "cca = CCA().fit(X, Y) # the same parameters with PLSRegression's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cca.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cca.inverse_transform([[0.5, 0.2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 朴素贝叶斯 `sklearn.naive_bayes`\n",
    "\\begin{aligned}P(y \\mid x_1, \\dots, x_n) \\propto P(y) \\prod_{i=1}^{n} P(x_i \\mid y)\\\\\\Downarrow\\\\\\hat{y} = \\arg\\max_y P(y) \\prod_{i=1}^{n} P(x_i \\mid y)\\end{aligned} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7.1 高斯朴素贝叶斯 Gaussian Naive Bayes\n",
    "Assuming Gaussian distributed features,\n",
    "\n",
    "$P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}} \\exp\\left(-\\frac{(x_i - \\mu_y)^2}{2\\sigma^2_y}\\right),$\n",
    "\n",
    "where $\\sigma _y$ and $\\mu _y$ are estimated using maximum likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gaussian_nb_clf = GaussianNB().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_nb_clf.class_prior_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_nb_clf.partial_fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_nb_clf.predict_proba(X_test[3:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7.2 多项式朴素贝叶斯 Multinomial Naive Bayes\n",
    "Suitable for discrete data.\n",
    "\n",
    "Assuming multinomially distributed features,\n",
    "\n",
    "$P(x_i\\mid y)=\\frac{N_{x_i}+\\alpha}{\\sum_{i=1}^nN_{x_i}+\\alpha n},$\n",
    "\n",
    "where $N_{x_i}$ is the number of times of feature $x_i$ appearing in the class $y$.\n",
    "\n",
    "\\* To know Complement Naive Bayes (CNB), an adaptation of the standard multinomial naive Bayes that is particularly suited for imbalanced data sets, refer <https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.ComplementNB.html#sklearn.naive_bayes.ComplementNB>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "multinomial_nb_clf = MultinomialNB(alpha=1.0).fit(X_train, y_train) # float # smooting parameter, 0 for no smoothing\n",
    "# parameters fit_prior and class_prior can be used to adjust prior probabilities of the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multinomial_nb_clf.partial_fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multinomial_nb_clf.predict(X_test[1:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7.3 伯努利贝叶斯 Bernoulli Naive Bayes\n",
    "Suitable for discrete data.\n",
    "\n",
    "Assuming multivariate Bernoulli distributed features, i.e., multiple features that each of them is assumed to be a binary-valued (Bernoulli, boolean) variable,\n",
    "\n",
    "$P(x_i \\mid y) = P(i \\mid y) x_i + (1 - P(i \\mid y)) (1 - x_i).$\n",
    "\n",
    "Binary-valued features required, if handed any other kind of data, the input may be binarized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "bernoulli_nb_clf = BernoulliNB(alpha=1.0, # float # smooting parameter, 0 for no smoothing\n",
    "                               binarize=0.0) # float or None # threshold for mapping features to bolleans, if None then input is presumed to already consist of binary vectors\n",
    "# parameters fit_prior and class_prior can be used to adjust prior probabilities of the classes\n",
    "X = np.random.randint(2, size=(6, 20))\n",
    "y = np.array([1, 2, 3, 4, 4, 5])\n",
    "bernoulli_nb_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bernoulli_nb_clf.class_log_prior_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bernoulli_nb_clf.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7.4 不同类特征的朴素贝叶斯分类器 Naive Bayes classifier for categorical features\n",
    "Suitable for discrete features that are categorically distributed.\n",
    "\n",
    "Assuming that each feature has its own categorical distribution,\n",
    "\n",
    "$P(x_i = t \\mid y) = \\frac{ N_{x_i = t}\\ + \\ \\alpha}{\\sum_{t=1}^{n_{x_i}}N_{x_i=t}\\ +\\ \\alpha n_{x_i}},$\n",
    "\n",
    "where $N_{x_i=t}$ is the number of times of category $t$ appearing in $x_i$, and $n_{x_i}$ is the number of available categories of feature $x_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import CategoricalNB\n",
    "categorical_nb_clf = CategoricalNB(alpha=1.0) # float # smooting parameter, 0 for no smoothing\n",
    "# parameters fit_prior and class_prior can be used to adjust prior probabilities of the classes\n",
    "X = np.random.randint(5, size=(6, 20))\n",
    "y = np.array([1, 2, 3, 4, 4, 3])\n",
    "categorical_nb_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_nb_clf.category_count_ # for each feature, count matrix=categories of (classes, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_nb_clf.predict(X[2:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8 决策树 `sklearn.tree`\n",
    "Advantages:\n",
    "- simple to understand and to interpret (Trees can be visualised)\n",
    "- requires little data preparation, but note that this module does not support missing values\n",
    "- the cost of using the tree (i.e., predicting data) is logarithmic in the number of data points used to train the tree\n",
    "- able to handle both numerical and categorical data\n",
    "- able to handle multi-output problems\n",
    "- possible to validate a model using statistical tests\n",
    "- performs well even if its assumptions are somewhat violated by the true model from which the data were generated\n",
    "\n",
    "Disadvantages:\n",
    "- can create over-complex trees that do not generalise well\n",
    "- can be unstable because small variations in the data might result in a completely different tree being generated\n",
    "- learning an optimal decision tree is known to be NP-complete under several aspects of optimality and even for simple concepts\n",
    "- some concepts are hard to learn because decision trees do not express them easily, such as XOR, parity or multiplexer problems\n",
    "- create biased trees if some classes dominate (it is therefore recommended to balance the dataset prior to fitting with the decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8.1 决策树分类 Decision Tree for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "ds_clf = tree.DecisionTreeClassifier(criterion='gini', # or 'entropy' # function to measure the quality of a split\n",
    "                                     splitter='best', # or 'random' # strategy used to choose the split at each node\n",
    "                                     max_depth=None, # int # maximum depth of the tree, if None then nodes are expanded until all leaves are pure or until all leaves contain less than main_samples_split samples\n",
    "                                     min_samples_split=2, # int or float # minimum number of samples required to split an internal node, float leads to a fraction of n_samples\n",
    "                                     min_samples_leaf=1, # int or float # minmum number of samples required to be split into a leaf node, i.e., a split point will only be considered if it has at least min_samples_leaf training samples in each of the branches, float leads to a fraction of n_samples\n",
    "                                     max_features=None, # int, float, or {'auto', 'sqrt', 'log2'} # number of features to consider when looking for the best split; 'auto' and 'sqrt': sqrt(n_features), 'log2': log2(n_features), None: n_features, float leads to a fraction of n_samples\n",
    "                                     max_leaf_nodes=None, # int # grow a tree with max_leaf_nodes in best-first fashion, if None then unlimited number of leaf nodes\n",
    "                                     min_impurity_decrease=0.0, # float # a node will be split if this split induces a decrease of the impurity geater or equal to min_imppurity_decrease\n",
    "                                     class_weight=None, # dict{class_label: weight}, list of dict (for multi-output problems), or 'balanced' # 'balanced' adjust weights inversely proportional to class frequencies; if None then all classes are supposed to have weight 1\n",
    "                                     ccp_alpha=0.0, # non-negative float # complexity parameter used for Minimal Cost-Complexity Pruning, the subtree with the largest cost complexity that is smaller than ccp_alpha will be chosen\n",
    "                                     random_state=None)\n",
    "ds_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds_clf.feature_importances_ # used like .coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('depth of the tree:', ds_clf.get_depth())\n",
    "print('number of leaves:', ds_clf.get_n_leaves())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_clf.apply(X_train[1:2]) # return the index of the leaf that each sample is predicted as."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds_clf.decision_path(X[1:2]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.plot_tree(ds_clf,  # refer https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html#sklearn.tree.plot_tree\n",
    "               feature_names=list(breast_cancer['feature_names']),\n",
    "               class_names=list(breast_cancer['target_names']))\n",
    "# more about plotting refer https://scikit-learn.org/stable/modules/tree.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tree.export_text(ds_clf, feature_names=list(breast_cancer['feature_names'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8.2 决策树回归 Decision Tree for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "ds_reg = DecisionTreeRegressor(criterion='mse' # 'friedman_mse', or 'mae' # function to measure the quality of a split\n",
    "                              ).fit(X_train, y_train) # other parameters are the same with DecisionTreeClassifier's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds_reg.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('depth of the tree:', ds_reg.get_depth())\n",
    "print('number of leaves:', ds_reg.get_n_leaves())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_reg.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tree.export_text(ds_reg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8.3 多输出问题 Multi-output problems\n",
    "DecisionTreeClassifier and DecisionTreeRegressor are able to deal with multi-output problems where output values may not independent (Y is a 2D array), which requires\n",
    "- store several output values in leaves, instead of 1\n",
    "- use splitting criteria that compute the average reduction across all outputs\n",
    "\n",
    "__Examples__\n",
    "- Multi-output Decision Tree Regression\\\n",
    "<https://scikit-learn.org/stable/auto_examples/tree/plot_tree_regression_multioutput.html#sphx-glr-auto-examples-tree-plot-tree-regression-multioutput-py>\n",
    "- Face completion with a multi-output estimators\\\n",
    "<https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_multioutput_face_completion.html#sphx-glr-auto-examples-miscellaneous-plot-multioutput-face-completion-py>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.9 集成学习 `sklearn.ensemble`\n",
    "Combining the predictions of several base estimators built with a given learning algorithm in order to improve generalizability / robustness over a single estimator.\n",
    "1. Ensemble learning in averaging methods is to build several estimators independently and then to average their predictions. On average, the combined estimator is usually better than any of the single base estimator because its variance is reduced.\\\n",
    "__Examples:__ __2.9.1__ Bagging meta-estimator, __2.9.2__ Random Forests\n",
    "\n",
    "\n",
    "2. Ensemble learning in boosting methods have sequentially built base estimators and try to reduce the bias of the combined estimator. The motivation is to combine several weak models to produce a powerful ensemble.\\\n",
    "__Examples:__ __2.9.3__ AdaBoost, __2.9.4__ Gradient Tree Boosting, __2.9.5__ Histogram-based Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9.1 Bagging 元估计器 Bagging meta-estimator\n",
    "Building several instances of a black-box estimator on random subsets of the original training set and then aggregate (by averaging or voting) their individual predictions to form a final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.9.1.1 Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "bagging_clf = BaggingClassifier(base_estimator=None, # object # base estimator to fit on, if None then a decision tree will be used\n",
    "                                n_estimators=10, # int # number of base estimators in the ensemble\n",
    "                                max_samples=1.0, # int or float # number of samples to draw from X to train each base estimator, float draw max_samples*X.shape[0] samples\n",
    "                                max_features=1.0, # int or float # number of features to draw from X to train each base estimator, float draw max_samples*X.shape[1] samples\n",
    "                                bootstrap=True, ## whether samples are drawn with replacement\n",
    "                                bootstrap_features=False, # whether features are drawn with replacement\n",
    "                                oob_score=False, ## whether to use out-of-bag samples to estimate the generalization error\n",
    "                                warm_start=False, ## whether to reuse the solution of the previous call to fit and add more estimators to the ensemble\n",
    "                                n_jobs=None, ## number of jobs to run in parallel of both fit and predict, , n_jobs=-1 uses all cores available on the machine\n",
    "                                random_state=None) ## used if the base estimator accepts random_state\n",
    "bagging_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging_clf.score(X_train, y_train) # mean accuracy on the given test data and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging_clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.9.1.2 Bagging Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "bagging_reg = BaggingRegressor().fit(X_train, y_train)\n",
    "# the same parameters with BaggingClassifier's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging_reg.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9.2 随机森林 Random Forests\n",
    "To know another averaging algorithm based on randomized decision trees, Extra-Trees, see:\n",
    "\n",
    "Classifier: <https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier> \\\n",
    "Regressor: <https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesRegressor.html#sklearn.ensemble.ExtraTreesRegressor>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.9.2.1 Random Forest Classification\n",
    "Each tree in the ensemble is built from a sample drawn with replacement from the training set. Furthermore, when splitting each node during the construction of a tree, the best split is found either from all input features or a random subset of size `max_features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, # int # number of trees in the forest\n",
    "                                criterion='gini', # or 'entropy' # function to measure the quality of a split\n",
    "                                max_depth=None, # int # maximum depth of th tree, None expands the tree as deep as possible\n",
    "                                min_samples_split=2, # int or float # minimum number or fraction of samples required to split an internal node\n",
    "                                min_samples_leaf=1, # int or float # minimum number or fraction of samples required to be at a leaf node\n",
    "                                max_features='auto', # 'sqrt', 'log2', int or float # number of features to consider when looking for the best split\n",
    "                                max_leaf_nodes=None, # int # grow trees with max_leaf_nodes in best-first fashion, if None then unlimited number of leaf nodes\n",
    "                                min_impurity_decrease=0.0, # a node will be split if this split induces a decrease of the impurity greater than or equal to this value\n",
    "                                ccp_alpha=0.0, # non-negative float # complexity parameter used for Minimal Cost-Complexity Pruning, the subtree with the largest cost complexity that is smaller than ccp_alpha will be chosen\n",
    "                                bootstrap=True, ## whether bootstrap samples are used when building trees, False uses the whole dataset\n",
    "                                max_samples=None, # int or float # if bootstrap==Ture, the number of samples to draw from X\n",
    "                                oob_score=False, ## whether to use out-of-bag samples to estimate the generalization accuracy\n",
    "                                class_weight=None, # 'balanced', 'balanced_subsample', dict{class_label: weight} or list of dicts # None associates equally weight one to all classes\n",
    "                                warm_start=False, ## whether to reuse the solution of the previous call to fit and add more estimators to the ensemble\n",
    "                                n_jobs=None, # int # number of jobs to run in parallel, , n_jobs=-1 uses all cores available on the machine\n",
    "                                random_state=None)\n",
    "rf_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf.feature_importances_ # impurity-based feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf.apply(X_test[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.9.2.2 Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf_reg = RandomForestRegressor(criterion='mse', # or 'mae'\n",
    "                               random_state=None).fit(X_train, y_train)\n",
    "# the same parameters with RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9.3 AdaBoost\n",
    "AdaBoost fits a sequence of weak learners on repeatedly modified versions of the data.\n",
    "\n",
    "Data modifications at each so-called boosting iteration consist of applying weights $w_1$, $w_2$, …, $w_N$ to each of the training samples. Initially, those weights are all set to $w_i=1/N$, so that the first step simply trains a weak learner on the original data. For each successive iteration, the sample weights are individually modified and the learning algorithm is reapplied to the reweighted data.\n",
    "\n",
    "At a given iteration, training examples that were incorrectly predicted at the previous step have their weights increased, whereas the weights are decreased for those that were predicted correctly. As iterations proceed, examples that are difficult to predict receive ever-increasing influence. Each subsequent weak learner is thereby forced to concentrate on the examples that are missed by the previous ones in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.9.3.1 AdaBoost Classification\n",
    "This class implements the algorithm known as AdaBoost-SAMME."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "adaboost_clf = AdaBoostClassifier(base_estimator=None, # object supporting for sample weight, .classes_, and .n_classes_# base estimator, None defaults DecisionTreeClassifier(max_depth=1)\n",
    "                                  n_estimators=50, # int # maximum number of estimators\n",
    "                                  learning_rate=1.0, # float # learning rate shrinks the contribution of each classifier\n",
    "                                  algorithm='SAMME.R', # or 'SAMME', for details see https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier\n",
    "                                  random_state=None) ## used if the base_estimator accepts random_state\n",
    "adaboost_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaboost_clf.estimator_weights_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaboost_clf.estimator_errors_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaboost_clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaboost_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.9.3.2 AdaBoost Regression\n",
    "This class implements the algorithm known as AdaBoost.R2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "adaboost_reg = AdaBoostRegressor(base_estimator=None, # object # None defaults DecisionTreeRegressor(max_depth=3)\n",
    "                                 n_estimators=50, # int # maximum number of estimators\n",
    "                                 learning_rate=1.0, # float # learning rate shrinks the contribution of each regressor\n",
    "                                 loss='linear', # 'square', or 'exponential' # loss function to use when updating the weights after each boosting iteration\n",
    "                                 random_state=None) ## used if the base_estimator accepts random_state\n",
    "adaboost_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaboost_reg.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaboost_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaboost_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9.4 Gradient Tree Boosting / Gradient Boosted Decision Trees\n",
    "A generalization of boosting to arbitrary differentiable loss functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.9.4.1 Gradient Boosting Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gradient_boost_clf = GradientBoostingClassifier(loss='deviance', # or 'exponential' # loss function to be optimized, 'exponential' recovers AdaBoost\n",
    "                                                learning_rate=1.0, # float # learning rate shrinks the contribution of each classifier\n",
    "                                                n_estimators=100, # int # maximum number of estimators\n",
    "                                                subsample=1.0, # float # fraction of samples to be used for fitting the individual base learners; if smaller than 1.0 this results in Stochastic Gradient Boosting\n",
    "                                                criterion='friedman_mse', # 'mse', or 'mae' # function to measure the quality of a split\n",
    "                                                max_depth=3, # int # maximum depth of the tree\n",
    "                                                min_samples_split=2, # int or float # minimum number of samples required to split an internal node, float leads to a fraction of n_samples\n",
    "                                                min_samples_leaf=1, # int or float # minmum number of samples required to be split into a leaf node, i.e., a split point will only be considered if it has at least min_samples_leaf training samples in each of the branches, float leads to a fraction of n_samples\n",
    "                                                max_features=None, # int, float, or {'auto', 'sqrt', 'log2'} # number of features to consider when looking for the best split; 'auto' and 'sqrt': sqrt(n_features), 'log2': log2(n_features), None: n_features, float leads to a fraction of n_samples\n",
    "                                                max_leaf_nodes=None, # int # grow a tree with max_leaf_nodes in best-first fashion, if None then unlimited number of leaf nodes\n",
    "                                                min_impurity_decrease=0.0, # float # a node will be split if this split induces a decrease of the impurity geater or equal to min_imppurity_decrease\n",
    "                                                ccp_alpha=0.0, # non-negative float # complexity parameter used for Minimal Cost-Complexity Pruning, the subtree with the largest cost complexity that is smaller than ccp_alpha will be chosen\n",
    "                                                init=None, # 'zero' or estimator # estimator object used to compute the initial predictions, 'zero' sets initial raw predictions to zero\n",
    "                                                warm_start=False, ## whether to reuse the solution of the previous call to fit and add more estimators to the ensemble\n",
    "                                                n_iter_no_change=None, # int # decide if early stopping will be used to terminate training when validation score is not improving, None disables early stopping\n",
    "                                                tol=1e-4, # float # tolerance for the early stopping\n",
    "                                                validation_fraction=0.1, # float # proportion of training data to set aside as validation set for early stopping, only used if n_iter_no_change is set to an integer\n",
    "                                                random_state=None)\n",
    "gradient_boost_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_boost_clf.train_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_boost_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_boost_clf.predict(X_test[2:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.9.4.2 Gradient Boosting Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gradient_boost_reg = GradientBoostingRegressor(loss='ls', # 'lad', 'huber', or 'quantile'\n",
    "                                               alpha=0.9, # float # alpha-quantile of the 'huber' and 'quantile' loss function\n",
    "                                               random_state=None)\n",
    "# other parameters are the same with GradientBoostingClassifier's\n",
    "gradient_boost_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_boost_reg.train_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_boost_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9.5 Histogram-based Gradient Boosting\n",
    "Two experimental implementations of gradient boosting trees:\n",
    "\n",
    "`HistGradientBoostingClassifier`: <https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html#sklearn.ensemble.HistGradientBoostingClassifier> \\\n",
    "\n",
    "`HistGradientBoostingRegressor`: <https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html#sklearn.ensemble.HistGradientBoostingRegressor>\n",
    "\n",
    "These histogram-based estimators can be orders of magnitude faster than `GradientBoostingClassifier` and `GradientBoostingRegressor` when the number of samples is larger than tens of thousands of samples. The only changed parameter is `max_iter` that replaces `n_estimators`, and controls the number of iterations of the boosting process\n",
    "\n",
    "For more details refer <https://scikit-learn.org/stable/modules/ensemble.html#histogram-based-gradient-boosting>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9.6 投票分类/回归器 Voting Classifier/Regressor\n",
    "To combine conceptually different machine learning learners and use a majority vote or the average predicted probabilities (soft vote) to predict the outputs. Such a learner can be useful for a set of equally well performing model in order to balance out their individual weaknesses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.9.6.1 Voting Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "clf1 = LogisticRegression()\n",
    "clf2 = RandomForestClassifier()\n",
    "clf3 = GaussianNB()\n",
    "voting_clf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n",
    "                              voting='hard', # or 'soft' # 'hard' uses majority rule while 'soft' predicts based on the argmax of the sums of the predicted probabilities\n",
    "                              weights=[1, 2, 2], # array of int or float\n",
    "                              flatten_transform=True, ## if voting='soft' and flatten_transform=True, transform method returns matrix with shape (n_samples, n_classifiers * n_classes); if flatten_transform=False, it returns (n_classifiers, n_samples, n_classes)\n",
    "                              n_jobs=None)\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array_equal(voting_clf.named_estimators_.lr.predict(X_train),\n",
    "               voting_clf.named_estimators_['lr'].predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_clf.predict(X_test[3:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_clf.transform(X_test) # return class labels or probabilities for each estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.9.6.2 Voting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, VotingRegressor\n",
    "estimators = [('lr', LinearRegression()), ('rf', RandomForestRegressor())]\n",
    "voting_reg = VotingRegressor(estimators)\n",
    "voting_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.10 半监督学习 `sklearn.semi_supervised`\n",
    "When some of the training samples are not labeled, semi-supervised algorighms are able to make use of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10.1 标签传播算法 Label Propagation\n",
    "- can be used for classification and regression tasks\n",
    "- kernel methods to project data into alternate dimensional spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.10.1.1 Label Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.semi_supervised import LabelPropagation\n",
    "label_prop_clf = LabelPropagation(kernel='rbf', # 'knn', or callable # function passed should take two inputs, both of shape (n_samples, n_features), and return a (n_samples, n_samples) shaped weight matrix\n",
    "                                  gamma=20., # float # parameter for rbf kernel\n",
    "                                  n_neighbors=7, # positive int # parameter for knn kernel\n",
    "                                  tol=1e-3, # folat # threshold to consider the system at steady state\n",
    "                                  n_jobs=None)\n",
    "label_prop_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_prop_clf.transduction_ # label assigned to each item via the transduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array_equal(label_prop_clf.predict(X_train)[np.where(X==1)[0]],\n",
    "               label_prop_clf.transduction_[np.where(X==1)[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.10.1.2 Label Spreading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.semi_supervised import LabelSpreading\n",
    "label_spread_clf = LabelSpreading(kernel='rbf', # 'knn', or callable # function passed should take two inputs, both of shape (n_samples, n_features), and return a (n_samples, n_samples) shaped weight matrix\n",
    "                                  gamma=20., # float # parameter for rbf kernel\n",
    "                                  n_neighbors=7, # positive int # parameter for knn kernel\n",
    "                                  alpha=0.2, # float in (0, 1) # clamping factor that specifies the relative amount that an instance should adopt its neighbors' opposed label\n",
    "                                  max_iter=30, # int # maximum number of iterations allowed\n",
    "                                  tol=1e-3, # float # threshold to consider the system at steady state\n",
    "                                  n_jobs=None)\n",
    "label_spread_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_spread_clf.n_iter_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_spread_clf.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.11 保序回归 `sklearn.isotonic`\n",
    "$\\begin{align}&\\min \\sum_i w_i(y_i-\\hat y_i)^2\\\\\n",
    "&s.t.\\ \\hat y_i \\leq \\hat y_j \\ {\\rm whenever}\\ x_i\\leq x_j\\end{align}$\n",
    "\n",
    "where the weights $w_i$ are strictly positive\n",
    "\n",
    "\\* The training data and target are both array-like of shape (n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "X, y = make_regression(n_samples=20, n_features=1, noise=50.0)\n",
    "X = X.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.isotonic import IsotonicRegression\n",
    "isotonic_reg = IsotonicRegression(y_min=None, # float # lower bound on the lowest predicted value, None defaults to -inf\n",
    "                                  y_max=None, # float # upper bound on the highest predicted value, None defaults to +inf\n",
    "                                  increasing=True, # bool or 'auto' # determines whether the predictions should be constrained to increase or decrease with x, 'auto' will decide based on the Spearman correlation estimate's sign\n",
    "                                  out_of_bounds='nan') # 'clip', or 'raise' # handles how to deal with x values outside of the training domain; 'nan': np.nan, 'clip': the nearest train interval endpoint, 'raise': raise a ValueError\n",
    "isotonic_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isotonic_reg.f_ # The stepwise interpolating function that covers the input domain X\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(X, y, 'ro')\n",
    "plt.plot(isotonic_reg.f_.x, isotonic_reg.f_.y, 'go-')\n",
    "plt.title('isotonic_reg.f_')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(X, y, 'ro')\n",
    "plt.plot(np.sort(X), isotonic_reg.transform(np.sort(X)), 'bo-')\n",
    "plt.title('isotonic_reg.transform()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isotonic_reg.increasing_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isotonic_reg.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isotonic_reg.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.12 概率校准 `sklearn.calibration`\n",
    "Well calibrated classifiers are probabilistic classifiers for which the output of the predict_proba method can be directly interpreted as a confidence level.\n",
    "\n",
    "The calibration module allows better probability calibration of a given model, or to add support for probability prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.12.1 校准图/可靠性曲线 Calibration plots (reliability curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import (brier_score_loss, precision_score, recall_score, f1_score)\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_calibration_curve(est, name, fig_index, X_train, y_train, X_test, y_test):\n",
    "    # Calibrated with isotonic calibration\n",
    "    isotonic = CalibratedClassifierCV(est, ## base_estimator\n",
    "                                      method='isotonic', ## not advised to use isotonic calibration with too few calibration samples (<<1000) since it tends to overfit\n",
    "                                      cv=2) ## int, 'prefit', cross-validation generator, or iterable\n",
    "\n",
    "    # Calibrated with sigmoid calibration\n",
    "    sigmoid = CalibratedClassifierCV(est, method='sigmoid', cv=2)\n",
    "\n",
    "    # Logistic regression with no calibration as baseline\n",
    "    lr = LogisticRegression(C=1.)\n",
    "\n",
    "    fig = plt.figure(fig_index, figsize=(10, 10))\n",
    "    ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\n",
    "    ax2 = plt.subplot2grid((3, 1), (2, 0))\n",
    "\n",
    "    ax1.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "    for clf, name in [(lr, 'Logistic'),\n",
    "                      (est, name),\n",
    "                      (isotonic, name + ' + Isotonic'),\n",
    "                      (sigmoid, name + ' + Sigmoid')]:\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        if hasattr(clf, \"predict_proba\"):\n",
    "            prob_pos = clf.predict_proba(X_test)[:, 1]\n",
    "        else:  # use decision function\n",
    "            prob_pos = clf.decision_function(X_test)\n",
    "            prob_pos = \\\n",
    "                (prob_pos - prob_pos.min()) / (prob_pos.max() - prob_pos.min())\n",
    "\n",
    "        clf_score = brier_score_loss(y_test, prob_pos, pos_label=y.max())\n",
    "        print(\"%s:\" % name)\n",
    "        print(\"\\tBrier: %1.3f\" % (clf_score))\n",
    "        print(\"\\tPrecision: %1.3f\" % precision_score(y_test, y_pred))\n",
    "        print(\"\\tRecall: %1.3f\" % recall_score(y_test, y_pred))\n",
    "        print(\"\\tF1: %1.3f\\n\" % f1_score(y_test, y_pred))\n",
    "\n",
    "        fraction_of_positives, mean_predicted_value = \\\n",
    "            calibration_curve(y_test, ## y_true, true targets\n",
    "                              prob_pos, ## y_prob, probabilities of the positive class\n",
    "                              normalize=False, ##whether y_prob needs to be normalized\n",
    "                              n_bins=10, # int, 5 by default # number of bins to discretize the [0, 1] interval\n",
    "                              strategy='uniform') # or 'quantile' # strategy used to define the widths of the bins\n",
    "\n",
    "        ax1.plot(mean_predicted_value, fraction_of_positives, \"s-\",\n",
    "                 label=\"%s (%1.3f)\" % (name, clf_score))\n",
    "\n",
    "        ax2.hist(prob_pos, range=(0, 1), bins=10, label=name,\n",
    "                 histtype=\"step\", lw=2)\n",
    "\n",
    "    ax1.set_ylabel(\"Fraction of positives\")\n",
    "    ax1.set_ylim([-0.05, 1.05])\n",
    "    ax1.legend(loc=\"lower right\")\n",
    "    ax1.set_title('Calibration plots  (reliability curve)')\n",
    "\n",
    "    ax2.set_xlabel(\"Mean predicted value\")\n",
    "    ax2.set_ylabel(\"Count\")\n",
    "    ax2.legend(loc=\"upper center\", ncol=2)\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_calibration_curve(GaussianNB(), 'Naive Bayes', 1, X_train, y_train, X_test, y_test)\n",
    "plot_calibration_curve(LinearSVC(), 'SVC', 2, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.13 神经网络 `sklearn.neural_network`\n",
    "This implementation is not intended for large-scale applications. In particular, scikit-learn offers no GPU support.\n",
    "\n",
    "\\* It is highly recommended to scale input first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.13.1 多层感知机分类 Multi-layer Perceptron Classification\n",
    "Implementing a multi-layer perceptron (MLP) algorithm that optimizes the log-loss function using LBFGS or stochastic gradient descent.\n",
    "\n",
    "Currently, MLPClassifier supports only the Cross-Entropy loss function, which allows probability estimates by running the predict_proba method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp_clf = MLPClassifier(hidden_layer_sizes=(100,), # tuple, length=n_layers-2 # the ith element represents the number of neurons in the ith hidden layer\n",
    "                        activation='relu', # 'identity', 'logistic', or 'tanh' # 'identity': f(x)=x; 'logistic': f(x)=1/(1+exp(-x)); 'tanh': f(x)=tanh(X); 'relu': f(x)=max(0, x)\n",
    "                        solver='adam', # 'lbfgs', or 'sgd' # solver for weight optimization, 'adam' works pretty well on relatively large (thousands or more) datasets while 'lbfgs' can converge faster and better on small datasets\n",
    "                        alpha=0.0001, # float # regularization(L2) term parameter\n",
    "                        ##--------------------------------------\n",
    "                        ## parameters used when solver=='lbfgs'\n",
    "                        batch_size='auto', # int # size of minibatches for stochastic optimizers; if solver=='lbfgs', the classifier will not use minibatch\n",
    "                        max_fun=15000, # int # maximum number of loss function calls, only used when solver='lbfgs'\n",
    "                        ## parameters used when solver=='sgd'\n",
    "                        learning_rate='constant', # 'invscaling', or 'adaptive' # learning rate schedule for weight updates, only used when solver=='sgd'\n",
    "                        power_t=0.5, # double # exponent for inverse scaling learning rate, only used when learning_rate=='invscaling' and solver=='sgd'\n",
    "                        momentum=0.9, # float in [0, 1] # momentum for gradient descent update, only used when solver='sgd'\n",
    "                        nesterovs_momentum=True, ## whether to use Nesterov's momentum, only used when solver='sgd' and momentum>0\n",
    "                        ## parameters used when solver=='lbfgs' or 'adam'\n",
    "                        learning_rate_init=0.001, # double # initial learning rate, only used when solver=='sgd' or 'adam'\n",
    "                        early_stopping=False, ## whether to terminate training when validation score is not improving, only effective when solver='sgd' or 'adam'\n",
    "                        n_iter_no_change=10, # int # maximum number of epochs to not meet tol improvement, only effective when solver='sgd' or 'adam'\n",
    "                        ## parameters used when solver=='adam'\n",
    "                        beta_1=0.9, # float in [0, 1) # exponential decay rate for estimates of first moment vector, only used when solver='adam'\n",
    "                        beta_2=0.999, # float in [0, 1) # exponential decay rate for estimates of second moment vector, only used when solver='adam'\n",
    "                        epsilon=1e-8, # float # value for numerical stability, only used when solver='adam'\n",
    "                        ##--------------------------------------\n",
    "                        max_iter=200, # int # maximum number of iterations/epochs\n",
    "                        tol=1e-4, # float # tolerance for the optimization\n",
    "                        verbose=False, ## whether to print progress messages to stdout\n",
    "                        warm_start=False, ## whether to reuse the solution of the previous call to fit as initialization\n",
    "                        shuffle=True, ## whether to shuffle samples in each iteration\n",
    "                        random_state=None) ## used when early_stopping==True or solver=='sgd'/'adam'\n",
    "mlp_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_clf.n_iter_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_clf.predict(X_test[3:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.13.2 多层感知机回归 Multi-layer Perceptron Regression\n",
    "Implementing a multi-layer perceptron (MLP) that trains using backpropagation with no activation function in the output layer, which can also be seen as using the identity function as activation function. Therefore, it uses the square error as the loss function, and the output is a set of continuous values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "mlp_reg = MLPRegressor(# the same parameters with MLPClassifier\n",
    "                       validation_fraction=0.1) # float in [0, 1] # proportion of training data to set aside as validation set for early stopping\n",
    "mlp_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_reg.predict(X_test[3:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 高斯混合模型 `sklearn.mixture`\n",
    "A Gaussian mixture model is a probabilistic model that assumes all the data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters.\n",
    "\n",
    "\\* Used for __Density Estimation__ and __Clustering__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 高斯混合模型的EM算法 Expecation-maximization (EM) algorithm for fitting Guassian Mixture Model\n",
    "Estimating the parameters of a Gaussian mixture distribution.\n",
    "\n",
    "__Advantages:__\n",
    "- fastest algorithm for learning mixture models\n",
    "- as it maximizes only the likelihood, the algorithm will not bias the means towards zero, or bias the cluster sizes to have specific structures that might or might not apply\n",
    "\n",
    "__Disadvantages:__\n",
    "- if the input does not have sufficient points per mixture, estimating the covariance matrices will become difficult\n",
    "- requiring held-out data or information criteria to decide how many components to use in the absence of external cues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "gmm = GaussianMixture(n_components=2, # int # number of mixture components\n",
    "                      covariance_type='full', # 'ties', 'diag', or 'spherical' # 'full': each component has its own general covariance matrix; 'tied': all components share the same general covariance matrix; 'diag': each component has its own diagonal covariance matrix; 'spherical': each component has its own single variance\n",
    "                      tol=1e-3, # float # covergence threshold, EM iterations will stop when the lower bound average gain is below this threshold\n",
    "                      reg_covar=1e-6, # float # non-negative regularization added to the diagonal of covariance to assure the covariance matrices are all positive\n",
    "                      max_iter=100, # int # number of EM iterations to perform\n",
    "                      n_init=1, # int # number of initializaitons to perform\n",
    "                      weights_init=None, # array-like of shape (n_components, n_features) # user-provided initial weights, if None then weights are initialized using the init_params method\n",
    "                      init_params='kmeans', # or 'random' # method used to initialize the weights, means, and precisions\n",
    "                      precisions_init=None, # array-like of shape (n_components, n_features, n_features) if 'full', (n_components,) if 'spherical', (n_features, n_features) if 'tied', (n_components, n_features) if 'diag' # user-provided initial precisions(inverse of the covariance matrices), if None then initialized by init_params\n",
    "                      warm_start=False, ## whether the solution of the last fitting will be used as initialization for the next call\n",
    "                      random_state=None)\n",
    "gmm.fit(X_train) ## predict methods can be used when input X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('weights of each component: ', gmm.weights_)\n",
    "print('shape of means of each component: ', gmm.means_.shape)\n",
    "print('shape of covariances of each component: ', gmm.covariances_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm.n_iter_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Akaike information criterion for the current model on the input X_test: ', gmm.aic(X_test))\n",
    "print('Bayesian information criterion for the current model on the input X_test: ', gmm.bic(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm.sample(1) # generate random samples form the fitted Gaussian distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm.predict(X_test[2:3]) # predict the component labels for input samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 变分贝叶斯高斯混合模型 Variational Bayesian Gaussian Mixture\n",
    "Implementing a variant of the Gaussian mixture model with variational inference algorithms, which maximizes a lower bound on model evidence (including priors) instead of data likelihood. The effective number of components can be inferred from the data.\n",
    "\n",
    "__Advantages:__\n",
    "- when weight_concentration_prior is small enough and n_components is larger than what is found necessary by the model, the model has a natural tendency to set some mixture weights values close to zero, which makes it possible to let the model choose a suitable number of effective components automatically\n",
    "- the variational inference with a Dirichlet process prior won’t change much with changes to the parameters, leading to more stability and less tuning\n",
    "- due to the incorporation of prior information, variational solutions have less pathological special cases than expectation-maximization solutions\n",
    "\n",
    "__Disadvantages:__\n",
    "- the extra parametrization necessary for variational inference make inference slower\n",
    "- requiring an extra hyperparameter that might need experimental tuning via cross-validation\n",
    "- there are many implicit biases in the inference algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "bgm = BayesianGaussianMixture(n_components=2, ## number of mixture components\n",
    "                              covariance_type='full', # 'ties', 'diag', or 'spherical' # 'full': each component has its own general covariance matrix; 'tied': all components share the same general covariance matrix; 'diag': each component has its own diagonal covariance matrix; 'spherical': each component has its own single variance\n",
    "                              tol=1e-3, # float # covergence threshold, EM iterations will stop when the lower bound average gain is below this threshold\n",
    "                              reg_covar=1e-6, # float # non-negative regularization added to the diagonal of covariance to assure the covariance matrices are all positive\n",
    "                              max_iter=100, # int # number of EM iterations to perform\n",
    "                              n_init=1, # int # number of initializaitons to perform\n",
    "                              init_params='kmeans', # or 'random' # method used to initialize the weights, means, and precisions\n",
    "                              weight_concentration_prior_type='dirichlet_process', # or 'dirichlet_distribution' # 'dirichlet_process' uses the Stick-breaking representation while 'dirichlet_distribution' can favor more uniform weights\n",
    "                              weight_concentration_prior=None, # float greater than 0, None sets it to 1./n_components # commonly called gamma; the higher concentration puts more mass in the center and will lead to more components being active, while a lower concentration parameter will lead to more mass at the edge of the mixture weights simplex\n",
    "                              mean_precision_prior=None, # float greater than 0, None sets it to 1 # precision prior on the mean distribution, larger values concentrate the cluster means around mean_prior\n",
    "                              mean_prior=None, # array-like of shape (n_features) # None sets it to the mean of the input X\n",
    "                              degrees_of_freedom_prior=None, # float, None sets it to n_features # prior of the number of degrees of freedom on the covariance distributions\n",
    "                              covariance_prior=None, # float or array-like, (n_features, n_features) if 'full', (n_features, n_features) if 'tied', (n_features) if 'diag', float if 'spherical', and None sets it using the covariance of input X\n",
    "                              warm_start=False, ## whether the solution of the last fitting will be used as initialization for the next call\n",
    "                              random_state=None)\n",
    "bgm.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgm.converged_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgm.score_samples(X_test[2:3]) # Compute the weighted log probabilities for each sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgm.predict(X_test[2:3]) # predict the component labels for input samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1* 密度估计\n",
    "Besides `GaussianMixture`, sklearn provides another algorithm for __Density Estimation__ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1*.1 核密度估计 Kernel Density Estimation (KDE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KernelDensity\n",
    "kde = KernelDensity(bandwidth=1.0, # float # bandwidth of the kernel\n",
    "                    algorithm='auto', # 'kd_tree', or 'ball_tree' # tree algorithm to use\n",
    "                    kernel='gaussian', # 'tophat', 'epanechnikov', 'exponential', 'linear', or 'cosine' # kernel to use\n",
    "                    metric='euclidean', # str # distance metric to use\n",
    "                    atol=0., # float # desired absolute tolerance of the result; a larger value generally accelerates the execution\n",
    "                    rtol=0., # float # desired relative tolerance of the result; a larger value generally accelerates the execution\n",
    "                    breadth_first=True, ## whether to use a breadth-first approach to the problem; False uses a depth-first approach\n",
    "                    leaf_size=40, # int # leaf size of the underlying tree\n",
    "                    metric_params=None) # dict # additional parameters to be passed to the tree for use with the metric\n",
    "kde.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kde.sample(5) # generate random samples from the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 流形学习 `sklearn.manifold`\n",
    "Learning the high-dimensional structure of the input data from the data itself, and outputting lower-dimensional representations.\n",
    "\n",
    "\\* Used for __Dimensionality Reduction__ \\\n",
    "\\* Make sure the same scale is used over all features because manifold learning methods are distance-based."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 等距特征映射 Isometric Mapping (Isomap)\n",
    "Isomap seeks a lower-dimensional embedding which maintains geodesic distances between all points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import Isomap\n",
    "isomap = Isomap(n_neighbors=5, # int # number of neighbors to consider for each point\n",
    "                n_components=2, # int # number of coordinates for the manifold\n",
    "                eigen_solver='auto', # 'arpack', or 'dense' # 'arpcak' uses Arnoldi decomposition to find the eigenvalues and eigenvectors; 'dense' uses a direct solver for the eigenvalue decomposition\n",
    "                tol=0., # float # convergence tolerance passed to arpack or lobpcg, not used if eigen_solver=='dense'\n",
    "                max_iter=None, # int # maximum number of iterations for the arpack solver, not used if eigen_solver=='dense'\n",
    "                path_method='auto', # 'FW', or 'D' # method to use in finding shortest path, 'D' uses Dijkstra's algorithm and 'FW' uses Floyd-Warshall algorithm\n",
    "                neighbors_algorithm='auto', # 'brute', 'kd_tree', or 'ball_tree' # algorithm to use for nearest neighbors search\n",
    "                metric='minkowski', # string or callable allowed by sklearn.metrics.pairwise_distances # metric to use when calculating distance between instances in a feature array\n",
    "                metric_params=None, # dict # additional keyword arguments for the metric function\n",
    "                p=2, # int # parameter for the Minkowski metric, p=1 is equivalent to using manhattan_distance(l1), and euclidean_distance (l2) for p=2\n",
    "                n_jobs=None)\n",
    "isomap.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isomap.embedding_.shape # store the embedding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isomap.dist_matrix_.shape # store the geodesic distance matrix of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_transformed = isomap.transform(X_test)\n",
    "X_test_transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 局部线性嵌入 Locally Linear Embedding\n",
    "Seeking a lower-dimensional projection of the data which preserves distances within local neighborhoods. It can be thought of as a series of local Principal Component Analyses which are globally compared to find the best non-linear embedding.\n",
    "\n",
    "\\* Through `method={'modified', 'hessian', 'lsta'}` to implement __Modified locally linear embedding (MLLE)__ , __Hessian Eignmapping / Hessian-based LLE (HLLE)__ and __Local tangent sapce alignment (LTSA)__ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-off\n",
    "from sklearn.manifold import locally_linear_embedding\n",
    "X_train_transformed, squared_error = locally_linear_embedding(X_train, n_neighbors=5, n_components=2)\n",
    "# other parameters are the same with the below transformer's\n",
    "print('reconstruction error for the embedding vectors is %f' %squared_error)\n",
    "X_train_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLE transformer\n",
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "lle = LocallyLinearEmbedding(n_neighbors=5, ## number of neighbors to consider for each point\n",
    "                             n_components=2, ## number of coordinates for the manifold\n",
    "                             reg=1e-3, # float # regularization constant, multiplying the trace of the local covariance matrix of the distances\n",
    "                             eigen_solver='auto', # 'arpack', or 'dense' # 'arpcak' uses Arnoldi decomposition to find the eigenvalues and eigenvectors; 'dense' uses a direct solver for the eigenvalue decomposition\n",
    "                             tol=1e-6, # float # tolerance for 'arpack' method, not used if eigen_solver=='dense'\n",
    "                             max_iter=100, # int # maximum number of iterations for the arpack solver, not used if eigen_solver=='dense'\n",
    "                             method='standard', # 'hessian', 'modified', or 'ltsa' # 'modified' requires n_neighbors>n_components, 'hessian' requires n_neighbors>n_components*(n_components + 3)/2\n",
    "                             neighbors_algorithm='auto', # 'brute', 'kd_tree', or 'ball_tree' # algorithm to use for nearest neighbors search\n",
    "                             n_jobs=None, # int # number of parallel jobs to run\n",
    "                             random_state=None) ## used when eigen_solver=='auto' or arpack'\n",
    "lle.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lle.embedding_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lle.reconstruction_error_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lle.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array_equal(X_train_transformed, lle.embedding_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.array_equal(X_train_transformed, lle.transform(X_train)) # theoretically they are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(np.abs(lle.embedding_ - lle.transform(X_train))) # but there is tiny differences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 谱嵌入 Spectral Embedding\n",
    "Using a spectral decomposition of the graph Laplacian to find a lower dimensional representation of the input. The graph generated can be considered as a discrete approximation of the low dimensional manifold in the high dimensional space. Minimization of a cost function based on the graph ensures that points close to each other on the manifold are mapped close to each other in the low dimensional space, preserving local distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import SpectralEmbedding\n",
    "spectral_embedding = SpectralEmbedding(n_components=2, ## dimension of the projected subspace\n",
    "                                       n_neighbors=None, ## number of nearest neighbors for nearest neighbors graph building, None sets it to max(n_samples/10, 1)\n",
    "                                       affinity='nearest_neighbors', # 'rbf', 'precomputed', 'precomputed_nearest_neighbors', or callable # callable passed as affinity function takes in data matrix (n_samples, n_features) and return affinity matrix (n_samples, n_samples)\n",
    "                                       gamma=None, # float # kernel coefficient for rbf kernel, None sets it to 1/n_features\n",
    "                                       eigen_solver=None, # 'arpack', 'lobpcg', or 'amg' # 'amg' requires pyamg to be installed\n",
    "                                       n_jobs=None, ## number of parallel jobs\n",
    "                                       random_state=None) ## used when eigen_solver=='amg'\n",
    "spectral_embedding.fit_transform(X_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectral_embedding.embedding_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectral_embedding.affinity_matrix_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4 多维缩放 Multi-dimensional Scaling (MDS)\n",
    "Based on similarity or dissimilarity seeking a low-dimensional representation of the data in which the distances respect well the distances in the original high-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS\n",
    "mds = MDS(n_components=2, ## number of dimensions in which to immerse the dissimilarities\n",
    "          metric=True, ## True performs metric MDS and False performs nonmetric MDS\n",
    "          dissimilarity='euclidean', # or 'precomputed' # dissimilarity measure to use, 'precomputed' requires input to be dissimilarity matrix\n",
    "          n_init=4, ## number of time the SMACOF algorithm will be run with different initializations\n",
    "          max_iter=300, ## maximum number of iterations of the SMACOF algorithm for a single run\n",
    "          eps=1e-3, # float # relative tolerance with respect to stress at which to declare convergence\n",
    "          n_jobs=None, ## number of parallel jobs to use\n",
    "          random_state=0)\n",
    "mds.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mds.embedding_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(np.abs(mds.embedding_ - mds.fit_transform(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mds.dissimilarity_matrix_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.5 TSNE t-distributed Stochastic Neighbor Embedding\n",
    "TSNE converts similarities between data points to joint probabilities and tries to minimize the Kullback-Leibler divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional data, which allows it to be particularly sensitive to local structure.\n",
    "\n",
    "__Advantages:__\n",
    "- revealing the structure at many scales on a single map\n",
    "- revealing data that lie in multiple, different, manifolds or clusters\n",
    "- reducing the tendency to crowd points together at the center\n",
    "\n",
    "__Disadvantages:__\n",
    "- computationally expensive, and can take several hours on million-sample datasets where PCA will finish in seconds or minutes\n",
    "- the Barnes-Hut t-SNE method is limited to two or three dimensional embeddings\n",
    "- stochastic and multiple restarts with different seeds can yield different embeddings\n",
    "- global structure is not explicitly preserved (mitigated by `init='pca'`)\n",
    "\n",
    "\\* Not recommended if the number of features is very high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, ## dimension of the embedded space\n",
    "            perplexity=30.0, # float # larger datasets usually require a larger perplexity; consider selecting a value between 5 and 50\n",
    "            early_exaggeration=12.0, # float # for larger values, the space between natural clusters will be larger in the embedded space\n",
    "            learning_rate=200.0, # float # usually in the range [10.0, 1000.0], may affect density of points\n",
    "            n_iter=1000, # int # maximum number of iterations for the optimization, should be at least 250\n",
    "            n_iter_without_progress=300, # maximum number of iterations without progress before we abort the optimization\n",
    "            min_grad_norm=1e-07, # float # if the gradient norm is below this threshold, the optimization will be stopped\n",
    "            metric='euclidean', # str or callable # metric to use when calculating distance between instances in a feature array\n",
    "            init='random', # 'pca', or ndarray of shape(n_samples, n_components) # initialization of embedding, 'pca' cannot be used with precomputed distances and is usually more globally stable than 'random'\n",
    "            random_state=None, \n",
    "            method='barnes_hut', # or 'exact' # 'exact' is slower but exact, used when nearest_neighbor errors need to be better than 3%\n",
    "            angle=0.5, # float # used if method='barnes_hut' to control the trade-off between speed and accuracy\n",
    "            n_jobs=None)\n",
    "X_train_transformed = tsne.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 聚类 `sklearn.cluster`\n",
    "\\* Used for __Clustring__ and __Biclustering__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Table of Clustering Algorithms__\n",
    "\n",
    "Method name|Parameters|Scalability|Usecase|Geometry (metric used)\n",
    ":---|:---|:---|:---|:---\n",
    "__3.1.1__ K-Means|number of clusters|Very large `n_samples`, medium `n_clusters` with MiniBatch code|General-purpose, even cluster size, flat geometry, not too many clusters|Distances between points\n",
    "__3.3.2__ Affinity propagation|damping, sample preference|Not scalable with `n_samples`|Many clusters, uneven cluster size, non-flat geometry|Graph distance (e.g. nearest-neighbor graph)\n",
    "__3.3.3__ Mean-shift|bandwidth|Not scalable with `n_samples`|Many clusters, uneven cluster size, non-flat geometry|Distances between points\n",
    "__3.3.4__ Spectral clustering|number of clusters|Medium `n_samples`, small `n_clusters`|Few clusters, even cluster size, non-flat geometry|Graph distance (e.g. nearest-neighbor graph)\n",
    "__3.3.5__ Agglomerative clustering|number of clusters or distance threshold, linkage type, distance|Large `n_samples` and `n_clusters`|Many clusters, possibly connectivity constraints, non Euclidean distances|Any pairwise distance\n",
    "__3.3.6__ DBSCAN|neighborhood size|Very large `n_samples`, medium `n_clusters`|Non-flat geometry, uneven cluster sizes|Distances between nearest points\n",
    "__3.3.7__ OPTICS|minimum cluster membership|Very large `n_samples`, large `n_clusters`|Non-flat geometry, uneven cluster sizes, variable cluster density|Distances between points\n",
    "__3.3.8__ Birch|branching factor, threshold, optional global clusterer|Large `n_clusters` and `n_samples`|Large dataset, outlier removal, data reduction|Euclidean distance between points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 K-means\n",
    "Trying to separate samples into n groups of equal variance, minimizing a criterion known as the inertia or within-cluster sum-of-squares: $\\sum_{i=0}^{n}\\min_{\\mu_j \\in C}(||x_i - \\mu_j||^2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1.1 K-means clustring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=4, ## number of clusters to form, 8 by default\n",
    "                init='k-means++', # 'random', callable, or array-like of shape (n_clusters, n_features)\n",
    "                n_init=10, # int # nomber of times the k-means will be run with different centroid seeds\n",
    "                max_iter=300, # maximum number of iterations of the k-means algorithm for a single run\n",
    "                tol=1e-4, # float # relative tolerance with regards to Frobenius norm of the difference in the cluster centers of two consecutive iterations to declare convergence\n",
    "                algorithm='auto', # 'full', or 'elkan' # k-means algorithm to use\n",
    "                random_state=None) ## determine random number generation for centroid initialization\n",
    "kmeans.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.labels_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.predict(X_test).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1.2 Mini-Batch K-means clustring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "minibatch_kmeans = MiniBatchKMeans(n_clusters=4, ## number of clusters to form, 8 by default\n",
    "                                   init='k-means++', # 'random', callable, or array-like of shape (n_clusters, n_features)\n",
    "                                   max_iter=100, ## maximum number of iterations over the complete dataset before stopping independently of any early stopping criterion heuristics\n",
    "                                   batch_size=100, ## size of the mini batches\n",
    "                                   compute_labels=True, ## whether to compute label assignment and inertia for the complete dataset once the minibatch optimization has converged in fit\n",
    "                                   tol=0.0, # float # control early stopping based on the relative center changes, 0.0 disables convergence detection based on normalized center change\n",
    "                                   max_no_improvement=10, # int # control early stopping based on the consecutive number of mini batches that does not yield an improvement on the smoothed inertia\n",
    "                                   init_size=None, # int # number of samples to randomly sample for speeding up the initialization, required to be larger than n_clusters; None defaults 3*batch_size\n",
    "                                   n_init=3, # int # number of random initializations that are tried\n",
    "                                   reassignment_ratio=0.01, # float # a higher value means that the model will take longer to converge, but should converge in a better clustering\n",
    "                                   random_state=None)  ## determine random number generation for centroid initialization and random reassignment\n",
    "minibatch_kmeans.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minibatch_kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minibatch_kmeans.labels_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minibatch_kmeans.predict(X_test[4:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minibatch_kmeans.partial_fit(X_test) # update k-means estimator on a single mini-batch input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 吸引子传播 Affinity Propagation\n",
    "AffinityPropagation creates clusters by sending messages between pairs of samples until convergence. The messages sent between pairs represent the suitability for one sample to be the exemplar of the other, which is updated in response to the values from other pairs.\n",
    "\n",
    "\\* Not recommended for large sized datasets due to its complexity\n",
    "\n",
    "Demo: <https://scikit-learn.org/stable/auto_examples/cluster/plot_affinity_propagation.html?highlight=demo>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AffinityPropagation\n",
    "apcluster = AffinityPropagation(damping=0.5, # float between 0.5 and 1 # the extent to which the current value is maintained relative to incoming values\n",
    "                                max_iter=200, ## maximum number of iterations\n",
    "                                convergence_iter=15, ## number of iterations with no change in the number of estimated clusters that stops the convergence\n",
    "                                preference=None, # float, or array-like of shape (n_samples) # preferences for each point-points, larger values are more likely to be chosen as exemplars\n",
    "                                affinity='euclidean', # or 'precomputed' # 'euclidean' uses the negative squared euclidean distance between points\n",
    "                                random_state=0)\n",
    "X_train_labels = apcluster.fit_predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apcluster.cluster_centers_indices_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3 均值偏移 Mean Shift\n",
    "A centroid based algorithm, working by updating candidates for centroids to be the mean of the points within a given region. These candidates are then filtered in a post-processing stage to eliminate near-duplicates to form the final set of centroids.\n",
    "\n",
    "Demo: <https://scikit-learn.org/stable/auto_examples/cluster/plot_mean_shift.html?highlight=demo>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MeanShift\n",
    "mean_shift = MeanShift(bandwidth=None, # float # bandwidth used in the RBF kernel; if not given, estimated using sklearn.cluster.estimate_bandwidth\n",
    "                       seeds=None, # array-like of shape (n_samples, n_features) # seeds used to initialize kernels; if not set, calculated by clustering.get_bin_seeds with bandwidth as the grid size and default values for other parameters\n",
    "                       bin_seeding=False, ## whether initial kernel locations are discretized points that are binned onto a grid whose coarseness corresponds to the bandwidth, ignored if seeds!=None\n",
    "                       min_bin_freq=1, # int # to speed up, accept only those bins with at least min_bin_freq points as seeds\n",
    "                       cluster_all=True, ## whether to cluster all points, if None then orphans that are not within any kernel will be given\n",
    "                       max_iter=300, # int # maximum number of iterations for per seed point if has not converged\n",
    "                       n_jobs=None) ## number of parallel jobs to use\n",
    "mean_shift.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_shift.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array_equal(mean_shift.labels_,\n",
    "               mean_shift.predict(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.4 谱聚类 Spectral Clustering\n",
    "Spectral Clustering performs a low-dimension embedding of the affinity matrix between samples, followed by clustering, e.g., by KMeans, of the components of the eigenvectors in the low dimensional space.\n",
    "\n",
    "\\* Very useful when the structure of the individual clusters is highly non-convex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "spectral_clustering = SpectralClustering(n_clusters=4, ## number of clusters to form, 8 by default\n",
    "                                         n_components=None, # int # None defaults n_clusters\n",
    "                                         eigen_solver=None, # 'arpack', 'lobpcg', or 'amg' # 'amg' requires pyamg to be installed; None defaults 'arpack'\n",
    "                                         eigen_tol=0.0, # float # stopping criterion for eigen decomposition of the Laplacian matrix when eigen_solver='arpack'\n",
    "                                         random_state=None, ## used for the initialization of the lobpcg eigen vectors decomposition and k-means initialization\n",
    "                                         affinity='rbf', # 'nearest_neighbors', 'poly', 'sigmoid', 'precomputed', 'precomputed_nearest_neighbors' or kernels supported by pairwise_kernels\n",
    "                                         gamma=1.0, # float # kernel coefficient for rbf, poly, sigmoid, laplacian and chiw kernels, ignored if affinity=='nearest_neighbors'\n",
    "                                         n_neighbors=10, ## number of neighbors to use when constructing the affinity matrix using the nearest neighbors method, ignored if affinity=='rbf'\n",
    "                                         degree=3, ## degree of the polynomial kernel, ignored by other kernels\n",
    "                                         assign_labels='kmeans', # or 'discretize' # strategy to use to assign labels in the embedding space\n",
    "                                         n_init=10, # int # number of times the k-means algorithm will be run with different centroid seeds\n",
    "                                         coef0=1., # float # zero coefficient ffor polynomial and sigmoid kernels, ignored by other kernels\n",
    "                                         kernel_params=None, # dict of str # parameters and values for kernel passed as callable object, ignored by other kernels\n",
    "                                         n_jobs=None) ## number of parallel jobs to run\n",
    "X_train_transformed = spectral_clustering.fit_predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.5 层次聚类 Hierarchical Clustering\n",
    "Hierarchical clustering is a general family of clustering algorithms that build nested clusters by merging or splitting them successively. This hierarchy of clusters is represented as a tree (or dendrogram)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "agglomerative_clustering = AgglomerativeClustering(n_clusters=4, # int or None # number of clusters to find, must be None if distance_threshold is not None\n",
    "                                                   distance_threshold=None, # float # if the linkage distance is above the threshold, clusters will not be merged; if not None, n_clusters must be None and compute_full_tree must be True\n",
    "                                                   compute_full_tree='auto', # or bool # stop early the constructin of the tree at n_clusters; must be True if distance_threshold is not None\n",
    "                                                   linkage='ward', # 'complete', 'average', or 'single' # linkage criterion that determines which distance to use between sets of ovservation\n",
    "                                                   affinity='euclidean', # 'l1', 'l2', 'manhattan', 'cosine', or callable # if linkage is 'ward', only 'euclidean' is accepted\n",
    "                                                   connectivity=None) # array-like or callable # connectivity matrix or a callable that transforms inpput data into a connectivity matrix\n",
    "agglomerative_clustering.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agglomerative_clustering.n_leaves_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agglomerative_clustering.labels_ # does not have transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Hierarchical Clustering Dendrogram\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "#--------------------------------------------------------\n",
    "def plot_dendrogram(model, **kwargs):\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack([model.children_, model.distances_,\n",
    "                                      counts]).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)\n",
    "#--------------------------------------------------------\n",
    "\n",
    "# setting distance_threshold=0 ensures we compute the full tree.\n",
    "agglomerative_clustering = AgglomerativeClustering(distance_threshold=0, n_clusters=None)\n",
    "\n",
    "agglomerative_clustering.fit(X_train)\n",
    "\n",
    "#-------------------------------------------------\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "# plot the top three levels of the dendrogram\n",
    "plot_dendrogram(agglomerative_clustering, truncate_mode='level', p=3)\n",
    "plt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.6 Density-Based Spatial Clustering of Applications with Noise (DBSCAN)\n",
    "The DBSCAN algorithm views clusters as areas of high density separated by areas of low density. Due to this rather generic view, clusters found by DBSCAN can be any shape, as opposed to k-means which assumes that clusters are convex shaped.\n",
    "\n",
    "Demo: <https://scikit-learn.org/stable/auto_examples/cluster/plot_dbscan.html?highlight=demo>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "dbscan = DBSCAN(eps=0.5, # float # maximum distance between two samples for one to be considered as in the neighborhood of the other\n",
    "                min_samples=5, ## number of samples in a neighborhood for a point to be considered as a core point\n",
    "                metric='euclidean', # string or callable allowed by sklearn.metrics.pairwise_distances # metric to use when calculating distance between instances in a feature array\n",
    "                metric_params=None, # dict # additional keyword arguments for the metric function\n",
    "                algorithm='auto', # 'ball_tree', 'kd_tree', or 'brute' # algorithm used to compute pointwise distances and find nearest neighbors\n",
    "                leaf_size=30, # int # leaf size passed to BallTree or KDTree\n",
    "                p=None, # float # the power of the Minkowski metric to calculate distance between points, None defaults 2\n",
    "                n_jobs=None) ## number of parallel jobs to run\n",
    "dbscan.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan.core_sample_indices_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.7 Ordering Points To Identify the Clustering Structure (OPTICS)\n",
    "Closely related to DBSCAN, OPTICS also finds core sample of high density and expands clusters from them. However, OPTICS keeps cluster hierarchy for a variable neighborhood radius, which makes it better suited for usage on large datasets.\n",
    "\n",
    "Domo: <https://scikit-learn.org/stable/auto_examples/cluster/plot_optics.html#sphx-glr-auto-examples-cluster-plot-optics-py>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import OPTICS\n",
    "optics = OPTICS(min_samples=5, # int>1 or float in (0, 1) # amount of samples in a neighborhood for a point to be considered as a core point, proportioned samples will be rounded to be at least 2\n",
    "                max_eps=np.inf, # float # maximum distance between two samples for one to be considered as in the neighborhood of the other, defaulted inf identifies clusters across all scales\n",
    "                metric='minkowski', # str or callable from sklearn.metrics or scipy.spatial.distance # metric to use for distance computation\n",
    "                p=2, # int # the power of the Minkowski metric\n",
    "                metric_params=None, # dict # additional keyword arguments for the metric function\n",
    "                cluster_method='xi', # or 'dbscan' # extraction method used to extract clusters using the calculated reachability and ordering\n",
    "                xi=0.05, # float in (0, 1) # used only when cluster_method=='xi', to determine the minimum steepness on the reachability plot that constitutes a cluster boundary\n",
    "                predecessor_correction=True, ## used only when cluster_method=='xi', whether to correct clusters according to the predecessors calculated by OPTICS\n",
    "                min_cluster_size=None, # int>1 or float in (0, 1) # used only when cluster_method=='xi', minimum number of samples in an OPTICS cluster, expressed as an absolute number or a fraction of the number of samples (rounded to be at least 2)\n",
    "                eps=None, # float # used only when cluster_method=='dbscan', maximum distance between two samples for one to be considered as in the neighborhood of the other\n",
    "                algorithm='auto', # 'ball_tree', 'kd_tree', or 'brute' # algorithm used to compute pointwise distances and find nearest neighbors\n",
    "                leaf_size=30, # int # leaf size passed to BallTree or KDTree\n",
    "                n_jobs=None) ## number of parallel jobs to run\n",
    "optics.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optics.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optics.core_distances_ # Distance at which each sample becomes a core point, indexed by object order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.8 Balanced Iterative Reducing and Clustering using Hierarchies (BIRCH)\n",
    "BIRCH is a memory-efficient, online-learning algorithm provided as an alternative to MiniBatchKMeans. It constructs a tree data structure with the cluster centroids being read off the leaf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import Birch\n",
    "birch = Birch(threshold=0.5, # float # the radius of the subcluster obtained by merging a new sample and the closest subcluster should be lesser than the threshold\n",
    "              branching_factor=50, # int # maximum number of CF subclusters in each node\n",
    "              n_clusters=3, # int or sklearn.cluster estimator # number of clusters to obtain\n",
    "              compute_labels=True) ## whether or not to compute labels for each fit\n",
    "birch.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "birch.labels_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "birch.predict(X_test[3:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "birch.transform(X_test[3:4]) # transform X into subcluster centroids dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.9 双聚类 Biclustering\n",
    "Biclustering algorithms simultaneously cluster rows and columns of a data matrix. These clusters of rows and columns are known as biclusters. Each determines a submatrix of the original data matrix with some desired properties.\n",
    "\n",
    "Instead of considering the input dataset as multi-featured samples, this method tries to see the input as an n_rows*n_columns board and to find subsets that have some desired properities. (This is my personal view and I am not sure if it is right)\n",
    "\n",
    "To implement Biclustering, see related __user gide__ <https://scikit-learn.org/stable/modules/biclustering.html> ,\\\n",
    "`sklearn.cluster.SpectralBiclustering` <https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralBiclustering.html#sklearn.cluster.SpectralBiclustering>, \\\n",
    "and `sklearn.cluster.SpectralCoclustering` <https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralCoclustering.html#sklearn.cluster.SpectralCoclustering>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 成分分解 `sklearn.decomposition`\n",
    "\\* Used for __Dimensionality Reduction__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1 主成分分析 Principal component analysis (PCA)\n",
    "PCA is used to decompose a multivariate dataset in a set of successive orthogonal components that explain a maximum amount of the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.1.1 Principal component analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=None, # int, float, or str # number of components to keep;\n",
    "          svd_solver='auto', # 'full', 'arpack', or 'randomized'\n",
    "          # if n_components=='mel' and svd_solver=='full'/'auto', Minka's MLE is used to guess the dimension\n",
    "          # if 0<n_components<1 and svd_solver=='full'/'auto', select the number of components such that the amount of variance that needs to be explained is greater than the percentage\n",
    "          # if svd_solver=='arpack', the number of components must be strictly less than the minimum of n_features and n_samples\n",
    "          # None defaults n_components to min(n_samples, n_features)\n",
    "          tol=0.0, # float≥0 # tolerance for singular values computed by svd_solver=='arpack'\n",
    "          iterated_power='auto', # int≥0 # number of iterations for the power method computed by svd_solver=='randomized' \n",
    "          whiten=False, ## whether the components_ vectors are multiplied by the square root of n_samples and then divided by the singular values to ensure uncorrelated outputs with unit component-wise variances\n",
    "          random_state=None) ## used when svd_solver=='arpack' or 'randomized'\n",
    "pca.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_ # percentage of variance explained by each of the selected components\n",
    "accumulated_ratio = np.zeros([pca.n_components_, 2])\n",
    "for i in range(pca.n_components_):\n",
    "    accumulated_ratio[i][0] = i\n",
    "    accumulated_ratio[i][1] = np.sum(pca.explained_variance_ratio_[:i])\n",
    "accumulated_ratio[accumulated_ratio[:,1]<0.9, :] # present accumulated explained ratio that are less than 90%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.transform(X_test).shape # apply dimensionality reduction to X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.1.2 Incremental principal component analysis\n",
    "The IncrementalPCA object uses a different form of processing and allows for partial computations which almost exactly match the results of PCA while processing the data in a minibatch fashion.\n",
    "\n",
    "\\* much more memory efficient than PCA, and allows sparse input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA\n",
    "incremental_pca = IncrementalPCA(n_components=None, # int # number of components to keep; None defaults n_components to min(n_samples, n_features)\n",
    "                                 whiten=False, ## whether the components_ vectors are multiplied by the square root of n_samples and then divided by the singular values to ensure uncorrelated outputs with unit component-wise variances\n",
    "                                 batch_size=None) # int # number of samples to use for each batch, only used when calling fit(); None sets it to 5*n_features\n",
    "incremental_pca.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incremental_pca.explained_variance_ratio_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incremental_pca.n_samples_seen_ # number of samples processed by the estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incremental_pca.partial_fit(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.1.3 Kernel PCA\n",
    "KernelPCA is an extension of PCA which achieves non-linear dimensionality reduction through the use of kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "kernel_pca = KernelPCA(n_components=None, # int # number of components, None remains all non-zero components\n",
    "                       kernel='linear', # 'poly', 'rbf', 'sigmoid', 'cosine', or 'precomputed'\n",
    "                       gamma=None, # float # kernel coefficient for 'rbf', None defaults 1/n_features\n",
    "                       degree=3, # int # degree for 'poly' kernel, ignored by other kernels\n",
    "                       coef0=1, # float # independent term in 'poly' and 'sigmoid' kernels, ignored by other kernels\n",
    "                       kernel_params=None, # dict # parameters and values for kernel passed as callable object, ignored by other kernels\n",
    "                       fit_inverse_transform=False, ## whether to learn the inverse transform for non-precomputed kernels\n",
    "                       alpha=1.0, # float # hyperparameter of the ridge regression that learns the inverse transform (when fit_inverse_transform=True)\n",
    "                       eigen_solver='auto', # 'dense', or 'arpack' # if n_components is much less than the number of training samples, 'arpack' may be more efficient\n",
    "                       tol=0, # float # convergence tolerance for arpack, 0 will choose optimal value\n",
    "                       max_iter=None, ## maximum number of iterations for 'arpack', None will choose optimal value\n",
    "                       remove_zero_eig=False, ## whether to remove components with zero eigenvalues so that the number of components in the output may be < n_components; ignored when n_components==None\n",
    "                       random_state=None, ## used when eigen_solver=='arpack'\n",
    "                       n_jobs=None) ## number of parallel jobs to run\n",
    "X_transformed = kernel_pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_pca.inverse_transform(X_transformed) # function when fit_inverse_transform==True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.1.4 Sparse PCA\n",
    "Finding the set of sparse components that can optimally reconstruct the data\n",
    "\n",
    "\\* Accepting sparse input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import SparsePCA\n",
    "sparse_pca = SparsePCA(n_components=None, # int # number of sparse atoms to extract\n",
    "                       alpha=1, # float # sparsity controlling parameter; higher values lead to sparser components\n",
    "                       ridge_alpha=0.01, # float # amount of ridge shrinkage to apply in order to improve conditioning when calling the transform method\n",
    "                       max_iter=1000, # int # maximum number of iterations to perform\n",
    "                       tol=1e-08, # float # tolerance for the stopping condition\n",
    "                       method='lars', # or 'cd' # 'lars' will be faster if the estimated components are sparse\n",
    "                       n_jobs=None, ## number of parallel jobs to run\n",
    "                       U_init=None, # ndarray of shape (n_samples, n_components) # initial values for the loadings for warm restart scenarios\n",
    "                       V_init=None, # ndarray of shape (n_samples, n_components) # initial values for the components for warm restart scenarios\n",
    "                       random_state=None) ## used during dictionary learning\n",
    "sparse_pca.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_pca.transform(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_pca.n_components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.1.5 Mini batch Sparse PCA\n",
    "\\* Accepting sparse input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import MiniBatchSparsePCA\n",
    "mini_batch_sparse_pca = MiniBatchSparsePCA(n_components=None, # int # number of sparse atoms to extract\n",
    "                                           alpha=1, # float # sparsity controlling parameter; higher values lead to sparser components\n",
    "                                           ridge_alpha=0.01, # float # amount of ridge shrinkage to apply in order to improve conditioning when calling the transform method\n",
    "                                           n_iter=100, ## number of iterations to perform for each mini batch\n",
    "                                           callback=None, # callable # callable that gets invoked every five iterations\n",
    "                                           batch_size=3, ## number of features to take in each mini batch\n",
    "                                           shuffle=True, ## whether to shuffle the data before splitting it in batches\n",
    "                                           n_jobs=None, ## number of parallel jobs to run\n",
    "                                           method='lars', # or 'cd' # 'lars' will be faster if the estimated components are sparse\n",
    "                                           random_state=None) ## used for random shuffling when shuffle is set to True\n",
    "mini_batch_sparse_pca.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_batch_sparse_pca.transform(X_test).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_batch_sparse_pca.n_components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2 截断奇异值分解/潜在语义分析 Truncated singular value decomposition / latent semantic analysis\n",
    "Performing linear dimensionality reduction by means of truncated singular value decomposition (SVD).\n",
    "\n",
    "Truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in `sklearn.feature_extraction.text`. In that context, it is known as latent semantic analysis (LSA).\n",
    "\n",
    "\\* Accepting sparse input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "lsa = TruncatedSVD(n_components=2, # int less than n_features # desired dimensionality of output data, 100 is recommended for lsa\n",
    "                   algorithm='randomized', # or 'arpack' # svd solver\n",
    "                   n_iter=5, # int # number of iterations for 'randomized' svd solver, ignored by 'arpack'\n",
    "                   random_state=None, ## used during randomized svd\n",
    "                   tol=0.0) # float # tolerance for 'arpack', 0 means machine precision\n",
    "lsa.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa.singular_values_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa.transform(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.3 字典学习 Dictionary learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.3.1 Generic dictionary learning\n",
    "Finding a dictionary (a set of atoms) that can best be used to represent data using a sparse code.\n",
    "\n",
    "$\\begin{split}(U^*, V^*) = \\underset{U, V}{\\operatorname{arg\\,min\\,}} & \\frac{1}{2}\n",
    "             ||X-UV||_2^2+\\alpha||U||_1 \\\\\n",
    "             \\text{subject to } & ||V_k||_2 = 1 \\text{ for all }\n",
    "             0 \\leq k < n_{\\mathrm{atoms}}\\end{split}$\n",
    "             \n",
    "\\* Accepting sparse input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import DictionaryLearning\n",
    "dict_learner = DictionaryLearning(n_components=None, # int # number of dictionary elements to extract\n",
    "                                   alpha=1, # float # sparsity controlling parameter\n",
    "                                   max_iter=1000, # int # maximum number of iterations to perform\n",
    "                                   tol=1e-08, # float # tolerance for numerical error\n",
    "                                   fit_algorithm='lars', # or 'cd' # 'lars' will be faster if the estimated components are sparse\n",
    "                                   transform_algorithm='omp', # 'lasso_lars', 'lasso_cd', 'lars', or 'threshold' # algorithm used to transform the data\n",
    "                                   transform_n_nonzero_coefs=None, # int # number of nonzero coefficients to target in each column of the solution, only used when transform_algorithm=='lars' or 'omp'; None defaults int(n_features/10)\n",
    "                                   transform_alpha=None, # float # parameter of transform algorithm, None defaults it to 1.0\n",
    "                                   code_init=None, # ndarray of shape (n_samples, n_components) # initial value for the code, for warm restart\n",
    "                                   dict_init=None, # ndarray of shape (n_components, n_features) # initial values for the dictionary\n",
    "                                   split_sign=False, ## whether to split the sparse feature vector into the concatenation of its negative part and its positive part\n",
    "                                   random_state=None, ## used for initializing the dictionary when dict_init is not specified\n",
    "                                   positive_code=False, ## whether to enforce positivity when finding the code\n",
    "                                   positive_dict=False, ## whether to enforce positivity when finding the dictionary\n",
    "                                   transform_max_iter=1000, # int # maximum number of iterations to perform if transform_algorithm='lasso_cd' or 'lasso_lars'\n",
    "                                   n_jobs=None) ## number of parallel jobs to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_sparse_coded_signal\n",
    "X, dictionary, code = make_sparse_coded_signal(\n",
    "    n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\n",
    "    random_state=42)\n",
    "X_transformed = dict_learner.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_learner.components_.shape # dictionary atoms extracted from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.3.2 Mini batch dictionary learning\n",
    "Implementing a faster but less accurate version of the dictionary learning algorithm that is better suited for large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import MiniBatchDictionaryLearning\n",
    "mini_batch_dict_learner = MiniBatchDictionaryLearning(n_components=None, # int # number of dictionary elements to extract\n",
    "                                                      alpha=1, # float # sparsity controlling parameter\n",
    "                                                      n_iter=1000, # int # total number of iterations to perform\n",
    "                                                      fit_algorithm='lars', # or 'cd' # 'lars' will be faster if the estimated components are sparse\n",
    "                                                      batch_size=3, # int # number of samples in each mini-batch\n",
    "                                                      shuffle=True, ## whether to shuffle the samples before forming batches\n",
    "                                                      dict_init=None, # ndarray of shape (n_components, n_features) # initial values of the dictionary for warm restart scenarios\n",
    "                                                      transform_algorithm='omp', # 'lasso_lars', 'lasso_cd', 'lars', or 'threshold' # algorithm used to transform the data\n",
    "                                                      transform_n_nonzero_coefs=None, # int # number of nonzero coefficients to target in each column of the solution, only used when transform_algorithm=='lars' or 'omp'; None defaults int(n_features/10)\n",
    "                                                      transform_alpha=None, # float # parameter of transform algorithm, None defaults it to 1.0\n",
    "                                                      split_sign=False, ## whether to split the sparse feature vector into the concatenation of its negative part and its positive part\n",
    "                                                      random_state=None, ## used for initializing the dictionary when dict_init is not specified\n",
    "                                                      positive_code=False, ## whether to enforce positivity when finding the code\n",
    "                                                      positive_dict=False, ## whether to enforce positivity when finding the dictionary\n",
    "                                                      transform_max_iter=1000, # int # maximum number of iterations to perform if transform_algorithm='lasso_cd' or 'lasso_lars'\n",
    "                                                      n_jobs=None) ## number of parallel jobs to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_sparse_coded_signal\n",
    "X, dictionary, code = make_sparse_coded_signal(\n",
    "    n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\n",
    "    random_state=42)\n",
    "X_transformed = mini_batch_dict_learner.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(X_transformed == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_batch_dict_learner.random_state_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.4 因子分析 Factor Analysis\n",
    "In Factor Analysis model, the observations are assumed to be caused by a linear transformation of lower dimensional latent factors and added Gaussian noise (zero mean and has an arbitrary diagonal covariance matrix). Without loss of generality the factors are distributed according to a Gaussian with zero mean and unit covariance.\n",
    "\n",
    "The main advantage for Factor Analysis over PCA is that it can model the variance in every direction of the input space independently (heteroscedastic noise), which allows better model selection than probabilistic PCA in the presence of heteroscedastic noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import FactorAnalysis\n",
    "factor_analysis = FactorAnalysis(n_components=3, # int # dimensionality of latent space, the number of components\n",
    "                                 tol=0.01, # float # stopping tolerance for log-likelihood increase\n",
    "                                 max_iter=1000, # int # maximum number of iterations\n",
    "                                 noise_variance_init=None, # ndarray of shape (n_features,) # initial guess of the noise variance for each feature, None defaults it to np.ones(n_features)\n",
    "                                 svd_method='randomized', # or 'lapack' # for most applications ‘randomized’ will be sufficiently precise while providing significant speed gains, accuracy can also be improved by setting higher values for iterated_power; for maximum precision choose ‘lapack’\n",
    "                                 iterated_power=3, # int # number of iterations for the power method, only used when svd_method=='randomized'\n",
    "                                 # rotation=None, # 'varimax', or 'quartimax' # new in version 0.24.; rotation method of the factors\n",
    "                                 random_state=0) ## used when svd_method=='randomized'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transformed = factor_analysis.fit_transform(X)\n",
    "X_transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.5 独立成分分析 Independent component analysis (ICA)\n",
    "With assuming non-Gaussian priors on the latent variables, and excluding noise a term, ICA model separates a multivariate signal into additive subcomponents that are maximally independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import FastICA\n",
    "ica = FastICA(n_components=3, # int # number of components to use\n",
    "              algorithm='parallel', # or 'deflation' # apply parallel or deflational algorithm\n",
    "              whiten=True, ## set it to False only if the input has been whitened\n",
    "              fun='logcosh', # 'exp', 'cube', or callable # functional form of the G function used in the approximation to neg-entropy\n",
    "              fun_args=None, # dict # arguments to send to the functional form; if fun=’logcosh’, None will set value {‘alpha’ : 1.0}\n",
    "              max_iter=200, # int # maximum number of iterations during fit\n",
    "              tol=0.0001, # float # tolerance on update at each iteration\n",
    "              w_init=None, # ndarray of shape (n_components, n_components) # mixing matrix to be used to initialize the algorithm\n",
    "              random_state=None) ## used to initialize w_init when not specified\n",
    "X_transformed = ica.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ica.inverse_transform([[0.0776125 , -0.13868129,  0.0942953]]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.6 非负矩阵分解 Non-negative matrix factorization (NMF or NNMF)\n",
    "By optimizing the distance between the input and the product of the two decomposed matrices, NMF is an alternative approach to decomposition that assumes that the data and the components are non-negative.\n",
    "\n",
    "\\* Efficient for representing images and text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "nmf = NMF(n_components=3, # int # number of components\n",
    "          init=None, # 'random', 'nndsvd', 'nndsvda', 'nndsvdar', or 'custom' # method used to initialize the procedure\n",
    "          solver='cd', # or 'mu' # numerical solver to use\n",
    "          beta_loss='frobenius', # 'kullback-leibler', 'itakura-saito', or float\n",
    "          tol=0.0001, # float # tolerance of the stopping condition\n",
    "          max_iter=200, # int # maximum number of iterations before timing out\n",
    "          random_state=None, ## used for initialisation (when init == ‘nndsvdar’ or ‘random’), and in Coordinate Descent\n",
    "          alpha=0.0, # float # constant that multiplies the regularization terms\n",
    "          l1_ratio=0.0, # float in (0, 1) # regularization mixing parameter\n",
    "          # regularization='both', # 'components', 'transformation', or None # new in version 0.24; select whether the regularization affects the components (H), the transformation (W), both or none of them\n",
    "          shuffle=False) ## whether to randomize the order of coordinates in the CD solver\n",
    "nmf.fit(X)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transformed = nmf.transform(X)\n",
    "X_transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4* 随机映射 `sklearn.random_projection`\n",
    "Implementing a simple and computationally efficient way to reduce the dimensionality of the data by trading a controlled amount of accuracy (as additional variance) for faster processing times and smaller model sizes. \n",
    "\n",
    "\\* Used for __Dimensionality Reduction__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4*.1 高斯随机映射 Gaussian random projection\n",
    "The method reduces the dimensionality by projecting the original input space on a randomly generated matrix where components are drawn from the following distribution $N(0, \\frac{1}{n_{components}})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "gaussian_random_projection = GaussianRandomProjection(n_components='auto', # or int # dimensionality of the target projection space; 'auto' automatically adjusts it according the Johnson-Lindenstrauss lemma\n",
    "                                                      eps=0.1, # float # parameter to control the quality of the embedding according to the Johnson-Lindenstrauss lemma when n_components is set to 'auto'\n",
    "                                                      random_state=None) ## used to generate the projection matrix at fit time\n",
    "# Note that Johnson-Lindenstrauss lemma can yield very conservative estimated of the required number of components as it makes no assumption on the structure of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.rand(100, 10000)\n",
    "X_transformed = gaussian_random_projection.fit_transform(X)\n",
    "X_transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4*.2 稀疏随机映射 Sparse random projection\n",
    "The method reduces the dimensionality by projecting the original input space using a sparse random matrix.\n",
    "\n",
    "\\* Accept sparse input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.random_projection import SparseRandomProjection\n",
    "sparse_random_projection = SparseRandomProjection(n_components='auto', # or int # dimensionality of the target projection space; 'auto' automatically adjusts it according the Johnson-Lindenstrauss lemma\n",
    "                                                  eps=0.1, # float # parameter to control the quality of the embedding according to the Johnson-Lindenstrauss lemma when n_components is set to 'auto'\n",
    "                                                  density='auto', # or float in (0, 1] # ratio of non-zero component in the random projection matrix; 'auto' set it to 1/sqrt(n_features)\n",
    "                                                  dense_output=False, ## whether to output of the random projection as a dense numpy array\n",
    "                                                  random_state=None) ## used to generate the projection matrix at fit time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.rand(100, 10000)\n",
    "X_transformed = sparse_random_projection.fit_transform(X)\n",
    "X_transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 异常值检测 `sklearn.***`\n",
    "Outlier detection and novelty detection are both used for anomaly detection, where one is interested in detecting abnormal or unusual observations. This section involves multiple modules.\n",
    "\n",
    "\\* Used for __Anomaly Detection__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.1 输入数据包含异常值 Outlier Detection\n",
    "The training data contains outliers which are defined as observations that are far from the others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.1.1 Fitting an elliptic envelope\n",
    "An object for detecting outliers in a Gaussian distributed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.covariance import EllipticEnvelope\n",
    "elliptic_detector = EllipticEnvelope(assume_centered=False, ## whethre to compute the support of robust location and covariance estimates, which is useful to work with data whose mean is significantly equal to zero but is not exactly zero\n",
    "                                     support_fraction=None, # float in (0, 1) # proportion of points to be included in the support of the raw MCD estimate, None defaults it to (n_samples + n_features + 1)/2\n",
    "                                     contamination=0.1, # float in (0, 0.5] # proportion of outliers in the data set\n",
    "                                     random_state=None) ## used for shuffling the data\n",
    "elliptic_detector.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elliptic_detector.location_ # estimated robust location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elliptic_detector.predict(X) # -1 labels anomalies/outliers and +1 labels inliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elliptic_detector.predict([[2,3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.1.2 Isolation Forest\n",
    "The method ‘isolates’ observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "rf_detector = IsolationForest(n_estimators=100, # int # number of base estimators in the ensemble\n",
    "                              max_samples='auto', # int or float # number of samples to draw from X to train each base estimator\n",
    "                              contamination='auto', # or float in (0, 0.5] # proportion of outliers in the data set\n",
    "                              max_features=1.0, # int or float # number of features to draw from X to train each base estimator\n",
    "                              bootstrap=False, ## whether to sample random subsets with replacement\n",
    "                              n_jobs=None, # int # number of jobs to run in parallel\n",
    "                              random_state=None, ## used for the selection of the feature and split values for each branching step and each tree in the forest\n",
    "                              warm_start=False) ## whether to reuse the solution of the previous call to fit and add more estimators to the ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_detector.fit_predict(X) # -1 labels anomalies/outliers and +1 labels inliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.1.3 Local Outlier Factor\n",
    "The algorithm computes a score reflecting the degree of abnormality of the observations. It measures the local density deviation of a given data point with respect to its neighbors.\n",
    "\n",
    "\\* When applying for outlier detection (`novelty==False`), there are no `predict`, `decision_function` and `score_samples` methods but only a `fit_predict` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "neighbors_detector = LocalOutlierFactor(n_neighbors=20, # int # number of neighbors to use\n",
    "                                        algorithm='auto', # 'ball_tree', 'kd_tree', or 'brute' # algorithm used to compute the nearest neighbors\n",
    "                                        leaf_size=30, # int # leaf size passed to 'ball_tree' or 'kd_tree'\n",
    "                                        metric='minkowski', # str or callable # metric used for the distance computation\n",
    "                                        p=2, # int # parameter for the 'minkowski' metric\n",
    "                                        metric_params=None, # dict # additional keyword arguments for the metric function\n",
    "                                        contamination='auto', # float in (0, 0.5] # proportion of outliers in the data set\n",
    "                                        novelty=False, ##  whether to apply for novelty detection\n",
    "                                        n_jobs=None) ## number of parallel jobs to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors_detector.fit_predict(X) # -1 labels anomalies/outliers and +1 labels inliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors_detector.negative_outlier_factor_ # the higher, the more normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.2 输入数据不包含异常值 Novelty Detection\n",
    "The training data is not polluted by outliers and we are interested in detecting whether a new observation is an outlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.2.1 One-Class SVM\n",
    "Estimate the support of a high-dimensional distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "svm_detector = OneClassSVM(kernel='rbf', # 'linear', 'poly', 'sigmoid', or 'precomputed' # kernel type to be used in the algorithm\n",
    "                           degree=3, # int # degree of the polynomial kernel function when kernel=='poly'\n",
    "                           gamma='scale', # 'auto', or float # kernel coefficient when kernel=='rbf'/'poly'/'sigmoid'\n",
    "                           coef0=0.0, # float # independent term in kernel function when kernel=='poly'/'sigmoid'\n",
    "                           tol=0.001, # float # tolerance for stopping criterion\n",
    "                           nu=0.5, # float in (0, 1] # upper bound on the fraction of training errors and lower bound of the fraction of support vectors\n",
    "                           shrinking=True, ## whether to use the shrinking heuristic.\n",
    "                           max_iter=- 1) # int # hard limit on iterations within solver, -1 for no limit\n",
    "svm_detector.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_detector.fit_status_ # 0 if correctly fitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_detector.predict([[2, 3]]) # -1 labels anomalies/outliers and +1 labels inliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.2.2 Local Outlier Factor with `novelty` set to `True`\n",
    "By default, LocalOutlierFactor (3.5.1.3) is only meant to be used for outlier detection (novelty=False). If `novelty` is set to `True`, only `predict`, `decision_function`, and `score_samples` can be used on new unseen data and not on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "neighbors_detector = LocalOutlierFactor(n_neighbors=20, # int # number of neighbors to use\n",
    "                                        algorithm='auto', # 'ball_tree', 'kd_tree', or 'brute' # algorithm used to compute the nearest neighbors\n",
    "                                        leaf_size=30, # int # leaf size passed to 'ball_tree' or 'kd_tree'\n",
    "                                        metric='minkowski', # str or callable # metric used for the distance computation\n",
    "                                        p=2, # int # parameter for the 'minkowski' metric\n",
    "                                        metric_params=None, # dict # additional keyword arguments for the metric function\n",
    "                                        contamination='auto', # float in (0, 0.5] # proportion of outliers in the data set\n",
    "                                        novelty=True, ##  whether to apply for novelty detection\n",
    "                                        n_jobs=None) ## number of parallel jobs to run\n",
    "neighbors_detector.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors_detector.predict([[2, 3]]) # -1 labels anomalies/outliers and +1 labels inliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 神经网络 `sklearn.neural_network`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.1 伯努利受限玻尔兹曼机 Bernoulli Restricted Boltzmann Machine (RBM)\n",
    "A Restricted Boltzmann Machine with binary visible units and binary hidden units. Parameters are estimated using Stochastic Maximum Likelihood (SML), also known as Persistent Contrastive Divergence (PCD)\n",
    "\n",
    "\\* inputs are either binary values or values between 0 and 1, each encoding the probability that the specific feature would be turned on\n",
    "\n",
    "\\* Used for __Dimentionality Reduction__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import BernoulliRBM\n",
    "bernoulli_rbm = BernoulliRBM(n_components=20, # int # number of binary hidden units\n",
    "                             learning_rate=0.1, # float # learning rate for weight updates, reasonable values are in the 10**[0., -3.] range\n",
    "                             batch_size=10, # int # number of examples per minibatch\n",
    "                             n_iter=10, # int # number of iterations/sweeps over the training dataset to perform during training\n",
    "                             random_state=None) ## used for 1) Gibbs sampling from visible and hidden layers, 2) initializing components, 3)sampling from layers during fit, and 4)corrupting the data when scoring samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bernoulli_rbm.fit_transform(X_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bernoulli_rbm.partial_fit(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bernoulli_rbm.transform(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Model Selection and Evaluation `sklearn.model_selection`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, section 4 will use `model` to refer to the model to be evaluated. Adjust the name in the practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for regression\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "diabetes = datasets.load_diabetes()\n",
    "X = diabetes.data[:150]\n",
    "y = diabetes.target[:150]\n",
    "model = linear_model.Lasso()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for classification\n",
    "import numpy as np\n",
    "from sklearn import datasets, svm\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:150]\n",
    "y = iris.target[:150]\n",
    "class_names = iris.target_names\n",
    "model = svm.SVC(kernel='linear', C=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 交叉验证 Cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 Cross validation score\n",
    "The simplest way to evaluate a score by cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(model, X, y, cv=3)\n",
    "# without defining `scoring`,  the model’s default scorer (if available) is used\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Cross validation function and multiple metric evaluation\n",
    "Differing from the simpler way, this function\n",
    "- allows specifying multiple metrics for evaluation\n",
    "- returns a dict containing fit-times, score-times (and optionally training scores as well as fitted estimators) in addition to the test score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = ['explained_variance', 'max_error'] # the two metrics are for regression\n",
    "# or scoring = {'explained_variance': 'explained_variance', 'max_error': 'max_error'}, which specifies call names\n",
    "# scoring metrics will be introduced in 4.2\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "cv = ShuffleSplit(n_splits=3, test_size=0.3, random_state=0)\n",
    "# cross validation iterators will be introduced in 4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "scores = cross_validate(model, X, y,\n",
    "                        scoring=scoring, # str, callable, list/tuple, or dict # if None, the estimator’s score method is used\n",
    "                        cv=cv, # int, cross-validation generator or an iterable # None uses the default 5-fold cross validation\n",
    "                        n_jobs=None, # int # number of jobs to run in parallel\n",
    "                        pre_dispatch='2*n_jobs', # controls the number of jobs that get dispatched during parallel execution; reducing this number can be useful to avoid an explosion of memory consumption when more jobs get dispatched than CPUs can process\n",
    "                        return_train_score=False) ## whether to include train scores\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores['test_explained_variance']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3 Obtaining predictions by cross-validation\n",
    "This method is appropriate for\n",
    "- visualization of predictions obtained from different models\n",
    "- model blending: when predictions of one supervised estimator are used to train another estimator in ensemble methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "y_pred = cross_val_predict(model, X, y, cv=10,\n",
    "                           method='predict') # 'predict_proba', 'predict_log_proba', or 'decision_function' #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.scatter(y, y_pred, edgecolors=(0, 0, 0))\n",
    "plt.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=2)\n",
    "plt.xlabel('Measured')\n",
    "plt.ylabel('Predicted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 评分指标 Scoring metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Predefined values\n",
    "For the most common use cases, you can designate a scorer object with the scoring parameter; the table below shows all possible values.\n",
    "\n",
    "Classification|Clustering|Regression\n",
    ":---|:---|:---\n",
    "'accuracy'|'adjusted_mutual_info_score'|'explained_variance'\n",
    "'balanced_accuracy'|'adjusted_rand_score'|'max_error'\n",
    "'top_k_accuracy'|'completeness_score'|'neg_mean_absolute_error'\n",
    "'average_precision'|'fowlkes_mallows_score'|'neg_mean_squared_error'\n",
    "'neg_brier_score'|'homogeneity_score'|'neg_root_mean_squared_error'\n",
    "'f1'|'mutual_info_score'|'neg_mean_squared_log_error'\n",
    "'f1_micro'|'normalized_mutual_info_score'|'neg_median_absolute_error'\n",
    "'f1_macro'|'rand_score'|'r2'\n",
    "'f1_weighted'|'v_measure_score'|'neg_mean_poisson_deviance'\n",
    "'f1_samples'||'neg_mean_gamma_deviance'\n",
    "'neg_log_loss'||'neg_mean_absolute_percentage_error'\n",
    "'precision' etc.||\n",
    "'recall' etc.||\n",
    "'jaccard' etc.||\n",
    "'roc_auc'||\n",
    "'roc_auc_ovr'||\n",
    "'roc_auc_ovo'||\n",
    "'roc_auc_ovr_weighted'||\n",
    "'roc_auc_ovo_weighted'||\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 Score functions\n",
    "__Classification metrics__\n",
    "\n",
    "function|description\n",
    ":---|:---\n",
    "`metrics.accuracy_score`(y_true, y_pred, *[, …]) | Accuracy classification score.|\n",
    "`metrics.auc`(x, y) | Compute Area Under the Curve (AUC) using the trapezoidal rule.\n",
    "`metrics.average_precision_score`(y_true, …) | Compute average precision (AP) from prediction scores.\n",
    "`metrics.balanced_accuracy_score`(y_true, …) | Compute the balanced accuracy.\n",
    "`metrics.brier_score_loss`(y_true, y_prob, *) | Compute the Brier score loss.\n",
    "`metrics.classification_report`(y_true, y_pred, *) | Build a text report showing the main classification metrics.\n",
    "`metrics.cohen_kappa_score`(y1, y2, *[, …]) | Cohen’s kappa: a statistic that measures inter-annotator agreement.\n",
    "`metrics.confusion_matrix`(y_true, y_pred, *) | Compute confusion matrix to evaluate the accuracy of a classification.\n",
    "`metrics.dcg_score`(y_true, y_score, *[, k, …]) | Compute Discounted Cumulative Gain.\n",
    "`metrics.det_curve`(y_true, y_score[, …]) | Compute error rates for different probability thresholds.\n",
    "`metrics.f1_score`(y_true, y_pred, *[, …]) | Compute the F1 score, also known as balanced F-score or F-measure.\n",
    "`metrics.fbeta_score`(y_true, y_pred, *, beta) | Compute the F-beta score.\n",
    "`metrics.hamming_loss`(y_true, y_pred, *[, …]) | Compute the average Hamming loss.\n",
    "`metrics.hinge_loss`(y_true, pred_decision, *) | Average hinge loss (non-regularized).\n",
    "`metrics.jaccard_score`(y_true, y_pred, *[, …]) | Jaccard similarity coefficient score.\n",
    "`metrics.log_loss`(y_true, y_pred, *[, eps, …]) | Log loss, aka logistic loss or cross-entropy loss.\n",
    "`metrics.matthews_corrcoef`(y_true, y_pred, *) | Compute the Matthews correlation coefficient (MCC).\n",
    "`metrics.multilabel_confusion_matrix`(y_true, …) | Compute a confusion matrix for each class or sample.\n",
    "`metrics.ndcg_score`(y_true, y_score, *[, k, …]) | Compute Normalized Discounted Cumulative Gain.\n",
    "`metrics.precision_recall_curve`(y_true, …) | Compute precision-recall pairs for different probability thresholds.\n",
    "`metrics.precision_recall_fscore_support`(…) | Compute precision, recall, F-measure and support for each class.\n",
    "`metrics.precision_score`(y_true, y_pred, *[, …]) | Compute the precision.\n",
    "`metrics.recall_score`(y_true, y_pred, *[, …]) | Compute the recall.\n",
    "`metrics.roc_auc_score`(y_true, y_score, *[, …]) | Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores.\n",
    "`metrics.roc_curve`(y_true, y_score, *[, …]) | Compute Receiver operating characteristic (ROC).\n",
    "`metrics.top_k_accuracy_score`(y_true, y_score, *) | Top-k Accuracy classification score.\n",
    "`metrics.zero_one_loss`(y_true, y_pred, *[, …]) | Zero-one classification loss.\n",
    "\n",
    "__Regression metrics__\n",
    "\n",
    "function | description\n",
    ":---|:---\n",
    "`metrics.explained_variance_score`(y_true, …) | Explained variance regression score function.\n",
    "`metrics.max_error`(y_true, y_pred) | max_error metric calculates the maximum residual error.\n",
    "`metrics.mean_absolute_error`(y_true, y_pred, *) | Mean absolute error regression loss.\n",
    "`metrics.mean_squared_error`(y_true, y_pred, *) | Mean squared error regression loss.\n",
    "`metrics.mean_squared_log_error`(y_true, y_pred, *) | Mean squared logarithmic error regression loss.\n",
    "`metrics.median_absolute_error`(y_true, y_pred, *) | Median absolute error regression loss.\n",
    "`metrics.mean_absolute_percentage_error`(…) | Mean absolute percentage error regression loss.\n",
    "`metrics.r2_score`(y_true, y_pred, *[, …]) | R^2 (coefficient of determination) regression score function.\n",
    "`metrics.mean_poisson_deviance`(y_true, y_pred, *) | Mean Poisson deviance regression loss.\n",
    "`metrics.mean_gamma_deviance`(y_true, y_pred, *) | Mean Gamma deviance regression loss.\n",
    "`metrics.mean_tweedie_deviance`(y_true, y_pred, *) | Mean Tweedie deviance regression loss.\n",
    "\n",
    "__Multilabel ranking metrics__\n",
    "\n",
    "function | description\n",
    ":---|:---\n",
    "`metrics.coverage_error`(y_true, y_score, *[, …]) | Coverage error measure.\n",
    "`metrics.label_ranking_average_precision_score`(…) | Compute ranking-based average precision.\n",
    "`metrics.label_ranking_loss`(y_true, y_score, *) | Compute Ranking loss measure.\n",
    "\n",
    "__Clustering metrics__\n",
    "\n",
    "function | description\n",
    ":---|:---\n",
    "`metrics.adjusted_mutual_info_score`(…[, …]) | Adjusted Mutual Information between two clusterings.\n",
    "`metrics.adjusted_rand_score`(labels_true, …) | Rand index adjusted for chance.\n",
    "`metrics.calinski_harabasz_score`(X, labels) | Compute the Calinski and Harabasz score.\n",
    "`metrics.davies_bouldin_score`(X, labels) | Computes the Davies-Bouldin score.\n",
    "`metrics.completeness_score`(labels_true, …) | Completeness metric of a cluster labeling given a ground truth.\n",
    "`metrics.cluster.contingency_matrix`(…[, …]) | Build a contingency matrix describing the relationship between labels.\n",
    "`metrics.cluster.pair_confusion_matrix`(…) | Pair confusion matrix arising from two clusterings.\n",
    "`metrics.fowlkes_mallows_score`(labels_true, …) | Measure the similarity of two clusterings of a set of points.\n",
    "`metrics.homogeneity_completeness_v_measure`(…) | Compute the homogeneity and completeness and V-Measure scores at once.\n",
    "`metrics.homogeneity_score`(labels_true, …) | Homogeneity metric of a cluster labeling given a ground truth.\n",
    "`metrics.mutual_info_score`(labels_true, …) | Mutual Information between two clusterings.\n",
    "`metrics.normalized_mutual_info_score`(…[, …]) | Normalized Mutual Information between two clusterings.\n",
    "`metrics.rand_score`(labels_true, labels_pred) | Rand index.\n",
    "`metrics.silhouette_score`(X, labels, *[, …]) | Compute the mean Silhouette Coefficient of all samples.\n",
    "`metrics.silhouette_samples`(X, labels, *[, …]) | Compute the Silhouette Coefficient for each sample.\n",
    "`metrics.v_measure_score`(labels_true, …[, beta]) | V-measure cluster labeling given a ground truth.\n",
    "\n",
    "__Biclustering metrics__\n",
    "\n",
    "function | description\n",
    ":---|:---\n",
    "`metrics.consensus_score`(a, b, *[, similarity]) | The similarity of two sets of biclusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Cross validation iterators\n",
    "Generating indices that can be used to generate dataset splits according to different cross validation strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 For i.i.d. data\n",
    "While i.i.d. is a common assumption in machine learning theory, it rarely holds in practice. If one knows that the samples have been generated using a time-dependent process, it is safer to use a time-series aware cross-validation scheme. Similarly, if we know that the generative process has a group structure (samples collected from different subjects, experiments, measurement devices), it is safer to use group-wise cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.1.1 K-fold\n",
    "Dividing all the samples in  groups of samples, called folds (if , this is equivalent to the Leave One Out strategy), of equal sizes (if possible)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "cv = KFold(n_splits=5, shuffle=False, random_state=None)\n",
    "# (number of folds, whether to shuffle the data, randomness control of each fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.1.2 Repeated K-fold\n",
    "Repeating K-fold n times. It can be used when one requires to run KFold n times, producing different splits in each repetition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedKFold\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=10, random_state=None)\n",
    "# (number of folds, number of times to repeat, randomness control)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.1.3 Leave One Out (LOO)\n",
    "Each learning set is created by taking all the samples except one, the test set being the sample left out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "cv = LeaveOneOut()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.1.4 Leave P Out (LPO)\n",
    "Creating all the possible training/test sets by removing  samples from the complete set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeavePOut\n",
    "cv = LeavePOut(4)\n",
    "# (size of the test sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.1.5 Random permutations cross-validation a.k.a. Shuffle & Split\n",
    "Samples are first shuffled and then split into a pair of train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "cv = ShuffleSplit(n_splits=5, ## number of re-shuffling & splitting iterations\n",
    "                  train_size=0.7, # int or float in (0.0, 1.0) # amount of train sets\n",
    "                  random_state=None) ## randomness control of the training and testing indices produced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2 For imbalance data\n",
    "Some classification problems can exhibit a large imbalance in the distribution of the target classes: for instance there could be several times more negative samples than positive samples. In such cases it is recommended to use iterators with stratification based on class labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.2.1 Stratified K-fold\n",
    "A variation of k-fold that returns stratified folds: each set contains approximately the same percentage of samples of each target class as the complete set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=None)\n",
    "# (number of folds, whether to shuffle, randomness control)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.2.2 Stratified Shuffle Split\n",
    "A variation of ShuffleSplit which returns stratified splits, i.e which creates splits by preserving the same percentage for each target class as in the complete set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "cv = StratifiedShuffleSplit(n_splits=5, train_size=0.7, random_state=None)\n",
    "# (number of iterations, amount of train sets, randomness control)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.3 For grouped data\n",
    "The underlying generative process yield groups of dependent samples. Such a grouping of data is domain specific."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.3.1 Group K-fold\n",
    "A variation of k-fold which ensures that the same group is not represented in both testing and training sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "cv = GroupKFold(5)\n",
    "# (number of folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.3.2 Leave One Group Out\n",
    "A cross-validation scheme which holds out the samples according to a third-party provided array of integer groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "cv = LeaveOneGroupOut()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.3.3 Leave P Groups Out\n",
    "This method is similar as `LeaveOneGroupOut`, but removes samples related to  groups for each training/test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeavePGroupsOut\n",
    "cv = LeavePGroupsOut(5)\n",
    "# (number of groups to leave out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.3.4 Group Shuffle Split\n",
    "This method behaves as a combination of `ShuffleSplit` and `LeavePGroupsOut`, and generates a sequence of randomized partitions in which a subset of groups are held out for each split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "cv = GroupShuffleSplit(n_splits=5, train_size=0.7, random_state=None)\n",
    "# (number of iterations, amount of train sets, randomness control)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.4 For time series data\n",
    "Spliting time series data samples that are observed at fixed time intervals, into train/test sets. It is a variation of `KFold`. \n",
    "\n",
    "- In the kth split, it returns first k folds as train set and the (k+1)th fold as test set.\n",
    "- In each split, test indices must be higher than that in the previous split.\n",
    "\n",
    "Thus shuffling in cross validator is inappropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "cv = TimeSeriesSplit(n_splits=5, ## number of splits\n",
    "                     # gap=0, # new in version 0.24 # number of samples to exclude from the end of each train set before the test set\n",
    "                     # test_size=None, # new in version 0.24 # None defaults it to n_samples//(n_splits+1), which is the maximum allowed value with gap=0\n",
    "                     max_train_size=None) ## with test_size=None, None defaults train_size to i*n_samples//(n_splits+1)+n_samples%(n_splits+1) in the ith split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 调参 Tuning the hyper-parameters\n",
    "Hyper-parameters are parameters that are not directly learnt within estimators, which in scikit-learn are passed as arguments to the constructor of the estimator classes.\n",
    "\n",
    "This is a complex and time-consuming process. Therefore, I will try to learn an automated machine learning toolkit <https://github.com/automl/auto-sklearn> in the near weeks.\n",
    "\n",
    "For reference if you are interested in tuning by yourself, please see <https://scikit-learn.org/stable/modules/grid_search.html> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 评估可视化 Evaluation Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.1 验证曲线 Validation curve\n",
    "A validation curve shows scores for an estimator with different values of a specified parameter. \n",
    "\n",
    "If the training score and the validation score are both low, the estimator will be underfitting. If the training score is high and the validation score is low, the estimator is overfitting and otherwise it is working very well. A low training score and a high validation score is usually not possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "param_range = np.linspace(0, 3, 5)\n",
    "train_scores, test_scores = validation_curve(model, X, y,\n",
    "                                             param_name='alpha', # str # name of the parameter that will be varied, depending on the model\n",
    "                                             param_range=param_range, # array-like of shape (n_values,) # values of the parameter that will be evaluated\n",
    "                                             cv=None, # int, cross-validation generator or an iterable # cross-validation splitting strategy\n",
    "                                             scoring='r2', # str or callable # \n",
    "                                             #fit_params=None, # dict # new in version 0.24, parameters to pass to the fit method of the estimator\n",
    "                                             n_jobs=None) # int # number of jobs to run in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.title(\"Validation Curve\")\n",
    "plt.xlabel(r\"$\\alpha$\")\n",
    "plt.ylabel(\"Score\")\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "plt.plot(param_range, train_scores_mean, label=\"Training score\", color=\"darkorange\", lw=2)\n",
    "plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.2,\n",
    "                 color=\"darkorange\", lw=2)\n",
    "plt.plot(param_range, test_scores_mean, label=\"Cross-validation score\", color=\"navy\", lw=2)\n",
    "plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.2,\n",
    "                 color=\"navy\", lw=2)\n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.2 学习曲线 Learning curve\n",
    "A learning curve shows the validation and training score of an estimator for varying numbers of training samples.\n",
    "\n",
    "It is a tool to find out how much we benefit from adding more training data and whether the estimator suffers more from a variance error or a bias error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function is taken from <https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html>\n",
    "def plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None,\n",
    "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"\n",
    "    Generate 3 plots: the test and training learning curve, the training\n",
    "    samples vs fit times curve, the fit times vs score curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : estimator instance\n",
    "        An estimator instance implementing `fit` and `predict` methods which\n",
    "        will be cloned for each validation.\n",
    "\n",
    "    title : str\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like of shape (n_samples, n_features)\n",
    "        Training vector, where ``n_samples`` is the number of samples and\n",
    "        ``n_features`` is the number of features.\n",
    "\n",
    "    y : array-like of shape (n_samples) or (n_samples, n_features)\n",
    "        Target relative to ``X`` for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    axes : array-like of shape (3,), default=None\n",
    "        Axes to use for plotting the curves.\n",
    "\n",
    "    ylim : tuple of shape (2,), default=None\n",
    "        Defines minimum and maximum y-values plotted, e.g. (ymin, ymax).\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, default=None\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "\n",
    "          - None, to use the default 5-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - :term:`CV splitter`,\n",
    "          - An iterable yielding (train, test) splits as arrays of indices.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "    n_jobs : int or None, default=None\n",
    "        Number of jobs to run in parallel.\n",
    "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
    "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
    "        for more details.\n",
    "\n",
    "    train_sizes : array-like of shape (n_ticks,), dtype={int, float}\n",
    "        Relative or absolute numbers of training examples that will be used to\n",
    "        generate the learning curve. If the ``dtype`` is float, it is regarded\n",
    "        as a fraction of the maximum size of the training set (that is\n",
    "        determined by the selected validation method), i.e. it has to be within\n",
    "        (0, 1]. Otherwise it is interpreted as absolute sizes of the training\n",
    "        sets. Note that for classification the number of samples usually have\n",
    "        to be big enough to contain at least one sample from each class.\n",
    "        (default: np.linspace(0.1, 1.0, 5))\n",
    "    \"\"\"\n",
    "    if axes is None:\n",
    "        _, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "    axes[0].set_title(title)\n",
    "    if ylim is not None:\n",
    "        axes[0].set_ylim(*ylim)\n",
    "    axes[0].set_xlabel(\"Training examples\")\n",
    "    axes[0].set_ylabel(\"Score\")\n",
    "\n",
    "    train_sizes, train_scores, test_scores, fit_times, _ = \\\n",
    "        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n",
    "                       train_sizes=train_sizes,\n",
    "                       return_times=True)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    fit_times_mean = np.mean(fit_times, axis=1)\n",
    "    fit_times_std = np.std(fit_times, axis=1)\n",
    "\n",
    "    # Plot learning curve\n",
    "    axes[0].grid()\n",
    "    axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                         train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                         color=\"r\")\n",
    "    axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                         test_scores_mean + test_scores_std, alpha=0.1,\n",
    "                         color=\"g\")\n",
    "    axes[0].plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "                 label=\"Training score\")\n",
    "    axes[0].plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "                 label=\"Cross-validation score\")\n",
    "    axes[0].legend(loc=\"best\")\n",
    "\n",
    "    # Plot n_samples vs fit_times\n",
    "    axes[1].grid()\n",
    "    axes[1].plot(train_sizes, fit_times_mean, 'o-')\n",
    "    axes[1].fill_between(train_sizes, fit_times_mean - fit_times_std,\n",
    "                         fit_times_mean + fit_times_std, alpha=0.1)\n",
    "    axes[1].set_xlabel(\"Training examples\")\n",
    "    axes[1].set_ylabel(\"fit_times\")\n",
    "    axes[1].set_title(\"Scalability of the model\")\n",
    "\n",
    "    # Plot fit_time vs score\n",
    "    axes[2].grid()\n",
    "    axes[2].plot(fit_times_mean, test_scores_mean, 'o-')\n",
    "    axes[2].fill_between(fit_times_mean, test_scores_mean - test_scores_std,\n",
    "                         test_scores_mean + test_scores_std, alpha=0.1)\n",
    "    axes[2].set_xlabel(\"fit_times\")\n",
    "    axes[2].set_ylabel(\"Score\")\n",
    "    axes[2].set_title(\"Performance of the model\")\n",
    "\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "fig, axes = plt.subplots(3, 1, figsize=(10, 15))\n",
    "title = \"Learning Curves\"\n",
    "# Cross validation with 100 iterations to get smoother mean test and train\n",
    "# score curves, each time with 20% data randomly selected as a validation set.\n",
    "cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)\n",
    "plot_learning_curve(model, # model is the model defined previously that you would like to evaluate\n",
    "                    title, X, y, axes=axes,\n",
    "                    cv=cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.3 Confusion matrix\n",
    "Usually used for supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "y_pred = model.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred, normalize=None) ## normalize={'true', 'pred', 'all'}\n",
    "cm_display = ConfusionMatrixDisplay(cm, display_labels=class_names).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "disp = plot_confusion_matrix(model, X_test, y_test,\n",
    "                             display_labels=class_names,\n",
    "                             cmap=plt.cm.Blues,\n",
    "                             normalize=None) # 'true', 'pred', or 'all'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.4 Roc curve\n",
    "Applying on binary classificaiton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the datasets used for classification in this section to binary one\n",
    "y = y==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "y_score = model.decision_function(X_test)\n",
    "fpr, tpr, _ = roc_curve(y_test, y_score, pos_label=model.classes_[1])\n",
    "roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Method 2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "disp = plot_roc_curve(model, X_test, y_test, alpha=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.5 Precision recall\n",
    "The example applys on binary classification.\n",
    "\n",
    "For application on multi-label problems, see <https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html#sphx-glr-auto-examples-model-selection-plot-precision-recall-py>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the datasets used for classification in this section to binary one\n",
    "y = y==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "y_score = model.decision_function(X_test)\n",
    "prec, recall, _ = precision_recall_curve(y_test, y_score,\n",
    "                                         pos_label=model.classes_[1])\n",
    "pr_display = PrecisionRecallDisplay(precision=prec, recall=recall).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import average_precision_score, plot_precision_recall_curve\n",
    "y_score = model.decision_function(X_test)\n",
    "average_precision = average_precision_score(y_test, y_score)\n",
    "disp = plot_precision_recall_curve(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Pipelines and Composite Estimators\n",
    "Code boxes in this section are examples. Adjust according to practical situations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Pipeline `sklearn.pipeline`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.1 Chaining estimators\n",
    "`Pipeline` is used to chain multiple estimators into one. This is useful as there is often a fixed sequence of steps in processing the data, for example feature selection, normalization and classification.\n",
    "\n",
    "\\* All estimators in a pipeline, except the last one, must be transformers (i.e. must have a `transform` method). The last estimator may be any type (transformer, classifier, etc.).\n",
    "\n",
    "\\* To avoid repeated computation within a pipeline, the parameter `memory` may be used, but note that the transformer instance given to the pipeline will not be inspected directly. See details about it on the two pages below,\n",
    "- <https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline>\n",
    "- <https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html#sklearn.pipeline.make_pipeline>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: filling the names of including steps automatically\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Binarizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "pipe = make_pipeline(Binarizer(),\n",
    "                     MultinomialNB())\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: self-defining the names of including steps\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "estimators = [('reduce_dim', PCA()),\n",
    "              ('clf', SVC())]\n",
    "pipe = Pipeline(estimators)\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.steps[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe['clf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.set_params(clf__C=10) # access and change parameters of an including estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2 Composite feature spaces\n",
    "`FeatureUnion` combines several transformer objects into a new transformer that combines their output. During fitting, each of these is fit to the data independently. The transformers are applied in parallel, and the feature matrices they output are concatenated side-by-side into a larger matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: filling the names of including steps automatically\n",
    "from sklearn.pipeline import make_union\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "combined = make_union(PCA(), TruncatedSVD())\n",
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: self-defining the names of including steps\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "estimators = [('linear_pca', PCA()),\n",
    "              ('kernel_pca', KernelPCA())]\n",
    "combined = FeatureUnion(estimators)\n",
    "combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Composite estimators `sklearn.compose`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1 Transforming target in regression\n",
    "Useful for applying a non-linear transformation to the target `y` in regression problems. This transformation can be given as a Transformer or as a function and its inverse such as log and exp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using a transformer\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "transformer = QuantileTransformer(output_distribution='normal')\n",
    "regressor = LinearRegression()\n",
    "reg_trans = TransformedTargetRegressor(regressor=regressor,\n",
    "                                       transformer=transformer)\n",
    "reg_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using a function and its inverse\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "reg_trans = TransformedTargetRegressor(regressor=LinearRegression(),\n",
    "                                       func=np.log, inverse_func=np.exp)\n",
    "reg_trans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2 Transformer for heterogeneous data\n",
    "`ColumnTransformer` helps performing different transformations for different columns of the data, within a Pipeline that is safe from data leakage and that can be parametrized. It works on arrays, sparse matrices, and pandas DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: filling the names of including steps automatically\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "column_trans = make_column_transformer((StandardScaler(), ['numerical_column']),\n",
    "                        (OneHotEncoder(), ['categorical_column']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: self-defining the names of including steps\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "ct = ColumnTransformer([(\"norm1\", Normalizer(norm='l1'), [0, 1]),\n",
    "                        (\"norm2\", Normalizer(norm='l1'), slice(2, 4))])\n",
    "X = np.array([[0., 1., 2., 2.],\n",
    "              [1., 1., 0., 1.]])\n",
    "ct.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diplays HTML representation in a jupyter context\n",
    "from sklearn import set_config\n",
    "set_config(display='diagram')\n",
    "\n",
    "ct # or other defined pipeline / composite transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Model persistence\n",
    "After training a scikit-learn model, it is desirable to have a way to persist the model for future use without having to retrain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Python自带的方法 Python's built-in method\n",
    "__Notes__\n",
    "- Never unpickle untrusted data as it could lead to malicious code being executed upon loading.\n",
    "- While models saved using one version of scikit-learn might load in other versions, this is entirely unsupported and inadvisable. It should also be kept in mind that operations performed on such data could give different and unexpected results.\n",
    "\n",
    "In order to rebuild a similar model with future versions of scikit-learn, additional metadata should be saved along the pickled model:\n",
    "- The training data, e.g. a reference to an immutable snapshot\n",
    "- The python source code used to generate the model\n",
    "- The versions of scikit-learn and its dependencies\n",
    "- The cross validation score obtained on the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.1 `pickle`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import datasets\n",
    "clf = svm.SVC()\n",
    "X, y= datasets.load_iris(return_X_y=True)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# save clf\n",
    "import pickle\n",
    "s = pickle.dumps(clf)\n",
    "\n",
    "# release clf as clf2\n",
    "clf2 = pickle.loads(s)\n",
    "clf2.predict(X[0:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.2 `joblib`\n",
    "In the specific case of scikit-learn, it may be better to use joblib’s replacement of pickle (dump & load), which is more efficient on objects that carry large numpy arrays internally as is often the case for fitted scikit-learn estimators, but can only pickle to the disk and not to a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "dump(clf, 'pickle_example.joblib')\n",
    "# naming by the string, the model is saved on the same path as the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then it can be loaded back\n",
    "clf = load('pickle_example.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 可用于交互操作的方式 Interoperable formats\n",
    "For reproducibility and quality control needs, when different architectures and environments should be taken into account, exporting the model in [Open Neural Network Exchange](https://onnx.ai/) format or [Predictive Model Markup Language (PMML)](http://dmg.org/pmml/v4-4-1/GeneralStructure.html) format might be a better approach than using pickle alone. These are helpful where you may want to use your model for prediction in a different environment from where the model was trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take advantage of these tools and use your talents to create a better world!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "199px",
    "width": "505px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "502.2px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
